[
{
	"uri": "/",
	"title": "Kubedge",
	"tags": [],
	"description": "Main page",
	"content": " Kubedge Personal, Portable Edge Network and Lab. Kubedge is personal and portable edge cloud. The key concept is to leverage the three network interfaces available on each PI:\n 1 GB Ethernet Wifi Bluetooth.  The PI are interconnected together using the local switch. The top/master PI is acting as a NAT/Router for the rest of the PI. This main advantage that when you move your lab from home to work, all the IPs of the cluster stay identical.\nThis page and web site is still work in progress.\n Trainings \u0026amp; Tutorials Associated GIT Repos master branch is mainly used for dev. arm32v7 branch contains stable code for Rasberry PI. amd64 branch contains stable code for AMD64 cluster\nSIMULATOR REPOS:\n kubesim_base kubesim_5gc kubesim_elte kubesim_epc kubesim_lte kubesim_nr  TUTORIAL REPOS:\n kubedge kubeplay  Infrastructure Repos INFRASTRUCTURE REPOS:\n kube-rpi ansible-kube-rpi  Docker imagesi for PI SIMULATOR DOCKER IMAGES FOR PI:\n kubesim_base kubesim_5gc kubesim_elte kubesim_epc kubesim_lte kubesim_nr  TUTORIAL REPOS FOR PI:\n kubedge kubeplay  Docker images for AMD64 SIMULATOR DOCKER IMAGES FOR AMD64:\n kubesim_base kubesim_5gc kubesim_elte kubesim_epc kubesim_lte kubesim_nr  TUTORIAL REPOS FOR AMD64:\n kubedge kubeplay  HELM REPOS  helmrepos  Gerrit Review SIMULATOR REPOS:\n kubesim_base kubesim_5gc kubesim_elte kubesim_epc kubesim_lte kubesim_nr  TUTORIAL REPOS:\n kubedge kubeplay  Automatic Build Review SIMULATOR REPOS:\n kubesim_base kubesim_5gc kubesim_elte kubesim_epc kubesim_lte kubesim_nr  TUTORIAL REPOS:\n kubedge kubeplay  "
},
{
	"uri": "/post/",
	"title": "Latest News",
	"tags": [],
	"description": "",
	"content": " Posts Convertion to new HUGO is still WIP\n"
},
{
	"uri": "/pi_cluster/applications/children/kubeplay/",
	"title": "Installing Kubedge Kubeplay",
	"tags": ["kubernetes", "kubedge", "rpi"],
	"description": "",
	"content": "Install simplistic Kubeplay Application on the PI Cluster. It is a primitiv golang based server which get the user to go through the entire DevOps process (to build the docker image) and deploy it using Helm on Kubedge.\n\nKey Aspects  Install Kubeplay Understand Helm and values.yaml Test access to Kubeplay  Deploy Using helm helm repo add kubedge1 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1\u0026quot; helm repo update  Apply the labels to the nodes No special label needed for kubeplay\nDirect installation from the repo If you feel lucky:\nhelm install kubedge1/kubeplay-arm32v7 --name kubeplay  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge1/kubeplay-arm32v7 tar xvf kubeplay-arm32v7-0.1.0.tgz cd kubeplay-arm32v7/ helm install . --name kubeplay  Testing $ kubectl get services | grep kubeplay kubeplay-kubeplay-arm32v7 NodePort 10.105.196.0 \u0026lt;none\u0026gt; 8005:32520/TCP 4m58s  Open a web browser and point at the master IP: example\nSource Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge kubeplay release \u0026quot;kubeplay\u0026quot; deleted  Deploy WIP\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/devops/cicd/children/setup_dockerhub/",
	"title": "Setting up DockerHub",
	"tags": ["devops", "github"],
	"description": "",
	"content": "This tutorial describes how to leverage DockerHub.com to save docker images for AMD64 and ARM32v7.\n\nKey Aspects  Create new repository in DockerHub Create new namespaces and teams in DockerHub.  Deploy Also the Kubedge team went through the process, it has not been documented.\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/devops/git_gerrit/children/setup_git/",
	"title": "Setting up GitHub",
	"tags": ["devops", "github"],
	"description": "",
	"content": "Create and maintain a github organisation\n\nKey Aspects  Create repository Create team  Deploy Also the Kubedge team went through the process, it has not been documented.\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/nfv/golang/children/golang/",
	"title": "GOLANG",
	"tags": ["kubernetes", "golang", "cni"],
	"description": "",
	"content": "GOLANG based Networking libraries usable in micro services in docker container to simulate VNF.\n\nGolang libraries This is an extract from awesome-go\n Libraries for working with various layers of the network.\n arp - Package arp implements the ARP protocol, as described in RFC 826. buffstreams - Streaming protocolbuffer data over TCP made easy. canopus - CoAP Client/Server implementation (RFC 7252). cidranger - Fast IP to CIDR lookup for Go. dhcp6 - Package dhcp6 implements a DHCPv6 server, as described in RFC 3315. dns - Go library for working with DNS. ether - Cross-platform Go package for sending and receiving ethernet frames. ethernet - Package ethernet implements marshaling and unmarshaling of IEEE 802.3 Ethernet II frames and IEEE 802.1Q VLAN tags. fasthttp - Package fasthttp is a fast HTTP implementation for Go, up to 10 times faster than net/http. fortio - Load testing library and command line tool, advanced echo server and web UI. Allows to specify a set query-per-second load and record latency histograms and other useful stats and graph them. Tcp, Http, gRPC. ftp - Package ftp implements a FTP client as described in RFC 959. gNxI - A collection of tools for Network Management that use the gNMI and gNOI protocols. go-getter - Go library for downloading files or directories from various sources using a URL. go-stun - Go implementation of the STUN client (RFC 3489 and RFC 5389). gobgp - BGP implemented in the Go Programming Language. golibwireshark - Package golibwireshark use libwireshark library to decode pcap file and analyse dissection data. gopacket - Go library for packet processing with libpcap bindings. gopcap - Go wrapper for libpcap. goshark - Package goshark use tshark to decode IP packet and create data struct to analyse packet. gosnmp - Native Go library for performing SNMP actions. gotcp - Go package for quickly writing tcp applications. grab - Go package for managing file downloads. graval - Experimental FTP server framework. HTTPLab - HTTPLabs let you inspect HTTP requests and forge responses. jazigo - Jazigo is a tool written in Go for retrieving configuration for multiple network devices. kcp-go - KCP - Fast and Reliable ARQ Protocol. kcptun - Extremely simple \u0026amp; fast udp tunnel based on KCP protocol. lhttp - Powerful websocket framework, build your IM server more easily. linkio - Network link speed simulation for Reader/Writer interfaces. llb - It\u0026rsquo;s a very simple but quick backend for proxy servers. Can be useful for fast redirection to predefined domain with zero memory allocation and fast response. mdns - Simple mDNS (Multicast DNS) client/server library in Golang. mqttPaho - The Paho Go Client provides an MQTT client library for connection to MQTT brokers via TCP, TLS or WebSockets. NFF-Go - Framework for rapid development of performant network functions for cloud and bare-metal (former YANFF). packet - Send packets over TCP and UDP. It can buffer messages and hot-swap connections if needed. peerdiscovery - Pure Go library for cross-platform local peer discovery using UDP multicast portproxy - Simple TCP proxy which adds CORS support to API\u0026rsquo;s which don\u0026rsquo;t support it. publicip - Package publicip returns your public facing IPv4 address (internet egress). quic-go - An implementation of the QUIC protocol in pure Go. raw - Package raw enables reading and writing data at the device driver level for a network interface. sftp - Package sftp implements the SSH File Transfer Protocol as described in https://filezilla-project.org/specs/draft-ietf-secsh-filexfer-02.txt. ssh - Higher-level API for building SSH servers (wraps crypto/ssh). sslb - It\u0026rsquo;s a Super Simples Load Balancer, just a little project to achieve some kind of performance. stun - Go implementation of RFC 5389 STUN protocol. tcp_server - Go library for building tcp servers faster. tspool - A TCP Library use worker pool to improve performance and protect your server. utp - Go uTP micro transport protocol implementation. water - Simple TUN/TAP library. winrm - Go WinRM client to remotely execute commands on Windows machines. xtcp - TCP Server Framework with simultaneous full duplex communication,graceful shutdown,custom protocol.\nBuild Kubernetes executables for ARM32V7  WIP\n Conclusion WIP\n Reference Links  DPDK FD.io "
},
{
	"uri": "/nfv/ovs/children/linux_bridging/",
	"title": "Linux Bridging",
	"tags": ["kubernetes", "linux bridging", "cni"],
	"description": "",
	"content": "The first goal is to replace the NAT+DHCP on the worker nodes by Linux Bridging setup. All the devices connecting to the cluser (Wifi AP, Bluetooth or derectly through the internal switch would will be in the same IP network).\n\nKey Aspects  Deploy network slicing simulation using linux bridging.  Install Linux Bridging on PI Cluster WIP\n Conclusion WIP\n Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/pi_cluster/os_installation/children/cluster_assembly/",
	"title": "Raspberry PI cluster Assembly",
	"tags": ["ansible", "rpi"],
	"description": "",
	"content": "This page contains examples of harware to use in order to build a cluster\n\nAssembled 3 nodes cluster 3 Nodes Cluster Hardware Assembly    Component URL Example Quantity Cost Estimate     Rack PI Rack 1 $30   PI PI 3 $105   SD Card SD Cards 3 $30   Heat Sink Heat Sink 6 $10   5-Port Power Supply Power Supply 3 $20   Power Cable for PI Power Cable for PI 3 $10   Power Cable for Switch Power Cable for Switch 1 $5   5-Port Switch Switch 1 $15   1ft CAT6 Cables Cat 6 Cables 3 $10   Total   $235   Blinkt Blinkt 3 $15   Transport Case Case 1 $25   Total with addons   $275    Pay attention to the PI ethernet speed when picking the switch. PI3B are only 100Mbps where PI3B+ are 1Gpbs\n 5 Nodes Cluster Hardware Assembly    Component URL Example Quantity Cost Estimate     Rack PI Rack 1 $30   PI PI 5 $175   SD Card SD Cards 5 $50   Heat Sink Heat Sink 10 $10   10-Port Power Supply Power Supply 1 $40   Power Cable for PI Power Cable for PI 5 $10   Power Cable for Switch Power Cable for Switch 1 $5   8-Port Switch Switch 1 $25   1.5f CAT6 Cables Cat 6 Cables 5 $10   Total   $355   Blinkt Blinkt 3 $15   Transport Case Case 1 $25   Total with addons   $395    Having available ports on the switch is very helpfull to connect to the cluster when the WIFI access is not working properly.\n 1ft long cables for the top PI are kind of too short. 3ft long cables are kind of too long.\n Reference Links  Video1 "
},
{
	"uri": "/pi_cluster/docker_kubernetes/children/2018-07-03-a/",
	"title": "Creating a Raspberry 3 B+ Kubernetes Cluster",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "Also GCE is perfect to learn Kubernetes, building Kubernetes on top of PI Cluster brings another dimension to the learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.\n\nKey Aspects  Build a Raspberry 3B+ Cluster Deploy Kubernetes on that Cluster  Kubernetes for ARM32V7 The current version is Kubernetes 12.2.2\nOn Master and Slaves Kubedge team did not automate yet the installation of kubeadm on each node\nsudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026quot;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; \u0026gt; /etc/apt/sources.list.d/kubernetes.list apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm kubectl kubelet  To help during the initialization phase, get kubeadm to download the images onto docker\nkubeadm config images pull  Initialize the Kubernetes Master node kubeadm init --pod-network-cidr 10.244.0.0/16 --apiserver-advertise-address 192.168.2.1 [init] using Kubernetes version: v1.12.2 [preflight] running pre-flight checks [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.04.0-ce. Latest validated version: 18.06 [preflight/images] Pulling images required for setting up a Kubernetes cluster [preflight/images] This might take a minute or two, depending on the speed of your internet connection [preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet] Writing kubelet environment file with flags to file \u0026quot;/var/lib/kubelet/kubeadm-flags.env\u0026quot; [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [preflight] Activating the kubelet service [certificates] Generated ca certificate and key. [certificates] Generated apiserver-kubelet-client certificate and key. [certificates] Generated apiserver certificate and key. [certificates] apiserver serving cert is signed for DNS names [kubemaster-pi kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.2.1] [certificates] Generated front-proxy-ca certificate and key. [certificates] Generated front-proxy-client certificate and key. [certificates] Generated etcd/ca certificate and key. [certificates] Generated etcd/peer certificate and key. [certificates] etcd/peer serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [192.168.2.1 127.0.0.1 ::1] [certificates] Generated etcd/server certificate and key. [certificates] etcd/server serving cert is signed for DNS names [kubemaster-pi localhost] and IPs [127.0.0.1 ::1] [certificates] Generated etcd/healthcheck-client certificate and key. [certificates] Generated apiserver-etcd-client certificate and key. [certificates] valid certificates and keys now exist in \u0026quot;/etc/kubernetes/pki\u0026quot; [certificates] Generated sa key and public key. [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/admin.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/kubelet.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/controller-manager.conf\u0026quot; [kubeconfig] Wrote KubeConfig file to disk: \u0026quot;/etc/kubernetes/scheduler.conf\u0026quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; [init] waiting for the kubelet to boot up the control plane as Static Pods from directory \u0026quot;/etc/kubernetes/manifests\u0026quot; [init] this might take a minute or longer if the control plane images have to be pulled [apiclient] All control plane components are healthy after 88.010108 seconds [uploadconfig] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.12\u0026quot; in namespace kube-system with the configuration for the kubeletsin the cluster [markmaster] Marking the node kubemaster-pi as master by adding the label \u0026quot;node-role.kubernetes.io/master=''\u0026quot; [markmaster] Marking the node kubemaster-pi as master by adding the taints [node-role.kubernetes.io/master:NoSchedule] [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;kubemaster-pi\u0026quot; asan annotation [bootstraptoken] using token: vej1mx.6qf2xljr39rr1i14 [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \u0026quot;cluster-info\u0026quot; ConfigMap in the \u0026quot;kube-public\u0026quot; namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \u0026quot;kubectl apply -f [podnetwork].yaml\u0026quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx  Initialize kubectl configuration As normal user (not root)\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get nodes  On the slave node kubeadm join 192.168.2.1:6443 --token vej1mx.yyyyyy --discovery-token-ca-cert-hash sha256:xxxxxx  Setup CNI At that point, nodes are not in ready state yet and CoreDNS in pending state\nkubectl get nodes kubectl get all -n kube-system  The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/flannel/ kubectl apply -f flannel.yaml  The nodes should turn ready and CoreDNS should start\nkubectl get nodes kubectl get all -n kube-system  Kubernetes for ARM64V8 Need to installed version 1.13.1-beta1 compiled with go 1.11 (required)\nOn Master and Slaves Kubedge team did not automate yet the installation of kubeadm on each node\nInstall only the kubelet on each node. At the time, 12.2.2 is installed since it is the stable version\nsudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026quot;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; \u0026gt; /etc/apt/sources.list.d/kubernetes.list apt-get update \u0026amp;\u0026amp; apt-get install -y kubelet  sudo systemctl stop kubelet curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.0-beta.1/bin/linux/arm64/kubectl curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.0-beta.1/bin/linux/arm64/kubeadm curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.13.0-beta.1/bin/linux/arm64/kubelet chmod 755 kubectl chmod 755 kubeadm chmod 755 kubelet mv kubectl /usr/bin/kubectl mv kubeadm /usr/bin/kubeadm mv kubelet /usr/bin/kubelet  To help during the initialization phase, get kubeadm to download the images onto docker\nkubeadm config images pull  Initialize the Kubernetes Master node Kubernetes Cluster 1: 5 node clusters kubectl get nodes NAME STATUS ROLES AGE VERSION kube-node01 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node02 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node03 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kube-node04 Ready \u0026lt;none\u0026gt; 23d v1.9.8 kubemaster-pi Ready master 23d v1.9.8  kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/hypriot-587768b4f5-7bqpz 1/1 Running 0 23d default pod/hypriot-587768b4f5-b8xjq 1/1 Running 0 23d default pod/hypriot-587768b4f5-s2mzt 1/1 Running 0 23d kube-system pod/etcd-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-controller-manager-kubemaster-pi 1/1 Running 0 23d kube-system pod/kube-dns-7b6ff86f69-l7lf6 3/3 Running 0 23d kube-system pod/kube-flannel-ds-8xbx4 1/1 Running 0 23d kube-system pod/kube-flannel-ds-9cz9f 1/1 Running 0 23d kube-system pod/kube-flannel-ds-rgpcq 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xnjtz 1/1 Running 0 23d kube-system pod/kube-flannel-ds-xxdf6 1/1 Running 0 23d kube-system pod/kube-proxy-5m95q 1/1 Running 0 23d kube-system pod/kube-proxy-7sh7m 1/1 Running 0 23d kube-system pod/kube-proxy-f7t9r 1/1 Running 0 23d kube-system pod/kube-proxy-pkqvd 1/1 Running 0 23d kube-system pod/kube-proxy-shrdr 1/1 Running 0 23d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 0 23d kube-system pod/kubernetes-dashboard-7fcc5cb979-8vbmp 1/1 Running 0 23d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/hypriot ClusterIP 10.110.24.241 \u0026lt;none\u0026gt; 80/TCP 23d default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 23d kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 23d kube-system service/kubernetes-dashboard NodePort 10.102.144.189 \u0026lt;none\u0026gt; 443:30383/TCP 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.extensions/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.extensions/kube-proxy 5 5 5 5 5 \u0026lt;none\u0026gt; 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default deployment.extensions/hypriot 3 3 3 3 23d kube-system deployment.extensions/kube-dns 1 1 1 1 23d kube-system deployment.extensions/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE default replicaset.extensions/hypriot-587768b4f5 3 3 3 23d kube-system replicaset.extensions/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.extensions/kubernetes-dashboard-7fcc5cb979 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 23d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 \u0026lt;none\u0026gt; 23d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE default deployment.apps/hypriot 3 3 3 3 23d kube-system deployment.apps/kube-dns 1 1 1 1 23d kube-system deployment.apps/kubernetes-dashboard 1 1 1 1 23d NAMESPACE NAME DESIRED CURRENT READY AGE default replicaset.apps/hypriot-587768b4f5 3 3 3 23d kube-system replicaset.apps/kube-dns-7b6ff86f69 1 1 1 23d kube-system replicaset.apps/kubernetes-dashboard-7fcc5cb979 1 1 1 23d  Cluster 2: 3 node clusters kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 56m v1.11.0 master-pi Ready master 1h v1.11.0 nas-pi Ready \u0026lt;none\u0026gt; 5m v1.11.0  kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-78fcdf6894-cw5p8 1/1 Running 0 59m kube-system pod/coredns-78fcdf6894-czjcj 1/1 Running 0 1h kube-system pod/etcd-master-pi 1/1 Running 0 1h kube-system pod/kube-apiserver-master-pi 1/1 Running 0 59m kube-system pod/kube-controller-manager-master-pi 1/1 Running 12 1h kube-system pod/kube-flannel-ds-bhllh 1/1 Running 2 3m kube-system pod/kube-flannel-ds-q7cp2 1/1 Running 0 22m kube-system pod/kube-flannel-ds-wqxsz 1/1 Running 0 22m kube-system pod/kube-proxy-4chwh 1/1 Running 0 45m kube-system pod/kube-proxy-6r5mn 1/1 Running 0 3m kube-system pod/kube-proxy-vvj6j 1/1 Running 0 1h kube-system pod/kube-scheduler-master-pi 1/1 Running 0 59m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 1h kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 1h NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 3 3 3 3 3 beta.kubernetes.io/arch=arm 53m kube-system daemonset.apps/kube-proxy 3 3 3 3 3 beta.kubernetes.io/arch=arm 1h NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2 2 2 2 1h NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-78fcdf6894 2 2 2 1h  Cleanup cleanup\nsudo kubeadm reset sudo docker rm $(sudo docker ps -qa) sudo docker image rm $(sudo docker image list -qa)  Reference Links  kubeadm1 kubeadm2 "
},
{
	"uri": "/devops/",
	"title": "DevOps",
	"tags": [],
	"description": "This is devops tutorial page",
	"content": "DevOps Since each PI acts a cloud node, you need to be able to install, manage, test and share the software. Kubedge leverages GitHub and GerritHub to share and review code, Travis-CI for continuous integration. One key aspect is to be able to build executables and docker image that can run on the broadcom processor of the PI. The docker images are available to download on dockerhub.\nIn order to build the present website, we had to understand how to use github pages and well as for instance the Hugo framework to convert the markdown documents into HTML.\nFinally some of the operations are easier to run on your laptop, for instance the development of the software. Kubedge team did create an Ubuntu base virtual machine using an SDK.\n\n Git/Gerrit  This is devops/git_gerrit tutorial page\n CI/CD  This is devops/cicd tutorial page\n Hugo/GitHubPages  This is devops/hugo_githubpages tutorial page\n Kubedge SDK  This is devops/kubedgesdk tutorial page\n MicroServices  This is pi_cluster/microservices tutorial page\n Advanced \u0026amp; WIP  Advanced tutorials and Work In Progress\n"
},
{
	"uri": "/devops/git_gerrit/",
	"title": "Git/Gerrit",
	"tags": [],
	"description": "This is devops/git_gerrit tutorial page",
	"content": " GIT/GERRIT  Setting up GitHub  Create and maintain a github organisation\n\n Setting up GerritHub  This tutorial describes how to leverage GerritHub.io for proper code review\n\n "
},
{
	"uri": "/nfv/ovs/",
	"title": "OVS &amp; ODL",
	"tags": [],
	"description": "OVS &amp; ODL Tutorial",
	"content": " OVS \u0026amp; OpenDayLight  Linux Bridging  The first goal is to replace the NAT+DHCP on the worker nodes by Linux Bridging setup. All the devices connecting to the cluser (Wifi AP, Bluetooth or derectly through the internal switch would will be in the same IP network).\n\n OVS  The second goal is to replace Linux Bridging with OVS. The setup will also use VLAN tagging on eth0 to seggrate kubernetes/pod traffic from \u0026ldquo;tenant/5g traffic.\n\n OpenDayLight  The third is to implement SDN/Network Slicing using ODL and controlling OVS\n\n "
},
{
	"uri": "/pi_cluster/os_installation/",
	"title": "Operating System",
	"tags": [],
	"description": "This is pi_cluster/os_installation tutorial page",
	"content": " OS Installation  Raspberry PI cluster Assembly  This page contains examples of harware to use in order to build a cluster\n\n OS Installation on Master PI  This page describes simple steps to install the OS (HypriotOS) on the PIs.\n\n Setting up WPA supplicant on Master PI  This tutorial how to setup WIFI on the master pi to connect to different WLAN\n\n Setting up DHCPD on master PI  This tutorial how to setup DHCPD and NAT so that the slave PI can access the internet through the NAT (ETH0-\u0026gt;WLAN0) running on Master PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to PI or PCs.\n\n OS Installation on Worker PI  This page describes simple steps to install the OS (HypriotOS) on the PIs.\n\n Using Ansible to manage Raspberry PI cluster  Even if the ultimate goal is to manage completly the cluster using Kubernetes, the ability to use Ansible during debug process is very usefull. The goal here is to setup ansible inventory, basic playbooks.\n\n Create a Rapsberry PI Rescue Dongle  I encountered multiple issues trying to repartition my SD on my PI. Because the / directory is mounted, it never really worked safely for me to use fdisk. Morevoer some of the powerfull tools such as gparted need X11 installed, which I don\u0026rsquo;t have by default.\nHopefully the new PI3 B and B+ are able to boot from USB, hence the idea of creating a Rescue Dongle\n\n "
},
{
	"uri": "/lte_to_5g/rpi_wifi_ap/",
	"title": "WIFI AP",
	"tags": [],
	"description": "This is lte_to_5g/rpi_wifi_bluetooth tutorial page",
	"content": " WIFI AP sequenceDiagram participant UE_PC participant 5GNR_WorkerPI participant 5GC_MasterPI participant MPLSNetwork UE_PC-5GNR_WorkerPI: Establish RAN Connection 5GNR_WorkerPI-UE_PC: Return IP UE_PC-5GNR_WorkerPI: wlan0/ran IP traffic loop Linux NAT 5GNR_WorkerPI-5GNR_WorkerPI: translate wlan0 IP into eth0 IP end 5GNR_WorkerPI-5GC_MasterPI: eth0/backhaul IP traffic loop Linux NAT 5GC_MasterPI-5GC_MasterPI: translate eth0 IP into wlan0 IP end 5GC_MasterPI-MPLSNetwork: wlan0/core IP traffic MPLSNetwork-5GC_MasterPI: IP traffic 5GC_MasterPI-5GNR_WorkerPI: IP traffic 5GNR_WorkerPI-UE_PC: IP traffic  We are currently using NAT here to keep the simulation simple. It would be more accurate to use Linux Briding or OVS to simulate the fact that the SGW running on the EPC is \u0026ldquo;allocating\u0026rdquo; the IP address.\n  Setting up HOSTAPD on 5G NR node  This tutorial how to setup WIFI AccessPoint on PI simulating 5G NR node\n\n Setting up DHCPD on 5G NR Node  This tutorial how to setup DHCPD and NAT so that the UE can access the internet through the AP \u0026amp; NAT running on 5G NR simulating PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to UE or PCs.\n\n "
},
{
	"uri": "/pi_cluster/applications/children/kubernetes_dashboard/",
	"title": "Installing Kubernetes Dashboard",
	"tags": ["kubernetes", "dashboard", "rpi"],
	"description": "",
	"content": "Install Kubernetes Dashboard on the PI Cluster\n\nKey Aspects  Install Dashboard Access the dashboard for a web browser  Deploy Using kubectl cd $KUBE_RPI_GITREPO_LOCATION/kube-deployment/dashboard kubectl apply -f kubernetes-dashboard-arm.yaml  Deploy Using helm Helm Chart is still work in progress\n helm repo add kubedge2 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2\u0026quot; helm repo update  Direct installation from the repo If you feel lucky:\nhelm install kubedge2/kubernetes-dashboard-arm32v7 --name kubernetes-dashboard  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge2/kubernetes-dashboard-arm32v7 tar xvf kubernetes-dashboard-arm32v7-0.7.5.tgz cd kubernetes-dashboard-arm32v7/ helm install . --name kubernetes-dashboard  Source Code The chart is available under:\n kubernetes-dashboard  Cleanup kubectl delete -f kubernetes-dashboard-arm.yaml secret \u0026quot;kubernetes-dashboard-certs\u0026quot; deleted serviceaccount \u0026quot;kubernetes-dashboard\u0026quot; deleted role.rbac.authorization.k8s.io \u0026quot;kubernetes-dashboard-minimal\u0026quot; deleted rolebinding.rbac.authorization.k8s.io \u0026quot;kubernetes-dashboard-minimal\u0026quot; deleted deployment.apps \u0026quot;kubernetes-dashboard\u0026quot; deleted service \u0026quot;kubernetes-dashboard\u0026quot; deleted  Kubernetes Dashboard Cluster 1: 5 node clusters List of the nodes in Cluster 1\nKubernetes kube-system overview of Cluster 1\nCluster 2: 3 node clusters List of the nodes in Cluster 2\nKubernetes kube-system overview of Cluster 2\nConclusion Still need to debug the Kubernetes Helm Chart.\n Reference Links WIP\n"
},
{
	"uri": "/lte_to_5g/lte_and_5g_core_simulation/children/kubesim_epc/",
	"title": "Kubedge EPC Simulation",
	"tags": ["kubernetes", "kubedge", "epc", "rpi"],
	"description": "",
	"content": "Install Kubedge EPC Application on the PI Cluster\n\nKey Aspects  Install EPC Test access to EPC  Deploy Using helm helm repo add hack4easy \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where EPC is installed\nFor instance:\nkubectl label nodes kubemaster-pi kubedgeNodeType=lte-core  Direct installation from the repo If you feel lucky:\nhelm install hack4easy/kubesim-epc-arm32v7 --name sim-epc  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch hack4easy/kubesim-epc-arm32v7 tar xvf kubesim-epc-arm32v7-0.1.0.tgz cd kubesim-epc-arm32v7/ helm install . --name sim-epc  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge sim-epc release \u0026quot;epc\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/lte_to_5g/lte_and_5g_ran_simulation/children/kubesim_lte/",
	"title": "Kubedge LTE Simulation",
	"tags": ["kubernetes", "kubedge", "lte", "rpi"],
	"description": "",
	"content": "Install Kubedge LTE Application on the PI Cluster\n\nKey Aspects  Install LTE Test access to LTE  Deploy Using helm helm repo add hack4easy \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where LTE is installed\nFor instance:\nkubectl label nodes kubemaster-pi kubedgeNodeType=lte-ran  Direct installation from the repo If you feel lucky:\nhelm install hack4easy/kubesim-lte-arm32v7 --name sim-lte  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch hack4easy/kubesim-lte-arm32v7 tar xvf kubesim-lte-arm32v7-0.1.0.tgz cd kubesim-lte-arm32v7/ helm install . --name sim-lte  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge sim-lte release \u0026quot;lte\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/devops/git_gerrit/children/setup_gerrit/",
	"title": "Setting up GerritHub",
	"tags": ["devops", "github"],
	"description": "",
	"content": "This tutorial describes how to leverage GerritHub.io for proper code review\n\nKey Aspects  Import repository into GerritHub. Create new label such as \u0026ldquo;Workflow\u0026rdquo;. Resynchronize GerritHub and GitHub if somebody was able to push by mistake directly to GitHub. Use HTTPS instead of simple SSH when behind corporate proxy.  Deploy Also the Kubedge team went through the process, it has not been documented.\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/devops/cicd/children/setup_travis-ci/",
	"title": "Setting up Travis-CI",
	"tags": ["devops", "github"],
	"description": "",
	"content": "Leverage Travis-CI for automatic build and integration\n\nKey Aspects  Connect to merge events in GitHub. Create .travis.yaml Use travis-ci.com instead of travis-ci.org CrossCompilation of arm32v7.  Deploy Also the Kubedge team went through the process, it has not been documented.\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/advanced/children/2018-07-18-a/",
	"title": "Compile and Test Portieris",
	"tags": ["kubernetes", "security", "portieris"],
	"description": "",
	"content": "One of the biggest security risks related to Kubernetes are often linked to the fact that it is really hard to ensure that only \u0026ldquo;approved\u0026rdquo; images are deployed in your Kubernetes cluster. The goal here is to leverage Notary and the a project called \u0026ldquo;Portieris\u0026rdquo; created by IBM.\n\nKey Aspects  Rebuild the Notary Rebuild and Deploy Portieris using Helm  Build Notary Clone go get github.com/theupdateframework/notary go install -tags pkcs11 github.com/theupdateframework/notary/cmd/notary  Run notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/library/alpine  $ notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/library/alpine NAME DIGEST SIZE (BYTES) ROLE ---- ------ ------------ ---- 2.6 9ace551613070689a12857d62c30ef0daa9a376107ec0fff0e34786cedb3399b 528 targets 2.7 9f08005dff552038f0ad2f46b8e65ff3d25641747d3912e3ea8da6785046561a 1374 targets 3.1 2f9dfa6adf602d3d7379f11f3d4fd0b7b4d1c526616ee7c0fd5e553a72e4bf79 433 targets 3.2 4b02d27451aabdf2b6bcd09888deed56b2a3b645aab3b77bc9511cf80d0820a6 433 targets 3.3 37f4d7bb352bde58797d0f0c4e6c4e69a9ed44d4e47a8ab4461888d117d14c6a 433 targets 3.4 c1aa0f93d13258dc8b4e87391f02432dc214736c3f176e2e433629c2afe96aa0 433 targets 3.5 4d3ec631cdde98a03b91477b411a1fb42a9cadd8139c2e78029e44e199e58433 433 targets 3.6 de5701d6a3a36dc6a5db260d21be0422fd30dd2d158c1e048b34263e73205cb6 2029 targets 3.7 56e2f91ef15847a2b02a5a03cbfa483949d67a242c37e33ea178e3e7e01e0dfd 2029 targets 3.8 7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430 2029 targets edge 8d9872bf7dc946db1b3cd2bf70752f59085ec3c5035ca1d820d30f1d1267d65d 2029 targets integ-test-base 3952dc48dcc4136ccdde37fbef7e250346538a55a0366e3fccc683336377e372 528 targets latest 7043076348bf5040220df6ad703798fd8593a0918d06d3ce30c6c93be117e430 2029 targets  $ notary -s https://notary.docker.io -d ~/.docker/trust list docker.io/library/nginx NAME DIGEST SIZE (BYTES) ROLE ---- ------ ------------ ---- 1 4a5573037f358b6cdfa2f3e8a9c33a5cf11bcd1675ca72ca76fbe5bd77d0d682 2029 targets 1-alpine 56a9367b64eaef37894842a6f7a19a0ef8e7bd5de964aa844a70b3e2d758033c 2035 targets 1-alpine-perl 26f3b1633ad04d85b76c867d0c46b309b41c5a8d42d5c5b9652769ba9e2c578e 2035 targets 1-perl a070af1e88c071310080cbe4d7b03e06d7fd2b9d982f520edccc8f93af5c641c 2029 targets 1.10 6202beb06ea61f44179e02ca965e8e13b961d12640101fca213efbfd145d7575 948 targets 1.10-alpine 4aacdcf186934dcb02f642579314075910f1855590fd3039d8fa4c9f96e48315 1154 targets 1.10.0-alpine 5b99c2a3ec2b3273a7f77b661941a94e6fa2aa38e5a94c1d90e0924eceefb1e6 13412 targets 1.10.1 35779791c05d119df4fe476db8f47c0bee5943c83eba5656a15fc046db48178b 948 targets 1.10.1-alpine dabd1d182f12e2a7d372338dfd0cde303ef042a6ba01cc829ef464982f9c9e2c 1154 targets 1.10.2 06f933c3fceac34b87bba85074fe7b24fe18ae5e4439d8f9dd038371f02947c3 948 targets 1.10.2-alpine ce50816e7216a66ff1e0d99e7d74891c4019952c9e38c690b3c5407f7af57555 1154 targets 1.10.3 6202beb06ea61f44179e02ca965e8e13b961d12640101fca213efbfd145d7575 948 targets 1.10.3-alpine 4aacdcf186934dcb02f642579314075910f1855590fd3039d8fa4c9f96e48315 1154 targets 1.11 e6693c20186f837fc393390135d8a598a96a833917917789d63766cab6c59582 1156 targets 1.11-alpine 5aadb68304a38a8e2719605e4e180413f390cd6647602bee9bdedd59753c3590 1154 targets 1.11.0 b2e588a4486786239561d73a14ec546cf5ac508bc1890b097dcc60fede1825b8 1956 targets 1.11.0-alpine ddafab6770f3f604672143757000fa1614dcf6afb1d5308dcc47153252fd0817 2369 targets 1.11.1 0fe6413f3e30fcc5920bc8fa769280975b10b1c26721de956e1428b9e2f29d04 1956 targets 1.11.1-alpine 23f809e7fd5952e7d5be065b4d3643fbbceccd349d537b62a123ef2201bc886f 1133 targets  Build and Deploy Portieris Clone $ mkdir -p ~/src/github.com/IBM $ cd ~/src/github.com/IBM $ git clone https://github.com/jbrette/portieris.git  Install helm package $ cd portieris/helm/portieris $ helm install -n portieris . --set IBMContainerService=false --debug  $ kubectl get all -n ibm-system NAME READY STATUS RESTARTS AGE pod/portieris-7b7cbf58c5-gmpwm 1/1 Running 0 1h pod/portieris-7b7cbf58c5-r5vvv 1/1 Running 0 1h pod/portieris-7b7cbf58c5-vkhkh 1/1 Running 0 1h NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/portieris ClusterIP 10.101.194.144 \u0026lt;none\u0026gt; 443/TCP 1h NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/portieris 3 3 3 3 1h NAME DESIRED CURRENT READY AGE replicaset.apps/portieris-7b7cbf58c5 3 3 3 1h  Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/nfv/golang/children/dpdk/",
	"title": "DPDK &amp; FD.io",
	"tags": ["kubernetes", "calico", "cni"],
	"description": "",
	"content": "DPDK and FD.io are providing some kind of support for ARM.\n\nKey Aspects  Intel GO DPDK FD.io  Build Kubernetes executables for ARM32V7 WIP\n Conclusion WIP\n Reference Links  DPDK FD.io "
},
{
	"uri": "/nfv/ovs/children/ovs/",
	"title": "OVS",
	"tags": ["kubernetes", "linux bridging", "cni"],
	"description": "",
	"content": "The second goal is to replace Linux Bridging with OVS. The setup will also use VLAN tagging on eth0 to seggrate kubernetes/pod traffic from \u0026ldquo;tenant/5g traffic.\n\nKey Aspects  Deploy network slicing simulation using OVS.  Install OVS on PI Cluster WIP\n Conclusion WIP\n Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/nfv/ovs/children/odl/",
	"title": "OpenDayLight",
	"tags": ["kubernetes", "ODL", "cni"],
	"description": "",
	"content": "The third is to implement SDN/Network Slicing using ODL and controlling OVS\n\nKey Aspects  Implements 5G SDN using ODL  Install ODL on PI Cluster WIP\n Conclusion WIP\n Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/pi_cluster/maintenance/children/2018-07-13-a/",
	"title": "Deply Helm and Tiller on PI Cluster",
	"tags": ["kubernetes", "helm"],
	"description": "",
	"content": "The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.\n\nKey Aspects  The goal is to setup helm and tiller on the Raspberry PI cluster Having the golang, glide\u0026hellip;and related libraries setup in a PI for compilation is kind of complicated. I started but encounter too many issues (even small), had to install too many compilation related packages on my PI system, hence decided to use an Ubuntu VM to compile and prepare the binaries for image for helm and tiller. This will be usefull to setup CI/CD pipeline with Travis-CI.  Build Tiller executable and Docker image Cross Compiling from Ubuntu Machine First check the go setup. Fetch the code\nLet\u0026rsquo;s check the go environment. See Setup GOLANG environment\nwhich go /usr/bin/go  go version go version go1.10.1 linux/amd64  export GOPATH=$HOME/src mkdir -p src/k8s.io cd src/k8s.io git clone -b release-2.9 git@github.com:kubernetes/helm.git  cd $HOME/src/k8s.io/helm make clean bootstrap build-cross dist APP=helm VERSION=2.9 TARGETS=linux/arm ... go build -o bin/protoc-gen-go ./vendor/github.com/golang/protobuf/protoc-gen-go CGO_ENABLED=0 gox -parallel=3 -output=\u0026quot;_dist/{{.OS}}-{{.Arch}}/{{.Dir}}\u0026quot; -osarch='linux/arm' -tags '' -ldflags '-w -s -X k8s.io/helm/pkg/version.Version=2.9 -X k8s.io/helm/pkg/version.BuildMetadata= -X k8s.io/helm/pkg/version.GitCommit=20adb27c7c5868466912eebdf6664e7390ebe710 -X k8s.io/helm/pkg/version.GitTreeState=clean -extldflags \u0026quot;-static\u0026quot;' k8s.io/helm/cmd/helm Number of parallel builds: 3 --\u0026gt; linux/arm: k8s.io/helm/cmd/helm ( \\ cd _dist \u0026amp;\u0026amp; \\ find * -type d -exec cp ../LICENSE {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec cp ../README.md {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec tar -zcf helm-2.9-{}.tar.gz {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec zip -r helm-2.9-{}.zip {} \\; \\ ) adding: linux-arm/ (stored 0%) adding: linux-arm/LICENSE (deflated 65%) adding: linux-arm/README.md (deflated 59%) adding: linux-arm/helm (deflated 67%)  Let\u0026rsquo;s transfer the helm binarie to RPI\nscp -r _dist/linux-arm/ rpiuser@192.168.1.95:/home/rpiuser/helm_binaries  Cross Compiling and Image cration from Travis-CI  WIP  Helm Cross Compiling from Ubuntu Machine Let\u0026rsquo;s assume we cloned the code for helm already.\nmake clean bootstrap build-cross dist APP=tiller VERSION=2.9 TARGETS=linux/arm ... go build -o bin/protoc-gen-go ./vendor/github.com/golang/protobuf/protoc-gen-go CGO_ENABLED=0 gox -parallel=3 -output=\u0026quot;_dist/{{.OS}}-{{.Arch}}/{{.Dir}}\u0026quot; -osarch='linux/arm' -tags '' -ldflags '-w -s -X k8s.io/helm/pkg/version.Version=2.9 -X k8s.io/helm/pkg/version.BuildMetadata= -X k8s.io/helm/pkg/version.GitCommit=20adb27c7c5868466912eebdf6664e7390ebe710 -X k8s.io/helm/pkg/version.GitTreeState=clean -extldflags \u0026quot;-static\u0026quot;' k8s.io/helm/cmd/tiller Number of parallel builds: 3 --\u0026gt; linux/arm: k8s.io/helm/cmd/tiller ( \\ cd _dist \u0026amp;\u0026amp; \\ find * -type d -exec cp ../LICENSE {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec cp ../README.md {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec tar -zcf helm-2.9-{}.tar.gz {} \\; \u0026amp;\u0026amp; \\ find * -type d -exec zip -r helm-2.9-{}.zip {} \\; \\ ) adding: linux-arm/ (stored 0%) adding: linux-arm/LICENSE (deflated 65%) adding: linux-arm/README.md (deflated 59%) adding: linux-arm/tiller (deflated 67%)  Build the docker image on the remote PI Create a Dockerfile called rootfs/Dockerfile.arm32v7\nFROM debian:jessie-slim RUN apt-get update \\ \u0026amp;\u0026amp; apt-get install -y --no-install-recommends ca-certificates \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ENV HOME /tmp COPY _dist/linux-arm / EXPOSE 44134 CMD [\u0026quot;/tiller\u0026quot;]  export DHUBREPO=jbrette/tiller-arm32v7 export VERSION=2.9 docker build -t $DHUBREPO:$VERSION -f rootfs/Dockerfile.arm32v7 . Sending build context to Docker daemon 284.1MB Step 1/6 : FROM debian:jessie-slim ---\u0026gt; d273fca45b31 Step 2/6 : RUN apt-get update \u0026amp;\u0026amp; apt-get install -y --no-install-recommends ca-certificates \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* ---\u0026gt; Running in e51ea5c31ad7 Get:1 http://security.debian.org jessie/updates InRelease [44.9 kB] Ign http://deb.debian.org jessie InRelease Get:2 http://deb.debian.org jessie-updates InRelease [145 kB] Get:3 http://deb.debian.org jessie Release.gpg [2420 B] Get:4 http://deb.debian.org jessie Release [148 kB] Get:5 http://security.debian.org jessie/updates/main armel Packages [576 kB] Get:6 http://deb.debian.org jessie-updates/main armel Packages [23.7 kB] Get:7 http://deb.debian.org jessie/main armel Packages [8902 kB] Fetched 9843 kB in 49s (199 kB/s) Reading package lists... Reading package lists... Building dependency tree... Reading state information... The following extra packages will be installed: libssl1.0.0 openssl The following NEW packages will be installed: ca-certificates libssl1.0.0 openssl 0 upgraded, 3 newly installed, 0 to remove and 1 not upgraded. Need to get 1692 kB of archives. After this operation, 3770 kB of additional disk space will be used. Get:1 http://security.debian.org/debian-security/ jessie/updates/main ca-certificates all 20141019+deb8u4 [185 kB] Get:2 http://deb.debian.org/debian/ jessie/main libssl1.0.0 armel 1.0.1t-1+deb8u8 [852 kB] Get:3 http://deb.debian.org/debian/ jessie/main openssl armel 1.0.1t-1+deb8u8 [655 kB] debconf: delaying package configuration, since apt-utils is not installed Fetched 1692 kB in 7s (235 kB/s) Selecting previously unselected package libssl1.0.0:armel. (Reading database ... 7451 files and directories currently installed.) Preparing to unpack .../libssl1.0.0_1.0.1t-1+deb8u8_armel.deb ... Unpacking libssl1.0.0:armel (1.0.1t-1+deb8u8) ... Selecting previously unselected package openssl. Preparing to unpack .../openssl_1.0.1t-1+deb8u8_armel.deb ... Unpacking openssl (1.0.1t-1+deb8u8) ... Selecting previously unselected package ca-certificates. Preparing to unpack .../ca-certificates_20141019+deb8u4_all.deb ... Unpacking ca-certificates (20141019+deb8u4) ... Setting up libssl1.0.0:armel (1.0.1t-1+deb8u8) ... debconf: unable to initialize frontend: Dialog debconf: (TERM is not set, so the dialog frontend is not usable.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/arm-linux-gnueabi/perl/5.20.2 /usr/local/share/perl/5.20.2 /usr/lib/arm-linux-gnueabi/perl5/5.20 /usr/share/perl5 /usr/lib/arm-linux-gnueabi/perl/5.20 /usr/share/perl/5.20 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.) debconf: falling back to frontend: Teletype Setting up openssl (1.0.1t-1+deb8u8) ... Setting up ca-certificates (20141019+deb8u4) ... debconf: unable to initialize frontend: Dialog debconf: (TERM is not set, so the dialog frontend is not usable.) debconf: falling back to frontend: Readline debconf: unable to initialize frontend: Readline debconf: (Can't locate Term/ReadLine.pm in @INC (you may need to install the Term::ReadLine module) (@INC contains: /etc/perl /usr/local/lib/arm-linux-gnueabi/perl/5.20.2 /usr/local/share/perl/5.20.2 /usr/lib/arm-linux-gnueabi/perl5/5.20 /usr/share/perl5 /usr/lib/arm-linux-gnueabi/perl/5.20 /usr/share/perl/5.20 /usr/local/lib/site_perl .) at /usr/share/perl5/Debconf/FrontEnd/Readline.pm line 7.) debconf: falling back to frontend: Teletype Updating certificates in /etc/ssl/certs... 152 added, 0 removed; done. Processing triggers for libc-bin (2.19-18+deb8u10) ... Processing triggers for ca-certificates (20141019+deb8u4) ... Updating certificates in /etc/ssl/certs... 0 added, 0 removed; done. Running hooks in /etc/ca-certificates/update.d....done. Removing intermediate container e51ea5c31ad7 ---\u0026gt; 1dccf46769e7 Step 3/6 : ENV HOME /tmp ---\u0026gt; Running in 45de8c047bd3 Removing intermediate container 45de8c047bd3 ---\u0026gt; baa6d8b15164 Step 4/6 : COPY _dist/linux-arm / ---\u0026gt; 59a6cc8bfdbe Step 5/6 : EXPOSE 44134 ---\u0026gt; Running in d2c6d9b3a3bc Removing intermediate container d2c6d9b3a3bc ---\u0026gt; 28a331217c52 Step 6/6 : CMD [\u0026quot;/tiller\u0026quot;] ---\u0026gt; Running in 9760ea4dc6c5 Removing intermediate container 9760ea4dc6c5 ---\u0026gt; 9384b4f39ab3 Successfully built 9384b4f39ab3 Successfully tagged jbrette/tiller-arm32v7:2.9  Cross Compiling and Image cration from Travis-CI  WIP  Install Tiller and Helm on RPI Deploy Tiller POD Let\u0026rsquo;s install tiller first. Note that tiller Docker image also contains the helm binary\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/tiller/ kubectl create -f tiller-serviceaccount.yaml kubectl create -f tiller.yaml  Check the deployment state\nkubectl get all -n kube-system NAME READY STATUS RESTARTS AGE pod/tiller-deploy-b59fcc885-66l7s 1/1 Running 0 5m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/tiller-deploy 1 1 1 1 5m NAME DESIRED CURRENT READY AGE replicaset.apps/tiller-deploy-b59fcc885 1 1 1 5m  Check the logs\nkubectl logs pod/tiller-deploy-b59fcc885-66l7s -n kube-system [main] 2018/07/15 18:08:10 Starting Tiller 2.9 (tls=false) [main] 2018/07/15 18:08:10 GRPC listening on :44134 [main] 2018/07/15 18:08:10 Probes listening on :44135 [main] 2018/07/15 18:08:10 Storage driver is ConfigMap [main] 2018/07/15 18:08:10 Max history per release is 0  Init the Helm client side locate the node running the tiller pod\nssh \u0026lt;nodewithtillerip\u0026gt; docker cp \u0026lt;tillercontainerid\u0026gt;:/helm . scp helm \u0026lt;masterpid\u0026gt;:/home/pirate/helm rm helm  Back on the master PI\nchmod 755 helm mv helm /usr/bin/helm  Initialize client side of Helm\nhelm init --client-only Creating /home/rpiuser/.helm Creating /home/rpiuser/.helm/repository Creating /home/rpiuser/.helm/repository/cache Creating /home/rpiuser/.helm/repository/local Creating /home/rpiuser/.helm/plugins Creating /home/rpiuser/.helm/starters Creating /home/rpiuser/.helm/cache/archive Creating /home/rpiuser/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /home/rpiuser/.helm. Not installing Tiller due to 'client-only' flag having been set Happy Helming!  Use helm on RPI Let\u0026rsquo;s access the simple helm repo designed for\nhelm repo add kubedge1 'https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1/' helm repo update  helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts kubedge1 https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1/  helm search kubedge1 NAME CHART VERSION APP VERSION DESCRIPTION kubedge1/kubeplay-arm32v7 0.1.0 0.1.0 A Helm chart for Kubernetes  Install simple hello world helm chart from kubeplay\nhelm install --name demo kubedge1/kubeplay-arm32v7 kubectl get all  Let\u0026rsquo;s open a browser towards the URL on http://192.168.2.1:kubeplay_NortPort:\nConclusion  Need to cleanup the helm chart and docker image to ensure consistency of the internal Need to improve the NodePort/ClusterIP/LoadBalancer handling.  Reference Links  Usage of remote API "
},
{
	"uri": "/pi_cluster/maintenance/children/2018-07-11-a/",
	"title": "Use github repo as helm chart repository",
	"tags": ["github", "helm", "kubernetes"],
	"description": "",
	"content": "In order to be able use Helm charts the \u0026ldquo;normal\u0026rdquo; way, it is need to buid your own helm repository. The goal of this post is to transform a github repo into a helm repo.\n\nKey Aspects  Save the helm charts on github mainly for the RPI Kubernetes cluster Figure out a way to access them  Build the chart and upload it to the helm repo For that purpose I used by kubeplay repo where I had helm charts and container ready\ngit clone -b arm32v7 git@github.com:jbrette/kubeplay.git cd kubeplay/ helm package charts/kubeplay-arm32v7/ --app-version 0.1.0 --destination ./helmrepo/ cd helmrepo/ helm repo index . git add . git commit -m \u0026quot;New chart version\u0026quot; git push  Use helm repo and chart On the ARM32V7 KUBEDGE Cluster Let\u0026rsquo;s access the simple helm repo designed for arm32v7\nhelm repo add kubedge1 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1\u0026quot; helm repo add kubedge2 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2\u0026quot; helm repo add kubedge3 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge3\u0026quot; helm repo add kubedge4 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge4\u0026quot; helm repo update  $ helm repo list NAME URL incubator https://kubernetes-charts-incubator.storage.googleapis.com/ stable https://kubernetes-charts.storage.googleapis.com/ hack4easy https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy/ kubedge1 https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1/ kubedge2 https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2/ kubedge3 https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge3/ kubedge4 https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge4/  helm search kubedge1 NAME CHART VERSION APP VERSION DESCRIPTION kubedge1/kubedge-arm32v7 0.1.0 0.1.0 A Helm chart for Kubernetes kubedge1/kubeplay-arm32v7 0.1.0 0.1.0 A Helm chart for Kubernetes kubedge1/kubesim-blinkt-arm32v7 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-linkio-arm32v7 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-nats-pub-arm32v7 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-nats-sub-arm32v7 0.1.0 0.1.0 A Helm chart for Kubedge Simulator  On the ARM64V8 KUBEDGE Cluster The Helm charts for ARM64V8 KUBEDGE Cluster contains arm64v8 in their paths and names\nhelm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts kubedge1 https://raw.githubusercontent.com/kubedge/helmrepos/arm64v8/kubedge1 kubedge2 https://raw.githubusercontent.com/kubedge/helmrepos/arm64v8/kubedge2 kubedge3 https://raw.githubusercontent.com/kubedge/helmrepos/arm64v8/kubedge3 kubedge4 https://raw.githubusercontent.com/kubedge/helmrepos/arm64v8/kubedge4 hack4easy https://raw.githubusercontent.com/kubedge/helmrepos/arm64v8/hack4easy  helm search kubedge1 NAME CHART VERSION APP VERSION DESCRIPTION kubedge1/kubedge-arm64v8 0.1.0 0.1.0 A Helm chart for Kubernetes kubedge1/kubeplay-arm64v8 0.1.0 0.1.0 A Helm chart for Kubernetes kubedge1/kubesim-blinkt-arm64v8 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-linkio-arm64v8 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-nats-pub-arm64v8 0.1.0 0.1.0 A Helm chart for Kubedge Simulator kubedge1/kubesim-nats-sub-arm64v8 0.1.0 0.1.0 A Helm chart for Kubedge Simulator  On the KUBEDGESDK The Helm charts for KUBEDGE AMD64 Cluster contains arm64v8 in their paths and names\nhelm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts kubedge1 https://raw.githubusercontent.com/kubedge/helmrepos/amd64/kubedge1 kubedge2 https://raw.githubusercontent.com/kubedge/helmrepos/amd64/kubedge2 kubedge3 https://raw.githubusercontent.com/kubedge/helmrepos/amd64/kubedge3 kubedge4 https://raw.githubusercontent.com/kubedge/helmrepos/amd64/kubedge4 hack4easy https://raw.githubusercontent.com/kubedge/helmrepos/amd64/hack4easy  Conclusion  Still has to find how to get Travis-CI to build the chart automatically  Reference Links  From hackernoon "
},
{
	"uri": "/pi_cluster/docker_kubernetes/children/2018-07-08-a/",
	"title": "Add Persistency Volume to PI Cluster",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes\n\nProcedures The procedure assumes that you have 32G SD Card that you can partition into one 16G disk and seven 7G disks.\nPartition the SD cards Use the Rescue dongle to create the partition. The ideal solution would be to update the cloud-init of the HypriotOS to do that automatically.\nMount the disks Mounting the partitions, involves creating new directories on each node and adding \u0026ldquo;.mount\u0026rdquo; files under systemd This kind of done automatically under by the mount-disks playbook\ncd $HOME/mgt/ ansible picluster -i inventory/ -m shell -a \u0026quot;df -kh\u0026quot; ansible-playbook -i inventory/ playbooks/mount_disks.yaml ansible picluster -i inventory/ -m shell -a \u0026quot;df -kh\u0026quot;  Create the Persistency Volumes in Kubernetes The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\nBecause we did not convert the go code to create the volumes automatically from the /mnt/disks/\u0026hellip;, we have to kind of create the PV by hand.\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd $HOME/kube-deployment/local-storage kubectl apply -f local-storageclass.yaml kubectl apply -f admin_account.yaml kubectl apply -f kubemaster-pi-volumes.yaml kubectl apply -f kube-node01-volumes.yaml kubectl apply -f kube-node02-volumes.yaml kubectl apply -f kube-node03-volumes.yaml kubectl apply -f kube-node04-volumes.yaml  Results    Reference Links  TBD "
},
{
	"uri": "/pi_cluster/os_installation/children/master_pi/",
	"title": "OS Installation on Master PI",
	"tags": ["ansible", "hypriot", "rpi"],
	"description": "",
	"content": "This page describes simple steps to install the OS (HypriotOS) on the PIs.\n\nOverview Kubedge uses HypriotOS because the quickest to set up. SSH and Docker supported by default\nEven if processor is 64bits, OS is still 32bits. (Memory is small anyway). Removed cloud-init once the site was up.\n  For simple arm32v7 OS, download the operating system from HypriotOS ARM32V7 For more complex arm64 OS, download the operating system from HypriotOS ARM64V8 Flash all 3 or 5 SD cards using Win32DiskImager or similar. It takes around 30 seconds per card.  OS on master on master PI Do not power up the slaves nodes for right now. Either plug your home router directly to the master PI, or to the switch. The goal here is to get an IP address allocated by your home router to the master PI.\n Access the node  Insert the SD card in the master PI. Access your home router and look for \u0026ldquo;black-pearl\u0026rdquo; IP address. SSH to the node using cygwin, putty or moba-xterm for instance. The credentials are pirate/hypriot.  Freeze your configuration Cloud init is perfect for the first boot. Once the node is up, it can be challenging not to preserve the fine tuning done to the OS.\nsudo apt-get remove --purge cloud-init sudo apt-get autoremove  Update OS Let\u0026rsquo;s update to the latest version\nsudo apt-get update sudo apt-get upgrade  Gain access to latest docker-ce\nsudo -i curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh usermod -aG docker pirate  At the time this article is written 18.09 is not supported yet.\nsudo apt-cache policy docker-ce sudo apt-get remove --purge docker-ce sudo apt-get autoremove sudo apt-get install docker-ce=18.06.1~ce~3-0~debian  Update master PI name As root, replace black-pearl by kubemaster-pi, in the two following files:\nsudo -i vi /etc/hosts  It seems on HypriotOS 64, you need to do\nsudo hostnamectl set-hostname kubemaster-pi  Configure the pirate account If you prefer to edit using vi instead of nano\nvi /etc/environment EDITOR=vi  Then you need to set up the ssh keys Either create a new ssh key (id_rsa, id_rsa.pub) and copy in id_rsa.pub into GitHub You also need to add those keys in GerritHub\nssh-keygen  or install your private and public key into the /home/pirate/.ssh directory (The same key you registered in GitHub)\nscp id_rsa pirate@\u0026lt;PI\u0026gt;:/home/pirate/.ssh/id_rsa scp id_rsa.pub pirate@\u0026lt;PI\u0026gt;:/home/pirate/.ssh/id_rsa.pub  Install GIT Since you have ethernet access, it is a good time to install GIT in order to download usefull scripts from github.com\nsudo apt-get update sudo apt-get install git sudo apt-get install git-review ```bash $ mkdir -p $HOME/proj/kubedge $ cd $HOME/proj/kubedge  create a cloneit1.sh file, edit it and run it\nIf you deployed on ARM64, be sure to replace arm32v7 by arm64v8\n #!/bin/bash GOODPATH=`pwd` USERID=yourgithubaccount for i in helmrepos kubeplay kubesim_5gc kubesim_base kubesim_elte kubesim_epc kubesim_lte kubesim_nr kubesim_blinkt kubesim_nats kubesim_linkio kubedge_utils do echo \u0026quot;======================================================\u0026quot; echo $i echo \u0026quot;======================================================\u0026quot; # If you want to contribute # git clone -b arm32v7 ssh://$USERID@review.gerrithub.io:29418/kubedge/$i \u0026amp;\u0026amp; scp -p -P 29418 $USERID@review.gerrithub.io:hooks/commit-msg $i/.git/hooks/ # If you want to just pull the code # git clone -b arm32v7 git@github.com:kubedge/$i.git cd $GOODPATH done  create a cloneit2.sh file, edit it and run it\n#!/bin/bash GOODPATH=`pwd` USERID=yourgithubaccount for i in kube-rpi ansible-kube-rpi do echo \u0026quot;======================================================\u0026quot; echo $i echo \u0026quot;======================================================\u0026quot; # If you want to contribute # git clone ssh://$USERID@review.gerrithub.io:29418/kubedge/$i \u0026amp;\u0026amp; scp -p -P 29418 $USERID@review.gerrithub.io:hooks/commit-msg $i/.git/hooks/ # If you want to just pull the code # git clone git@github.com:kubedge/$i.git cd $GOODPATH done  You know have access especially through kube-rpi and ansible-kube-rpi to example files and ansible playbook usefull to install your first node: cd $HOME/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi\n Reference Links  HypriotOS Video2 "
},
{
	"uri": "/lte_to_5g/rpi_bluetooth_pan/",
	"title": "Bluetooth PAN",
	"tags": [],
	"description": "This is Bluetooth PAN tutorial page",
	"content": " Bluetooth PAN sequenceDiagram participant UE_PC participant ENODEB_WorkerPI participant EPC_MasterPI participant MPLSNetwork UE_PC-ENODEB_WorkerPI: Establish RAN Connection ENODEB_WorkerPI-UE_PC: Return IP UE_PC-ENODEB_WorkerPI: pan0/ran IP traffic loop Linux NAT ENODEB_WorkerPI-ENODEB_WorkerPI: translate pan0 IP into eth0 IP end ENODEB_WorkerPI-EPC_MasterPI: eth0/backhaul IP traffic loop Linux NAT EPC_MasterPI-EPC_MasterPI: translate eth0 IP into wlan0 IP end EPC_MasterPI-MPLSNetwork: wlan0/core IP traffic MPLSNetwork-EPC_MasterPI: IP traffic EPC_MasterPI-ENODEB_WorkerPI: IP traffic ENODEB_WorkerPI-UE_PC: IP traffic  We are currently using NAT here to keep the simulation simple. It would be more accurate to use Linux Briding or OVS to simulate the fact that the SGW running on the EPC is \u0026ldquo;allocating\u0026rdquo; the IP address.\n  Setting up PAN  This tutorial how to setup PAN on the slave pi to simulator LTE and eLTE RAN.\n\n Setting up DHCPD on LTE \u0026amp; eLTE Node  This tutorial how to setup DHCPD and NAT so that the UE can access the internet through the AP \u0026amp; NAT running on LTE \u0026amp; eLTE simulating PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to UE or PCs.\n\n "
},
{
	"uri": "/devops/cicd/",
	"title": "CI/CD",
	"tags": [],
	"description": "This is devops/cicd tutorial page",
	"content": " CI/CD  Setting up DockerHub  This tutorial describes how to leverage DockerHub.com to save docker images for AMD64 and ARM32v7.\n\n Setting up Travis-CI  Leverage Travis-CI for automatic build and integration\n\n "
},
{
	"uri": "/nfv/golang/",
	"title": "GO Networking",
	"tags": [],
	"description": "This is nfv/fd_io_and_opnfv tutorial page",
	"content": " GOLANG based networking  GOLANG  GOLANG based Networking libraries usable in micro services in docker container to simulate VNF.\n\n DPDK \u0026amp; FD.io  DPDK and FD.io are providing some kind of support for ARM.\n\n Rebuild Calico for ARM32V7  Neither calico nor canal seems to be available for usage yet on ARM32V7 for PI. The attempt here is to cross-compile the calico containers and use them on the PI cluster. Since calico\u0026rsquo;s role is mainly to setup ip table to get Kubernetes POD-POD communication established, the likelyhood of being able to reuse some of calico go for a VNF are really slim.\n\n "
},
{
	"uri": "/pi_cluster/advanced/children/os_64_bits/",
	"title": "Installing Hypriot OS",
	"tags": ["kubernetes", "security", "64", "rpi"],
	"description": "",
	"content": "Also ARM processor on the Raspberry PI 3B+ is a 64 bit processor, because the board is only equipped with 1G of RAM, a 64 bit operating system is not really needed except when\u0026hellip;.\n\nKey Aspects  The goal is to deploy HypriotOS 64 The key target applications are:  DPDK CEPH BlockChain   The OS One version of HypriotOS seems to be available here.\n 64BitImage  Everything Else  Upgrade all the Travis-CI and branches to support arm64v8 Rebuild all the docker images for the new architecture.  The real hard stuff  Get DPDK to work on ARM64. Get CEPH to work on ARM64.  Reference Links  GitHub "
},
{
	"uri": "/pi_cluster/docker_kubernetes/",
	"title": "Kubernetes",
	"tags": [],
	"description": "This is pi_cluster/docker_kubernetes tutorial page",
	"content": " Docker and Kubernetes  Creating a Raspberry 3 B\u0026#43; Kubernetes Cluster  Also GCE is perfect to learn Kubernetes, building Kubernetes on top of PI Cluster brings another dimension to the learning, from setting up the OS, partitionning the OS, DHCP, NAT, cross compiling for the ARM32V7.\n\n Add Persistency Volume to PI Cluster  In order to install Prometheus, NATS, Cassandra using Kubernetes, we need to first create Persistency Volumes\n\n Deploy Flannel in PI Cluster  In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).\n\n Add Raspberry PI node to Kubernetes Cluster in 10 min  During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.\n\n Enable docker Remote API  In order to build image for Raspberry PI, it is sometimes usefull to be able to leverate infrastruture from a remote VM. For instance you can cross-build golang executable for ARM32v7 and transfer it to build an image on the remote PI. (used for Tiller docker image)\n\n "
},
{
	"uri": "/pi_cluster/",
	"title": "RPI Cluster",
	"tags": [],
	"description": "This is pi_cluster tutorial page",
	"content": "Rapsberry PI Cluster The assembly of the cluster itself brings its set of lessons. What to be carreful of when you purchase your power supply, the length of your ethernet cables or power supply cables. Differences between the PI 3B+ vs PI 3B (1Gb ethernet instead of 100Mb) have to be accounted when you pick up your switch. Account for having 1 additonal port available on your switch to be able to plug your PC to cluster using Ethernet.\nNext step is to install the OS. Immediatly you will realize that it is unpracticle to plug each node one after the other to an HDMI screen, USB mouse and keyboard in order to perform the first initalization. Some of the OS such HyperiotOS have the great idea to preconfigure the node with SSH enabled, docker. This makes every plug and play: Flush your SD card, connect the PI to your home router and voila you just have to SSH.\nThen you will have to learn how to create a NAT and DHCP server on your master node.\nThe bigger your cluster the quicker you will realize that some of the operations are repetitiv, hence the need for automation. The easiet one to learn is ansible. Put if you install everything on the OS directly, then start the issue of maintaining the OS. Because HyperiotOS comes by default with docker, which brings us back to lesson 1: Have everything has a container.\nThe defacto tools to manage a multi node docker cluster is currently kubernetes. By using kubeadm the installation is quite simple as long as you pay attention that the images pulled by kubernetes are actually the one for PI and not the one for your usual cloud.\nFor people with Kubernetes experience (for instance on GCP), this is where the real lessons start: - A lot of the software is not available by default on PI. How do you recompile calico, tiller\u0026hellip;for the PI. - It is much easier to install components using the Kubernetes HELM but a lot of the helm charts are pulling the AMD64 version of the ARM image. - Finally, how do you create your own helm chart repository using github so that you can run a clean helm add repo and helm install commands.\nAnd mainly, you will started to appreciate go and the new Java 9 modules to actually create true microservices Because both langugage have the ability to create standalone executables, which in turn allow to create containers starting from \u0026ldquo;scratch\u0026rdquo;. Compare it with a python container and you will understand very quickly why it is beneficial. The PI only has 1G of RAM\u0026hellip;which helps to understand the interest of having slim true microservices.\n\n Operating System  This is pi_cluster/os_installation tutorial page\n Kubernetes  This is pi_cluster/docker_kubernetes tutorial page\n Helm \u0026amp; Maintenance  This is pi_cluster/maintenance tutorial page\n Applications  This is pi_cluster/docker_kubernetes tutorial page\n Advanced \u0026amp; WIP  Advanced tutorials and Work In Progress\n "
},
{
	"uri": "/pi_cluster/applications/children/kubesim_blinkt/",
	"title": "Installing Kubedge Blinkt",
	"tags": ["kubernetes", "kubedge", "blinkt", "rpi"],
	"description": "",
	"content": "Install Kubedge Blinkt Application on the PI Cluster\n\n  Key Aspects  Install Blinkt Test access to Blinkt  Deploy Using helm helm repo add kubedge1 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge1\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where Blinkt is installed\nFor instance:\nkubectl label nodes kubemaster-pi blinktInstalled=true kubectl label nodes home-pi blinktInstalled=true kubectl label nodes nas-pi blinktInstalled=true  Direct installation from the repo If you feel lucky:\nhelm install kubedge1/kubesim-blinkt-arm32v7 --name blinkt  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge1/kubesim-blinkt-arm32v7 tar xvf kubesim-blinkt-arm32v7-0.1.0.tgz cd kubesim-blinkt-arm32v7/ helm install . --name blinkt  Replica Control Ensure the right labels have been applied to the nodes where Blinkt is installed\nhelm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set blinkt.algorithm=blinkt5 --set replicaCount=2 helm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set blinkt.algorithm=blinkt5 --set replicaCount=3 helm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set blinkt.algorithm=blinkt5 --set replicaCount=0  Rolling Upgrade Use blinkt coloring to highlight rolling upgrade\nhelm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set replicaCount=3 --set blinkt.release=green helm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set replicaCount=3 --set blinkt.release=blue helm upgrade blinkt kubedge1/kubesim-blinkt-arm32v7 --set image.pullPolicy=IfNotPresent --set replicaCount=3 --set blinkt.release=red  Rollback $ helm history blinkt REVISION UPDATED STATUS CHART DESCRIPTION .... 16 Fri Nov 2 21:33:30 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 17 Sat Nov 3 02:00:18 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 18 Sat Nov 3 02:22:11 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 19 Sat Nov 3 02:23:11 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 20 Sun Nov 4 15:16:33 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 21 Mon Nov 5 00:11:42 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 22 Mon Nov 5 00:11:56 2018 SUPERSEDED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete 23 Mon Nov 5 00:13:41 2018 DEPLOYED kubesim-blinkt-arm32v7-0.1.0 Upgrade complete  $ helm rollback blinkt 18 Rollback was a success! Happy Helming!  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge blinkt release \u0026quot;blinkt\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/lte_to_5g/lte_and_5g_core_simulation/children/kubesim_5gc/",
	"title": "Kubedge 5GC Simulation",
	"tags": ["kubernetes", "kubedge", "5gc", "rpi"],
	"description": "",
	"content": "Install Kubedge 5GC Application on the PI Cluster\n\nKey Aspects  Install 5GC Test access to 5GC  Deploy Using helm helm repo add hack4easy \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where 5GC is installed\nFor instance:\nkubectl label nodes kubemaster-pi kubedgeNodeType=5g-core  Direct installation from the repo If you feel lucky:\nhelm install hack4easy/kubesim-5gc-arm32v7 --name sim-5gc  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch hack4easy/kubesim-5gc-arm32v7 tar xvf kubesim-5gc-arm32v7-0.1.0.tgz cd kubesim-5gc-arm32v7/ helm install . --name sim-5gc  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge sim-5gc release \u0026quot;5gc\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/lte_to_5g/lte_and_5g_ran_simulation/children/kubesim_elte/",
	"title": "Kubedge eLTE Simulation",
	"tags": ["kubernetes", "kubedge", "elte", "rpi"],
	"description": "",
	"content": "Install Kubedge eLTE Application on the PI Cluster\n\nKey Aspects  Install eLTE Test access to eLTE  Deploy Using helm helm repo add hack4easy \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where eLTE is installed\nFor instance:\nkubectl label nodes kubemaster-pi kubedgeNodeType=lte-ran  Direct installation from the repo If you feel lucky:\nhelm install hack4easy/kubesim-elte-arm32v7 --name sim-elte  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch hack4easy/kubesim-elte-arm32v7 tar xvf kubesim-elte-arm32v7-0.1.0.tgz cd kubesim-elte-arm32v7/ helm install . --name sim-elte  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge sim-elte release \u0026quot;elte\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/lte_to_5g/rpi_wifi_ap/children/setup_hostapd/",
	"title": "Setting up HOSTAPD on 5G NR node",
	"tags": ["wpa_supplicant", "rpi"],
	"description": "",
	"content": "This tutorial how to setup WIFI AccessPoint on PI simulating 5G NR node\n\nKey Aspects  Setup /etc/network/interfaces.d/wlan0. Setup /etc/dhcpcd.conf Check /etc/resolv.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n  wlan0 dhcpcd hostapd  Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/lte_to_5g/rpi_bluetooth_pan/children/setup_pan/",
	"title": "Setting up PAN",
	"tags": ["wpa_supplicant", "rpi"],
	"description": "",
	"content": "This tutorial how to setup PAN on the slave pi to simulator LTE and eLTE RAN.\n\nKey Aspects  Setup /etc/network/interfaces.d/pan0. Setup /etc/dhcpcd.conf Check /etc/resolv.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n  pan0 dhcpcd  Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/os_installation/children/setup_wpa_applicant/",
	"title": "Setting up WPA supplicant on Master PI",
	"tags": ["wpa_supplicant", "rpi"],
	"description": "",
	"content": "This tutorial how to setup WIFI on the master pi to connect to different WLAN\n\nKey Aspects  Setup /etc/wpa_supplicant/wpa_supplicant Setup /etc/network/interfaces.d/wlan0 Setup /etc/dhcpcd.conf Setup /etc/resolv.conf.tail Check /etc/resolv.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n Install dhcpcd5. The dhcpcd service controls the start of the interfaces as well as the creation of the /etc/resolv.conf\nInstall DHCPCD sudo apt-get install dhcpcd5  The /etc/dhcpcd.conf (not the /etc/dhcpd/dhcpd.conf) controls the startup of the interfaces. We want the eth0 to be kind of static, the wlan0 to be dynamic\ndiff dhcpcd.conf /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/dhcpcd.conf  Prepare lo and eth0 You can keep 3 files in the /etc/network/interfaces.d: l0, eth0 and wlan0. Note comment out the iptables line at first in the eth0 file. eth0 is static so that the internal IP of the master PI is always 192.168.2.1\ndiff lo /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/network/interfaces.d/lo diff eth0 /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/network/interfaces.d/eth0  Enable WLAN0. In wlan0 the list of the sssid you want to potentially connect to. Needs to match the id_str in the wpa_supplicant.conf\nvi wlan0 vi /etc/wpa_supplicant/wpa_supplicant.conf  Check the IP address. Depending on PI3B or PI3B+, the WLAN network may be different. Use the files in $HOME/proj/kubedge/kube-rpi/config/cluster1/hypriotos/ as examples.\nip a iwconfig sudo ip link set wlan0 up sudo iwlist scan | grep ES sudo iwlist scan | grep ED wpa_passphrase \u0026lt;sommessid\u0026gt; sudo vi /etc/network/interfaces.d/wlan0 sudo vi /etc/wpa_supplicant/wpa_supplicant.conf sudo systemctl restart dhcpcd  At that point the wlan0 should come up and IP 192.168.2.1 should be assigned to the master-pi.\nExamples of files  wpa_supplicant wlan0 dhcpcd  Conclusion At that point you should be able to unplug the eth0 cable, reboot the PI and check in your router for a kubemaster-pi and ssh into the node. It should allow you access the kubemaster-pi using WIFI.\nWIP\n Reference Links WIP\n"
},
{
	"uri": "/nfv/golang/children/calico/",
	"title": "Rebuild Calico for ARM32V7",
	"tags": ["kubernetes", "calico", "cni"],
	"description": "",
	"content": "Neither calico nor canal seems to be available for usage yet on ARM32V7 for PI. The attempt here is to cross-compile the calico containers and use them on the PI cluster. Since calico\u0026rsquo;s role is mainly to setup ip table to get Kubernetes POD-POD communication established, the likelyhood of being able to reuse some of calico go for a VNF are really slim.\n\nKey Aspects  Rebuild the Calico  Build Kubernetes executables for ARM32V7 WIP\n Conclusion WIP\n Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/pi_cluster/docker_kubernetes/children/2018-07-14-a/",
	"title": "Deploy Flannel in PI Cluster",
	"tags": ["kubernetes", "rpi", "cni"],
	"description": "",
	"content": "In order to get the nodes and pods interface with each other accross the cluster. This post describes how I deployed Flannel acounting with the fact that some of the nodes have multiple interfaces (wlan0 and eth0).\n\nKey Aspects  Flannel seems to deploy ok. Looks like in trouble when multiple interfaces available Calico in not compiled by default for Rapsberry PI  Flannel Setup through kubectl The kubectl deployment file for flannel and adapted to kubedge is available here: Note: Be sure to have picked the right branch (arm32v7 or arm64v8) when pulling kube-deployment\ncd $HOME cp -r proj/kubedge/kubedge_utils/kube-deployment/ . cd kube-deployment/flannel/ kubectl apply -f flannel.yaml ### Flannel Issue 1: flannel.1. Link has incompatible address on master-pi, both the WLAN and LAN interfaces were activated. After unplugging the CAT5, behavior was similar. Moreover this had some impact on the kube-apiserver (see the number of restarts). ```bash $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-9w97m 1/1 Running 0 4h kube-system pod/coredns-78fcdf6894-cw5p8 1/1 Running 15 10d kube-system pod/coredns-78fcdf6894-czjcj 1/1 Running 15 10d kube-system pod/etcd-master-pi 1/1 Running 11 10d kube-system pod/kube-apiserver-master-pi 1/1 Running 599 10d kube-system pod/kube-controller-manager-master-pi 1/1 Running 38 10d kube-system pod/kube-flannel-ds-bhllh 1/1 Running 13 9d kube-system pod/kube-flannel-ds-q7cp2 0/1 CrashLoopBackOff 401 9d kube-system pod/kube-flannel-ds-wqxsz 1/1 Running 16 9d kube-system pod/kube-proxy-4chwh 1/1 Running 9 9d kube-system pod/kube-proxy-6r5mn 1/1 Running 5 9d kube-system pod/kube-proxy-vvj6j 1/1 Running 11 10d kube-system pod/kube-scheduler-master-pi 1/1 Running 13 10d kube-system pod/kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 20 7d kube-system pod/tiller-deploy-b59fcc885-66l7s 1/1 Running 0 6h  $ kubectl logs pod/kube-flannel-ds-q7cp2 -n kube-system I0716 00:42:46.596796 1 main.go:474] Determining IP address of default interface I0716 00:42:46.598043 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:42:46.598138 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:42:46.775936 1 kube.go:283] Starting kube subnet manager I0716 00:42:46.775907 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:42:47.776280 1 kube.go:137] Node controller sync successful I0716 00:42:47.776400 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:42:47.776431 1 main.go:237] Installing signal handlers I0716 00:42:47.776697 1 main.go:352] Found network config - Backend type: vxlan I0716 00:42:47.776900 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0716 00:42:47.778884 1 main.go:279] Error registering network: failed to configure interface flannel.1: link has incompatible addresses. Remove additional addresses and try again.... I0716 00:42:47.778991 1 main.go:332] Stopping shutdownHandler...  Deleting the pod, did not help. After recreation same issue reappeared.\n$ kubectl delete pod/kube-flannel-ds-q7cp2 -n kube-system  $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 0/1 Error 1 17s  Deleting the interface flannel.1 interface actually worked:\n$ sudo ip link delete flannel.1  $ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/kube-flannel-ds-z7w4f 1/1 Running 5 3m  $ kubectl logs pod/kube-flannel-ds-z7w4f -n kube-system I0716 00:52:14.555290 1 main.go:474] Determining IP address of default interface I0716 00:52:14.564490 1 main.go:487] Using interface with name wlan0 and address 192.168.1.95 I0716 00:52:14.564578 1 main.go:504] Defaulting external address to interface address (192.168.1.95) I0716 00:52:14.802491 1 kube.go:130] Waiting 10m0s for node controller to sync I0716 00:52:14.802544 1 kube.go:283] Starting kube subnet manager I0716 00:52:15.803114 1 kube.go:137] Node controller sync successful I0716 00:52:15.803308 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - master-pi I0716 00:52:15.803909 1 main.go:237] Installing signal handlers I0716 00:52:15.804662 1 main.go:352] Found network config - Backend type: vxlan I0716 00:52:15.804985 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0716 00:52:15.875242 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0716 00:52:15.875367 1 main.go:303] Running backend. I0716 00:52:15.875489 1 main.go:321] Waiting for all goroutines to exit I0716 00:52:15.875559 1 vxlan_network.go:56] watching for new subnet leases  Flannel issue 2: Multiple interfaces Some of the PI have two interfaces running: wlan0 and eth0. The internal cluster network is using eth0. We need to force Flannel to use it.\nSetup through kubectl Realize I was using v0.9.1 instead of v0.10.0. Let\u0026rsquo;s update the file\n$ mkdir -p $HOME/kube-deployments/flannel $ cd $HOME/kube-deployments/flannel $ curl -sSL https://rawgit.com/coreos/flannel/v0.10.0/Documentation/kube-flannel.yml | sed \u0026quot;s/amd64/arm/g\u0026quot; \u0026gt; flannel.yaml  Let\u0026rsquo;s add \u0026ndash;iface=eth0 to the flanneld to in the flannel.yaml\n Let\u0026rsquo;s update flannel from 0.9.1 to 0.10.0 at the same time we specify which interface to use.\n$ kubectl apply -f flannel.yaml clusterrole.rbac.authorization.k8s.io/flannel configured clusterrolebinding.rbac.authorization.k8s.io/flannel configured serviceaccount/flannel unchanged configmap/kube-flannel-cfg configured daemonset.extensions/kube-flannel-ds configured  It seems it solved the flannel issue. The bug in kube 1.11.0 still there (restart of kube-apiserver) Will update to 1.11.1 when it is published\n$ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/coredns-78fcdf6894-bn6wl 1/1 Running 0 6d kube-system pod/coredns-78fcdf6894-k52xb 1/1 Running 0 6d kube-system pod/etcd-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-apiserver-kubemaster-pi 1/1 Running 3 6d kube-system pod/kube-controller-manager-kubemaster-pi 0/1 CrashLoopBackOff 1740 6d kube-system pod/kube-flannel-ds-62fz9 1/1 Running 984 6d kube-system pod/kube-flannel-ds-gwzdt 1/1 Running 0 6d kube-system pod/kube-flannel-ds-h7ln5 1/1 Running 0 6d kube-system pod/kube-flannel-ds-qs9lf 1/1 Running 0 6d kube-system pod/kube-flannel-ds-vwsjk 1/1 Running 0 6d kube-system pod/kube-proxy-45z5s 1/1 Running 0 6d kube-system pod/kube-proxy-4trsd 1/1 Running 0 6d kube-system pod/kube-proxy-ksj7c 1/1 Running 4 6d kube-system pod/kube-proxy-t7gmc 1/1 Running 0 6d kube-system pod/kube-proxy-tfmqb 1/1 Running 0 6d kube-system pod/kube-scheduler-kubemaster-pi 1/1 Running 4 6d NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 6d kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 6d NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/kube-flannel-ds 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d kube-system daemonset.apps/kube-proxy 5 5 5 5 5 beta.kubernetes.io/arch=arm 6d NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/coredns 2 2 2 2 6d NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/coredns-78fcdf6894 2 2 2 6d  $ kubectl logs pod/kube-flannel-ds-62fz9 -n kube-system I0714 14:34:26.719081 1 main.go:474] Determining IP address of default interface I0714 14:34:26.728184 1 main.go:487] Using interface with name wlan0 and address 192.168.1.94 I0714 14:34:26.728273 1 main.go:504] Defaulting external address to interface address (192.168.1.94) I0714 14:34:26.942686 1 kube.go:283] Starting kube subnet manager I0714 14:34:26.943203 1 kube.go:130] Waiting 10m0s for node controller to sync I0714 14:34:27.943672 1 kube.go:137] Node controller sync successful I0714 14:34:27.943771 1 main.go:234] Created subnet manager: Kubernetes Subnet Manager - kubemaster-pi I0714 14:34:27.943819 1 main.go:237] Installing signal handlers I0714 14:34:27.944064 1 main.go:352] Found network config - Backend type: vxlan I0714 14:34:27.944222 1 vxlan.go:119] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false I0714 14:34:28.004675 1 main.go:299] Wrote subnet file to /run/flannel/subnet.env I0714 14:34:28.004748 1 main.go:303] Running backend. I0714 14:34:28.004880 1 main.go:321] Waiting for all goroutines to exit I0714 14:34:28.004931 1 vxlan_network.go:56] watching for new subnet leases I0714 14:34:28.049933 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.050202 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.053918 1 iptables.go:114] Some iptables rules are missing; deleting and recreating rules I0714 14:34:28.054003 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.057332 1 iptables.go:136] Deleting iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.061665 1 iptables.go:136] Deleting iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.066452 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -j ACCEPT I0714 14:34:28.069910 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.075067 1 iptables.go:136] Deleting iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE I0714 14:34:28.078310 1 iptables.go:124] Adding iptables rule: -d 10.244.0.0/16 -j ACCEPT I0714 14:34:28.082389 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 -d 10.244.0.0/16 -j RETURN I0714 14:34:28.098375 1 iptables.go:124] Adding iptables rule: -s 10.244.0.0/16 ! -d 224.0.0.0/4 -j MASQUERADE I0714 14:34:28.111379 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/24 -j RETURN I0714 14:34:28.122424 1 iptables.go:124] Adding iptables rule: ! -s 10.244.0.0/16 -d 10.244.0.0/16 -j MASQUERADE  Calico Compile Calico for Raspberry PI  WIP  Deploy on Raspberry PI  WIP  Results  WIP  Reference Links  Flannel Issue Flannel Issue2 Flannel Issue3 "
},
{
	"uri": "/nfv/akraino/",
	"title": "Akraino",
	"tags": [],
	"description": "This is nfv/akraino tutorial page",
	"content": " Akraino  "
},
{
	"uri": "/lte_to_5g/lte_and_5g_core_simulation/",
	"title": "EPC and 5GC",
	"tags": [],
	"description": "This is lte_to_5g/lte_and_5g_core_simulation tutorial page",
	"content": "LTE and 5G Core Kubedege created simulators for LTE EPC and the 5G Core.\n\n Kubedge EPC Simulation  Install Kubedge EPC Application on the PI Cluster\n\n Kubedge 5GC Simulation  Install Kubedge 5GC Application on the PI Cluster\n\n"
},
{
	"uri": "/pi_cluster/maintenance/",
	"title": "Helm &amp; Maintenance",
	"tags": [],
	"description": "This is pi_cluster/maintenance tutorial page",
	"content": "HELM and Maintenance Those tutorials describe how to use Helm and associated tools to perform rolling upgrades of the applications and of Kubernetes\n\n Deply Helm and Tiller on PI Cluster  The main purpose of this exercise is to be able to use Helm on the Rapsberry PI Cluster.\n\n Use github repo as helm chart repository  In order to be able use Helm charts the \u0026ldquo;normal\u0026rdquo; way, it is need to buid your own helm repository. The goal of this post is to transform a github repo into a helm repo.\n\n Upgrade RPI Kubernetes cluster to 1.12  The new Kubernetes 1.12 is out. THe goal is to update my two clusters to 1.12 using kubeadm 1.12\n\n Compile and Test SONOBUOY  Sonobouy, deploys in a Kubernetes cluster and helps to assesse the compliance of that cluster\n\n "
},
{
	"uri": "/devops/hugo_githubpages/",
	"title": "Hugo/GitHubPages",
	"tags": [],
	"description": "This is devops/hugo_githubpages tutorial page",
	"content": " Hugo and GitHub Pages This is a devops/hugo_githubpages tutorial page\n"
},
{
	"uri": "/lte_to_5g/",
	"title": "LTE to 5G Simulations",
	"tags": [],
	"description": "This is lte_to_5g tutorial page",
	"content": "LTE to 5G Transition PI is well adapted to simulate 5G and LTE networks:\n The upstream wifi network from the master PI acts as the core network. You connect to the internet. The master PI is running the LTE EPC and the 5G CORE components. Some of those of components such as SGW Gateway are running on the Master PI. Some secondary PIs are used to simulate the LTE eNODEb and 5G NR nodes. 5G NR nodes (leveraging the Wifi spectrum). The Ethernet cables act as the backhaul network between the EPC and eNodeB/5G NR. a PI enabling its WIFI access point acts 5G NR nodes and spectrum. a PI enabling its Bluetooth as an Personal Area Network acts an LTE or eLTE eNodeB (and spectrum)  \n WIFI AP  This is lte_to_5g/rpi_wifi_bluetooth tutorial page\n Bluetooth PAN  This is Bluetooth PAN tutorial page\n EPC and 5GC  This is lte_to_5g/lte_and_5g_core_simulation tutorial page\n LTE, eLTE and 5G RAN  This is lte_to_5g/lte_and_5g_ran_simulation tutorial page\n NSA vs SA. Option 3  This is lte_to_5g/nsa_and_sa tutorial page\n Advanced \u0026amp; WIP  Advanced tutorials and Work In Progress\n"
},
{
	"uri": "/pi_cluster/maintenance/children/2018-09-28-a/",
	"title": "Upgrade RPI Kubernetes cluster to 1.12",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "The new Kubernetes 1.12 is out. THe goal is to update my two clusters to 1.12 using kubeadm 1.12\n\nMaster node upgrade using kubeadm # apt-mark unhold kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm \u0026amp;\u0026amp; \\ \u0026gt; apt-mark hold kubeadm kubeadm was already not hold. Hit:2 http://raspbian.raspberrypi.org/raspbian stretch InRelease Hit:3 https://download.docker.com/linux/raspbian stretch InRelease Hit:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Hit:5 http://archive.raspberrypi.org/debian stretch InRelease Hit:4 https://packagecloud.io/Hypriot/rpi/debian stretch InRelease Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubeadm 1 upgraded, 0 newly installed, 0 to remove and 38 not upgraded. Need to get 8,095 kB of archives. After this operation, 3,100 kB disk space will be freed. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubeadm armhf 1.12.0-00 [8,095 kB] Fetched 8,095 kB in 4s (1,962 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubeadm_1.12.0-00_armhf.deb ... Unpacking kubeadm (1.12.0-00) over (1.11.1-00) ... Setting up kubeadm (1.12.0-00) ... kubeadm set on hold.  sudo kubeadm upgrade plan [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade] Fetching available versions to upgrade to [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/versions] Latest stable version: v1.12.0 [upgrade/versions] Latest version in the v1.11 series: v1.11.3 Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.11.3 Upgrade to the latest version in the v1.11 series: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.11.3 Controller Manager v1.11.0 v1.11.3 Scheduler v1.11.0 v1.11.3 Kube Proxy v1.11.0 v1.11.3 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.18 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.11.3 _____________________________________________________________________ Components that must be upgraded manually after you have upgraded the control plane with 'kubeadm upgrade apply': COMPONENT CURRENT AVAILABLE Kubelet 3 x v1.11.1 v1.12.0 Upgrade to the latest stable version: COMPONENT CURRENT AVAILABLE API Server v1.11.0 v1.12.0 Controller Manager v1.11.0 v1.12.0 Scheduler v1.11.0 v1.12.0 Kube Proxy v1.11.0 v1.12.0 CoreDNS 1.1.3 1.2.2 Etcd 3.2.18 3.2.24 You can now apply the upgrade by executing the following command: kubeadm upgrade apply v1.12.0 _____________________________________________________________________  $ sudo kubeadm upgrade apply v1.12.0 [preflight] Running pre-flight checks. [upgrade] Making sure the cluster is healthy: [upgrade/config] Making sure the configuration is correct: [upgrade/config] Reading configuration from the cluster... [upgrade/config] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [upgrade/apply] Respecting the --cri-socket flag that is set with higher priority than the config file. [upgrade/version] You have chosen to change the cluster version to \u0026quot;v1.12.0\u0026quot; [upgrade/versions] Cluster version: v1.11.0 [upgrade/versions] kubeadm version: v1.12.0 [upgrade/confirm] Are you sure you want to proceed with the upgrade? [y/N]: y [upgrade/prepull] Will prepull images for components [kube-apiserver kube-controller-manager kube-scheduler etcd] [upgrade/prepull] Prepulling image for component kube-apiserver. [upgrade/prepull] Prepulling image for component kube-controller-manager. [upgrade/prepull] Prepulling image for component kube-scheduler. [upgrade/prepull] Prepulling image for component etcd. [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-etcd [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 0 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-scheduler [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-apiserver [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-kube-controller-manager [apiclient] Found 1 Pods for label selector k8s-app=upgrade-prepull-etcd [upgrade/prepull] Prepulled image for component etcd. [upgrade/prepull] Prepulled image for component kube-apiserver. [upgrade/prepull] Prepulled image for component kube-controller-manager. [upgrade/prepull] Prepulled image for component kube-scheduler. [upgrade/prepull] Successfully prepulled the images for all the control plane components [upgrade/apply] Upgrading your Static Pod-hosted control plane to version \u0026quot;v1.12.0\u0026quot;... Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f [etcd] Wrote Static Pod manifest for a local etcd instance to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/etcd.yaml\u0026quot; [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/etcd.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/etcd.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 00575b778fb80d4e48241f80ceb2ac0f Static pod: etcd-master-pi hash: 77c6076a4d6ee044b744b041125cf918 [apiclient] Found 1 Pods for label selector component=etcd [upgrade/staticpods] Component \u0026quot;etcd\u0026quot; upgraded successfully! [upgrade/etcd] Waiting for etcd to become available [util/etcd] Waiting 0s for initial delay [util/etcd] Attempting to see if all cluster endpoints are available 1/10 [upgrade/staticpods] Writing new Static Pod manifests to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315\u0026quot; [controlplane] wrote Static Pod manifest for component kube-apiserver to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-apiserver.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-controller-manager to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-controller-manager.yaml\u0026quot; [controlplane] wrote Static Pod manifest for component kube-scheduler to \u0026quot;/etc/kubernetes/tmp/kubeadm-upgraded-manifests040184315/kube-scheduler.yaml\u0026quot; [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-apiserver.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-apiserver.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-apiserver-master-pi hash: c30b2fa49c49e091538b2ce8e4dae186 Static pod: kube-apiserver-master-pi hash: a92106d6db4c8b5835a47f5f56c33fdb [apiclient] Found 1 Pods for label selector component=kube-apiserver [upgrade/staticpods] Component \u0026quot;kube-apiserver\u0026quot; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-controller-manager.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-controller-manager.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-controller-manager-master-pi hash: 22f67939f8b1abea8ba99666b78b5c93 Static pod: kube-controller-manager-master-pi hash: 980b4156606df8caafd0ad8abacc1485 [apiclient] Found 1 Pods for label selector component=kube-controller-manager [upgrade/staticpods] Component \u0026quot;kube-controller-manager\u0026quot; upgraded successfully! [upgrade/staticpods] Moved new manifest to \u0026quot;/etc/kubernetes/manifests/kube-scheduler.yaml\u0026quot; and backed up old manifest to \u0026quot;/etc/kubernetes/tmp/kubeadm-backup-manifests-2018-09-30-00-46-56/kube-scheduler.yaml\u0026quot; [upgrade/staticpods] Waiting for the kubelet to restart the component [upgrade/staticpods] This might take a minute or longer depending on the component/version gap (timeout 5m0s Static pod: kube-scheduler-master-pi hash: 0e545194d6b033abd681f02dfd11f4c8 Static pod: kube-scheduler-master-pi hash: 1b5ec5be325bf29f60be62789416a99e [apiclient] Found 1 Pods for label selector component=kube-scheduler [upgrade/staticpods] Component \u0026quot;kube-scheduler\u0026quot; upgraded successfully! [uploadconfig] storing the configuration used in ConfigMap \u0026quot;kubeadm-config\u0026quot; in the \u0026quot;kube-system\u0026quot; Namespace [kubelet] Creating a ConfigMap \u0026quot;kubelet-config-1.12\u0026quot; in namespace kube-system with the configuration for the kubelets in the cluster [kubelet] Downloading configuration for the kubelet from the \u0026quot;kubelet-config-1.12\u0026quot; ConfigMap in the kube-system namespace [kubelet] Writing kubelet configuration to file \u0026quot;/var/lib/kubelet/config.yaml\u0026quot; [patchnode] Uploading the CRI Socket information \u0026quot;/var/run/dockershim.sock\u0026quot; to the Node API object \u0026quot;master-pi\u0026quot; as an annotation [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy [upgrade/successful] SUCCESS! Your cluster was upgraded to \u0026quot;v1.12.0\u0026quot;. Enjoy! [upgrade/kubelet] Now that your control plane is upgraded, please proceed with upgrading your kubelets if you haven't already done so.  Kubernetes servers are running v1.12\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;11\u0026quot;, GitVersion:\u0026quot;v1.11.1\u0026quot;, GitCommit:\u0026quot;b1b29978270dc22fecc592ac55d903350454310a\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-07-17T18:53:20Z\u0026quot;, GoVersion:\u0026quot;go1.10.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T16:55:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;}  my kubelets are still running kubernetes v1.11.1\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.11.1 master-pi Ready master 86d v1.11.1 nas-pi Ready \u0026lt;none\u0026gt; 85d v1.11.1  Upgrade kubectl Install newer version of kubectl using apt-get\n$ sudo apt-get install kubectl Reading package lists... Done Building dependency tree Reading state information... Done The following packages will be upgraded: kubectl 1 upgraded, 0 newly installed, 0 to remove and 37 not upgraded. Need to get 8,639 kB of archives. After this operation, 1,783 kB of additional disk space will be used. Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main armhf kubectl armhf 1.12.0-00 [8,639 kB] Fetched 8,639 kB in 6s (1,270 kB/s) (Reading database ... 30574 files and directories currently installed.) Preparing to unpack .../kubectl_1.12.0-00_armhf.deb ... Unpacking kubectl (1.12.0-00) over (1.11.1-00) ... Setting up kubectl (1.12.0-00) ...  Check that kubectl is now 1.12\n$ kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T17:05:32Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T16:55:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;}  Upgrade kubelet on master node $ sudo apt-get upgrade -y kubelet Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages will be upgraded: ...  $ sudo systemctl restart kubelet  $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.11.1 master-pi Ready master 86d v1.12.0 nas-pi Ready \u0026lt;none\u0026gt; 85d v1.11.1  Looks that as useual something is wrong with flannel CNI\n$ kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE default pod/helm-rpi-kubeplay-arm32v7-6cb66496c6-6gvf6 1/1 Running 0 60d kube-system pod/coredns-576cbf47c7-nzhfj 1/1 Running 0 37m kube-system pod/coredns-576cbf47c7-wkmhr 1/1 Running 0 37m kube-system pod/etcd-master-pi 1/1 Running 0 3m43s kube-system pod/kube-apiserver-master-pi 1/1 Running 2 3m43s kube-system pod/kube-controller-manager-master-pi 1/1 Running 1 3m43s kube-system pod/kube-flannel-ds-4495g 1/1 Running 4 75d kube-system pod/kube-flannel-ds-52ssk 0/1 Error 10 75d kube-system pod/kube-flannel-ds-gj65n 1/1 Running 4 75d Investigation shows: ```bash $ kubectl logs pod/kube-flannel-ds-52ssk -n kube-system I0930 01:34:13.768925 1 main.go:475] Determining IP address of default interface I0930 01:34:13.778622 1 main.go:488] Using interface with name wlan0 and address 192.168.1.95 I0930 01:34:13.778716 1 main.go:505] Defaulting external address to interface address (192.168.1.95) I0930 01:34:14.168318 1 kube.go:131] Waiting 10m0s for node controller to sync I0930 01:34:14.168890 1 kube.go:294] Starting kube subnet manager I0930 01:34:15.169929 1 kube.go:138] Node controller sync successful I0930 01:34:15.170035 1 main.go:235] Created subnet manager: Kubernetes Subnet Manager - master-pi I0930 01:34:15.170064 1 main.go:238] Installing signal handlers I0930 01:34:15.170415 1 main.go:353] Found network config - Backend type: vxlan I0930 01:34:15.170797 1 vxlan.go:120] VXLAN config: VNI=1 Port=0 GBP=false DirectRouting=false E0930 01:34:15.256843 1 main.go:280] Error registering network: failed to configure interface flannel.1: failed to ensure address of interface flannel.1: link has incompatible addresses. Remove additional addresses and try again. \u0026amp;netlink.Vxlan{LinkAttrs:netlink.LinkAttrs{Index:5, MTU:1450, TxQLen:0, Name:\u0026quot;flannel.1\u0026quot;, HardwareAddr:net.HardwareAddr{0x26, 0xd5, 0xd0, 0xdd, 0x56, 0x1a}, ...  Let\u0026rsquo;s apply usual receipe ``bash sudo ip link delete flannel.1\n Brute force fix seems to have done the fix....This is lucky we don't have production traffic on that node. ```bash $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 51m coredns-576cbf47c7-wkmhr 1/1 Running 0 51m etcd-master-pi 1/1 Running 0 17m kube-apiserver-master-pi 1/1 Running 2 17m kube-controller-manager-master-pi 1/1 Running 1 17m kube-flannel-ds-4495g 1/1 Running 4 75d kube-flannel-ds-52ssk 1/1 Running 13 75d kube-flannel-ds-gj65n 1/1 Running 4 75d kube-proxy-c2264 1/1 Running 0 51m kube-proxy-snjsg 1/1 Running 0 49m kube-proxy-zgqjb 1/1 Running 1 50m kube-scheduler-master-pi 1/1 Running 1 17m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d  Update first slave node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet  Same flannel issue\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 87m coredns-576cbf47c7-wkmhr 1/1 Running 1 87m etcd-master-pi 1/1 Running 0 53m kube-apiserver-master-pi 1/1 Running 2 53m kube-controller-manager-master-pi 1/1 Running 1 53m kube-flannel-ds-4495g 0/1 CrashLoopBackOff 10 75d ...  same hack to fix the issue\nsudo ip link delete flannel.1  same hack seems to be effective\n$ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 0 111m coredns-576cbf47c7-wkmhr 1/1 Running 1 111m etcd-master-pi 1/1 Running 0 77m kube-apiserver-master-pi 1/1 Running 2 77m kube-controller-manager-master-pi 1/1 Running 1 77m kube-flannel-ds-4495g 1/1 Running 11 75d ...  Other slave node sudo apt-get clean sudo apt-get update sudo apt-get install kubeadm sudo apt-get install kubelet sudo kubeadm upgrade node config --kubelet-version $(kubelet --version | cut -d ' ' -f 2) sudo systemctl restart kubelet sudo ip link delete flannel.1  Check consistency of the cluster $ kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-nzhfj 1/1 Running 1 143m coredns-576cbf47c7-wkmhr 1/1 Running 1 143m etcd-master-pi 1/1 Running 0 109m kube-apiserver-master-pi 1/1 Running 2 109m kube-controller-manager-master-pi 1/1 Running 1 109m kube-flannel-ds-4495g 1/1 Running 11 76d kube-flannel-ds-52ssk 1/1 Running 13 76d kube-flannel-ds-gj65n 1/1 Running 13 76d kube-proxy-c2264 1/1 Running 1 143m kube-proxy-snjsg 1/1 Running 1 141m kube-proxy-zgqjb 1/1 Running 1 142m kube-scheduler-master-pi 1/1 Running 1 109m kubernetes-dashboard-7d59788d44-rchkk 1/1 Running 25 83d tiller-deploy-b59fcc885-dbvlv 1/1 Running 0 60d  $ kubectl get nodes NAME STATUS ROLES AGE VERSION home-pi Ready \u0026lt;none\u0026gt; 86d v1.12.0 master-pi Ready master 86d v1.12.0 nas-pi Ready \u0026lt;none\u0026gt; 86d v1.12.0  Conclusion  At a glance, the cluster seems to be healthy I still need to find sometool like sonobuoy to validate that the cluster is healthy  Reference Links  Official Upgrade documentation "
},
{
	"uri": "/pi_cluster/applications/children/prometheus/",
	"title": "Installing Prometheus",
	"tags": ["kubernetes", "prometheus", "rpi"],
	"description": "",
	"content": "Install Prometheus on the PI Cluster\n\nKey Aspects  Install Prometheus Access the dashboard for a web browser  Deploy Using helm helm repo add kubedge2 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2\u0026quot; helm repo update  Direct installation from the repo If you feel lucky:\nhelm install kubedge2/prometheus-arm32v7 --name prometheus  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge2/prometheus-arm32v7 tar xvf prometheus-arm32v7-7.3.4.tgz cd prometheus-arm32v7/ helm install . --name prometheus  Source Code The images creation scripts are availble under:\n prometheus alertmanager node-exporter pushgateway config-reload kube-state-metrics  The chart is available under:\n prometheus  Cleanup Run the delete command\nhelm delete --purge prometheus release \u0026quot;prometheus\u0026quot; deleted  Conclusion We still have to be able to create the images automatically using .travis.yml\n Reference Links WIP\n"
},
{
	"uri": "/lte_to_5g/lte_and_5g_ran_simulation/children/kubesim_nr/",
	"title": "Kubedge 5G NR Simulation",
	"tags": ["kubernetes", "kubedge", "nr", "rpi"],
	"description": "",
	"content": "Install Kubedge LTE Application on the PI Cluster\n\nKey Aspects  Install LTE Test access to LTE  Deploy Using helm helm repo add hack4easy \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/hack4easy\u0026quot; helm repo update  Apply the labels to the nodes Ensure the right labels have been applied to the nodes where LTE is installed\nFor instance:\nkubectl label nodes kubemaster-pi kubedgeNodeType=5g-ran  Direct installation from the repo If you feel lucky:\nhelm install hack4easy/kubesim-nr-arm32v7 --name sim-nr  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch hack4easy/kubesim-nr-arm32v7 tar xvf kubesim-nr-arm32v7-0.1.0.tgz cd kubesim-nr-arm32v7/ helm install . --name sim-nr  Source Code The images creation scripts are availble under:\n Repo ApplicationCode Dockerfile Chart  Cleanup $ helm delete --purge sim-nr release \u0026quot;nr\u0026quot; deleted  Conclusion WIP\n Reference Links  Video1 Video2 "
},
{
	"uri": "/lte_to_5g/rpi_wifi_ap/children/setup_dhcpd/",
	"title": "Setting up DHCPD on 5G NR Node",
	"tags": ["dhcpd", "rpi"],
	"description": "",
	"content": "This tutorial how to setup DHCPD and NAT so that the UE can access the internet through the AP \u0026amp; NAT running on 5G NR simulating PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to UE or PCs.\n\nKey Aspects  Setup /etc/dhcp/dhcpd.conf. Setup /etc/default/isc-dhcp-server Setup /etc/systcl.conf Setup /etc/dhcpcd.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n  dhcpd isc-dhcp-server systemctl dhcpcd  Verification On the laptop, under the wifi icon, join a network xxx@kubedge network If the worker PI and the master PI are configured properly (dhcpd, network/eth0,pan0) the worker PI and master PI will acts as routers. The worker PI will provide the laptop with an IP address in the 192.168.0xx.0/255 range.\nSSH to the node.\nssh 192.168.0xx.1 -l pirate  On your PC itself, open a command prompt or cygwin\nipconfig /all  Notice the IP address of the home router 192.168.1.1 in the DNS server list\n If the previous test is successful, the Name Server list should contain the IP and your home router, and you should be able to surf the internet.\nReference Links WIP\n"
},
{
	"uri": "/lte_to_5g/rpi_bluetooth_pan/children/setup_dhcpd/",
	"title": "Setting up DHCPD on LTE &amp; eLTE Node",
	"tags": ["dhcpd", "rpi"],
	"description": "",
	"content": "This tutorial how to setup DHCPD and NAT so that the UE can access the internet through the AP \u0026amp; NAT running on LTE \u0026amp; eLTE simulating PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to UE or PCs.\n\nKey Aspects  Setup /etc/dhcp/dhcpd.conf Setup /etc/default/isc-dhcp-server Setup /etc/systcl.conf Setup /etc/dhcpcd.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n  dhcpd isc-dhcp-server systemctl dhcpcd  Verification WIP: The connection to the PAN is still kind of unstable\n On the laptop, under the bluetook icon, use the Join Personal Area Network. If the worker PI and the master PI are configured properly (dhcpd, network/eth0,pan0) the worker PI and master PI will acts as routers. The worker PI will provide the laptop with an IP address in the 192.168.1xx.0/255 range.\nSSH to the node.\nssh 192.168.1xx.1 -l pirate  On your PC itself, open a command prompt or cygwin\nipconfig /all  If the previous test is successful, the Name Server list should contain the IP and your home router, and you should be able to surf the internet.\nNotice the IP address of the home router 192.168.1.1 in the DNS server list\n Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/os_installation/children/setup_dhcpd/",
	"title": "Setting up DHCPD on master PI",
	"tags": ["dhcpd", "rpi"],
	"description": "",
	"content": "This tutorial how to setup DHCPD and NAT so that the slave PI can access the internet through the NAT (ETH0-\u0026gt;WLAN0) running on Master PI. This tutorial also describe how to setup the DHCPD to automatically allocated IPs to PI or PCs.\n\nKey Aspects  Setup /etc/dhcp/dhcpd.conf. Setup /etc/default/isc-dhcp-server Setup /etc/systcl.conf  Deploy Also the Kubedge team went through the process, it has not been documented yet. Still some example files are available bellow.\n DHCP server We want to install a DHCP server so that the additional PI obtain a IP address in the 192.168.2.1 when connected through the switch.\nsudo apt-get install isc-dhcp-server  Check the setup of the dhcp server\ndiff /etc/default/isc-dhcp-server /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/default/isc-dhcp-server vi /etc/default/isc-dhcp-server  Update the dhcpd.conf. Comment out the fix allocation at first until you have access to the eth0 mac address.\ndiff /etc/dhcp/dhcpd.conf /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/dhcp/dhcpd.conf vi /etc/dhcp/dhcpd.conf  sudo service isc-dhcp-server restart sudo service isc-dhcp-server status  Network Address Translation Check the ipforwarding setup\ndiff /etc/sysctl.conf /home/pirate/proj/kubedge/kube-rpi/config/cluster1/hypriotos/kubemaster-pi/etc/sysctl.conf vi /etc/sysctl.conf  The Master PI acts as a router from eth0 to wlan0 for the other nodes.\nsudo sh -c \u0026quot;echo 1 \u0026gt; /proc/sys/net/ipv4/ip_forward\u0026quot; sudo iptables -t nat -A POSTROUTING -o wlan0 -j MASQUERADE sudo iptables -A FORWARD -i wlan0 -o eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT sudo iptables -A FORWARD -i eth0 -o wlan0 -j ACCEPT sudo sh -c \u0026quot;iptables-save \u0026gt; /etc/iptables.ipv4.nat\u0026quot;  You can go an uncomment the iptable in the eth0\nvi /etc/network/interfaces.d/eth0  Reference Files  dhcpd sysctl.conf isc-dhcp-server  Verification This is where is important to have a 5 ports switch for a 3 nodes cluster , and a 8 ports switch for a 5 nodes cluster.\n sequenceDiagram participant PC participant MasterPI participant Internet PC-MasterPI: Get IP MasterPI-PC: Return IP in 192.168.2.x PC-MasterPI: eth0 IP traffic loop Linux NAT MasterPI-MasterPI: translate eth0 IP into wlan0 IP end MasterPI-Internet: wifi IP traffic Internet-MasterPI: IP traffic MasterPI-PC: IP traffic  Plug your laptop onto the switch using an ethernet cable. If the master PI is configured properly (dhcpd, network/eth0..), the master PI will acts as a router. It will provide the laptop with an IP address in the 192.168.2.0/255 range.\nSSH to the node.\nssh 192.168.2.1 -l pirate  On your PC itself, open a command prompt or cygwin\nipconfig /all  If the previous test is successful, the Name Server list should contain the IP and your home router, and you should be able to surf the internet.\nReference Links WIP\n"
},
{
	"uri": "/pi_cluster/docker_kubernetes/children/2018-06-19-a/",
	"title": "Add Raspberry PI node to Kubernetes Cluster in 10 min",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "During some of the manipulation of the partition table of my SD card, I ended up screwing up both my SD card and my backup Win32DiskImage backup. Moreover if your SD card is 32G, it takes around 30 minute to restore from backup. Hence the idea to come up with a way to build more resiliency in the cluster. Recreating a node from scratch should not take more than 10 mn. The propose procedure is still rather long because I did not push enough yet what the HypriotOS team, aka build a default SD image where cloud-init does 100% of the initialization work.\n\nBase OS Flash HypriotOS to SD and reboot Pi.\nOS Flash the SD Card with HypriotOS Connect Pi through to the cluster switch  Freeze the new node IP Access the Master PI, run arp -a and find the new IP. Freeze the new IP in the dhcpcd.conf\nssh \u0026lt;masterip\u0026gt; arp -a vi /etc/dhcp/dhcpd.conf  Ssh to the new node Access the node from the Master PI:\nssh 192.168.2.xxx docker ps  Basic hostname Freeze your configuration\nsudo apt-get remove --purge cloud-init sudo apt-get autoremove  Set the host name\nsudo vi /etc/hosts sudo hostnamectl set-hostname \u0026lt;xxx\u0026gt;  Install kubeadm On Master PI Firt access the kubemaster node and regenerate a token (if you did not use ttl=0 when using kubeadm init on the master):\nkubeadm token create  On New Slave PI sudo -i curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - echo \u0026quot;deb http://apt.kubernetes.io/ kubernetes-xenial main\u0026quot; \u0026gt; /etc/apt/sources.list.d/kubernetes.list apt-get update \u0026amp;\u0026amp; apt-get install -y kubeadm kubectl kubelet  To help during the initialization phase, get kubeadm to download the images onto docker\nkubeadm config images pull  Get the node to join the cluster\nkubeadm join 192.168.2.1:6443 --token yyyyyy.xxxx --discovery-token-ca-cert-hash sha256:zzzz  Conclusion  Will have to come back later and use cloud-init, create a clean \u0026amp; small SD image for Win32DiskImage Will have to create more advanced partition on the SD card. "
},
{
	"uri": "/pi_cluster/applications/",
	"title": "Applications",
	"tags": [],
	"description": "This is pi_cluster/docker_kubernetes tutorial page",
	"content": " Applications  Installing Kubedge Kubeplay  Install simplistic Kubeplay Application on the PI Cluster. It is a primitiv golang based server which get the user to go through the entire DevOps process (to build the docker image) and deploy it using Helm on Kubedge.\n\n Installing Kubernetes Dashboard  Install Kubernetes Dashboard on the PI Cluster\n\n Installing Kubedge Blinkt  Install Kubedge Blinkt Application on the PI Cluster\n\n Installing Prometheus  Install Prometheus on the PI Cluster\n\n Installing NATS  Install NATS on the PI Cluster\n\n Installing Grafana  Install Grafana on the PI Cluster. Grafana helps to plot the data in the system. It is an OSS like tools which helps to manage the kubedge cluster.\n\n Installing Kibana  Install Kibana \u0026amp; fluentd on the PI Cluster. Kibana is a good maintenance/OSS tools allowing traces and logs analysis.\n\n "
},
{
	"uri": "/devops/kubedgesdk/",
	"title": "Kubedge SDK",
	"tags": [],
	"description": "This is devops/kubedgesdk tutorial page",
	"content": " KUBEDGE SDK  Recompile Kubernetes components for Raspberry PI  During the installation of official Kubernetes 1.11.0 on RPI Cluster 1, encountered a bug on the controller manager preventing the controller-manager from starting. The problem here was to be able to cross compiled the latest version of Kubernetes 1.11.1 before the code was officially released and of course rebuild the images.\n\n docker.io versus docker-ce  Wondering why you have a strange error such as \u0026lsquo;from \u0026hellip;\u0026rsquo; when running docker build. The reason is linked to an older version of docker installed.\n\n Setup SingleNode Kubernetes Cluster using kubeadm  Setup simple kubernetes cluster for test purposes.\n\n "
},
{
	"uri": "/lte_to_5g/lte_and_5g_ran_simulation/",
	"title": "LTE, eLTE and 5G RAN",
	"tags": [],
	"description": "This is lte_to_5g/lte_and_5g_ran_simulation tutorial page",
	"content": "LTE and 5G RAN Kubedge proposes simulators of LTE, eLTE and 5G NR nodes.\n\n Kubedge LTE Simulation  Install Kubedge LTE Application on the PI Cluster\n\n Kubedge eLTE Simulation  Install Kubedge eLTE Application on the PI Cluster\n\n Kubedge 5G NR Simulation  Install Kubedge LTE Application on the PI Cluster\n\n"
},
{
	"uri": "/nfv/",
	"title": "NFV/SDN/Network Slicing",
	"tags": [],
	"description": "This is nfv tutorial page",
	"content": "Network Slicing Finally currently when the traffic is following through WIFI to ETH0 on the 5G NR node and from ETH0 to WIFI on the EPC node, the routing of the traffic is done using Linux Kernel mainly. The goal here is to build an NFV able to throttle for instance the traffic wifi network (simulating 5G) in order to create a simulation of network slicing. At that point the traffic on the 5G NR node will not go through the kernel but will go from WIFI through the NFV back to ETH0.\n\nsequenceDiagram participant PC participant Phone participant 5GNR_WorkerPI participant 5GC_MasterPI participant MPLSNetwork PC-5GNR_WorkerPI: Establish RAN Connection 5GNR_WorkerPI-PC: Return IP Phone-5GNR_WorkerPI: Establish RAN Connection 5GNR_WorkerPI-Phone: Return IP PC-5GNR_WorkerPI: wlan0/ran IP traffic Phone-5GNR_WorkerPI: wlan0/ran IP traffic loop NFV 5GNR_WorkerPI-5GNR_WorkerPI: Prioritize ran-backhaul traffic end 5GNR_WorkerPI-5GC_MasterPI: eth0/backhaul IP traffic loop NFV 5GC_MasterPI-5GC_MasterPI: Prioritize backhaul-core Traffic end 5GC_MasterPI-MPLSNetwork: wlan0/core IP traffic MPLSNetwork-5GC_MasterPI: IP traffic loop NFV 5GC_MasterPI-5GC_MasterPI: Prioritize core-backhaul Traffic end 5GC_MasterPI-5GNR_WorkerPI: IP traffic loop NFV 5GNR_WorkerPI-5GNR_WorkerPI: Prioritize backhaul-ran traffic end 5GNR_WorkerPI-Phone: IP traffic 5GNR_WorkerPI-PC: IP traffic   OVS \u0026amp; ODL  OVS \u0026amp; ODL Tutorial\n GO Networking  This is nfv/fd_io_and_opnfv tutorial page\n Akraino  This is nfv/akraino tutorial page\n OPNFV  This is an OPNFV tutorial page\n Advanced \u0026amp; WIP  Advanced tutorials and Work In Progress\n"
},
{
	"uri": "/nfv/advanced/children/onap/",
	"title": "ONAP",
	"tags": [],
	"description": "ONAP Tutorial",
	"content": " ONAP "
},
{
	"uri": "/nfv/opnfv/",
	"title": "OPNFV",
	"tags": [],
	"description": "This is an OPNFV tutorial page",
	"content": " OPNFV  "
},
{
	"uri": "/pi_cluster/applications/children/nats/",
	"title": "Installing NATS",
	"tags": ["kubernetes", "nats", "rpi"],
	"description": "",
	"content": "Install NATS on the PI Cluster\n\nKey Aspects  Install NATS as a CRD in Kubedge using Helm Verify using simple pub/sub that the system is working  $ kubectl get crd NAME CREATED AT natsclusters.nats.io 2018-11-11T23:26:59Z natsserviceroles.nats.io 2018-11-11T23:26:59Z $ kubectl get all -n nats-io NAME READY STATUS RESTARTS AGE pod/nats-cluster-1 1/1 Running 0 89m pod/nats-cluster-2 1/1 Running 0 89m pod/nats-cluster-3 1/1 Running 0 89m pod/nats-nats-operator-arm32v7-9f66d65d6-dnp9p 1/1 Running 1 90m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/nats-cluster ClusterIP 10.107.86.149 \u0026lt;none\u0026gt; 4222/TCP 89m service/nats-cluster-mgmt ClusterIP None \u0026lt;none\u0026gt; 6222/TCP,8222/TCP 89m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/nats-nats-operator-arm32v7 1 1 1 1 90m NAME DESIRED CURRENT READY AGE replicaset.apps/nats-nats-operator-arm32v7-9f66d65d6 1 1 1 90m  Deploy Using helm helm repo add kubedge2 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2\u0026quot; helm repo update  Direct installation from the repo If you feel lucky:\nhelm install --namespace nats-io --name nats kubedge2/nats-operator-arm32v7  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge2/nats-operator-arm32v7 tar xvf nats-operator-arm32v7-7.3.4.tgz cd nats-operator-arm32v7/ kubectl create -f templates/customresourcedefinition.yaml helm install --namespace nats-io --name nats .  Install a Subscriber Use the subscriber help chart\nhelm install --name nats-sub kubedge1/nats-sub-arm32v7  Install a Publisher Use the publisher helm chart\nhelm install --name nats-pub kubedge1/nats-pub-arm32v7  Check the messages flow The messages published by the publisher are received in the subscriber\nkubectl logs pod/nats-sub-kubesim-nats-sub-55d8d899b5-sfvlc kubesim-nats-sub-arm32v7 Listening on [dockerfile-hardcoded-subject] [#1] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#2] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#3] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#4] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#5] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#6] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#7] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#8] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#9] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#10] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message' [#11] Received on [dockerfile-hardcoded-subject]: 'dockerfile-hardcoded-message'  Source Code The images creation scripts are availble under:\n nats-operator  The chart is available under:\n nats-operator  Cleanup Run the delete command\nhelm delete --purge nats release \u0026quot;nats\u0026quot; deleted  Conclusion We still have to be able to create the images automatically using .travis.yml\n Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/applications/children/grafana/",
	"title": "Installing Grafana",
	"tags": ["kubernetes", "grafana", "rpi"],
	"description": "",
	"content": "Install Grafana on the PI Cluster. Grafana helps to plot the data in the system. It is an OSS like tools which helps to manage the kubedge cluster.\n\nKey Aspects  Install Dashboard Access the dashboard for a web browser  Deploy Using helm Helm Chart is still work in progress\n helm repo add kubedge2 \u0026quot;https://raw.githubusercontent.com/kubedge/helmrepos/arm32v7/kubedge2\u0026quot; helm repo update  Direct installation from the repo If you feel lucky:\nhelm install kubedge2/grafana-arm32v7 --name grafana  Two steps installation from local If you want to better understand the setup:\ncd $MY_LOCAL_HELM_CHARTS helm fetch kubedge2/grafana-arm32v7 tar xvf grafana-arm32v7-1.17.4.tgz cd grafana-arm32v7/ helm install . --name grafana  Source Code The chart is available under:\n grafana  Cleanup Run the delete command\nhelm delete --purge grafana release \u0026quot;grafana\u0026quot; deleted  Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/docker_kubernetes/children/2018-07-12-a/",
	"title": "Enable docker Remote API",
	"tags": ["docker"],
	"description": "",
	"content": "In order to build image for Raspberry PI, it is sometimes usefull to be able to leverate infrastruture from a remote VM. For instance you can cross-build golang executable for ARM32v7 and transfer it to build an image on the remote PI. (used for Tiller docker image)\n\nGenerating the server certs In the case this Kubernetes cluster, the master is running on master-pi with IP address 192.168.1.95\nas root\nmkdir -p $HOME/dockercerts cd $HOME/dockercerts/ openssl genrsa -aes256 -out ca-key.pem 4096 openssl req -new -x509 -days 365 -key ca-key.pem -sha256 -out ca.pem openssl genrsa -out server-key.pem 4096 export HOST=master-pi openssl req -subj \u0026quot;/CN=$HOST\u0026quot; -sha256 -new -key server-key.pem -out server.csr echo subjectAltName = DNS:$HOST,IP:192.168.1.95,IP:127.0.0.1 \u0026gt;\u0026gt; extfile.cnf echo extendedKeyUsage = serverAuth \u0026gt;\u0026gt; extfile.cnf openssl x509 -req -days 365 -sha256 -in server.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out server-cert.pem -extfile extfile. rm -v client.csr server.csr chmod -v 0400 ca-key.pem key.pem server-key.pem chmod -v 0444 ca.pem server-cert.pem cert.pem  Installing certs and configuring dockerd service cp $HOME/dockercerts/ca.pem /etc/docker cp $HOME/dockercerts/server-cert.pem /etc/docker/ cp $HOME/dockercerts/server-key.pem /etc/docker/  mkdir -p /etc/systemd/system/docker.service.d vi /etc/systemd/system/docker.service.d/10-tls-verify.conf [Service] ExecStart= ExecStart=/usr/bin/dockerd -H fd:// -H tcp://192.168.1.95:2376 --tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server-cert.pem --tlskey=/etc/docker/server-key.pem Environment=\u0026quot;DOCKER_OPTS=--tlsverify --tlscacert=/etc/docker/ca.pem --tlscert=/etc/docker/server-cert.pem --tlskey=/etc/docker/server-key.pem\u0026quot;  Restart the service\nsystemctl daemon-reload systemctl restart docker.service  Installing client side on the PI does not have to be root.\ncp /root/dockercerts/ca.pem $HOME/.docker cp /root/dockercerts/key.pem $HOME/.docker/ cp /root/dockercerts/cert.pem $HOME/.docker/  docker --tlsverify -H tcp://192.168.1.95:2376 --tlscacert=$HOME/.docker/ca.pem --tlscert=$HOME/.docker/cert.pem --tlskey=$HOME/.docker/key.pem images  Installing client side on the remove VM Transfer the key from the master-pi to the local Ubuntu VM\nmkdir -p $HOME/.docker/master-pi cd $HOME/.docker/master-pi/ scp rpiuser@192.168.1.95:/home/rpiuser/.docker/master-pi/* .  Verify that the VM can access the remote PI\ndocker --tlsverify -H tcp://192.168.1.95:2376 --tlscacert=$HOME/.docker/master-pi/ca.pem --tlscert=$HOME/.docker/master-pi/cert.pem --tlskey=$HOME/.docker/master-pi/key.pem version  Use environment variable to simplify the command line\nexport DOCKER_CERT_PATH=~/.docker/master-pi export DOCKER_HOST=tcp://192.168.1.95:2376 export DOCKER_TLS_VERIFY=1 docker ps docker image list  Reference Links  coreos dockerd "
},
{
	"uri": "/pi_cluster/os_installation/children/worker_pi/",
	"title": "OS Installation on Worker PI",
	"tags": ["ansible", "hypriot", "rpi"],
	"description": "",
	"content": "This page describes simple steps to install the OS (HypriotOS) on the PIs.\n\nOverview Kubedge uses HypriotOS because the quickest to set up. SSH and Docker supported by default\nOS on worker/slave PI If your NAT setup was successfull on the master PI, it is time to push the SD card on the worker PIs, plug them to the ethernet switch and enable the power.\n Access the node  Insert the SD card in the worker PI and power it on. SSH to the master PI. From the master PI, ssh to the new PI. And IP address in 192.168.2.1xx will have been assigned to the new node.  Freeze your configuration Cloud init is perfect for the first boot. Once the node is up, it can be challenging not to preserve the fine tuning done to the OS.\nsudo apt-get remove --purge cloud-init sudo apt-get autoremove  Update OS Let\u0026rsquo;s update to the latest version\nsudo apt-get update sudo apt-get upgrade  Gain access to latest docker-ce\nsudo -i curl -fsSL https://get.docker.com -o get-docker.sh sh get-docker.sh usermod -aG docker pirate  At the time this article is written 18.09 is not supported yet.\nsudo apt-cache policy docker-ce sudo apt-get remove --purge docker-ce sudo apt-get autoremove sudo apt-get install docker-ce=18.06.1~ce~3-0~debian  Update worker PI name As root, replace black-pearl by kube-nodeXX, in the two following files:\nsudo -i vi /etc/hosts  It seems on HypriotOS 64, you need to do\nsudo hostnamectl set-hostname kubemaster-pi  The /etc/resolv.conf on the new node should contain the IP address of your home router if dhcpd is setup properly on your master node.\n Update the static IP on the master PI It is important to edit the dhcpd.conf file in order to enfore the same internal IP address on the 192.168.2.x network every time the cluster is rebooted. Be sure to check then isc-dhcp-server is still working and that you need not corrupt the configuration file.\narp -a sudo vi /etc/dhcpd/dhcpd.conf sudo service isc-dhcp-server restart  Reference Links  HypriotOS "
},
{
	"uri": "/pi_cluster/advanced/",
	"title": "Advanced &amp; WIP",
	"tags": [],
	"description": "Advanced tutorials and Work In Progress",
	"content": " Advanced  Compile and Test Portieris  One of the biggest security risks related to Kubernetes are often linked to the fact that it is really hard to ensure that only \u0026ldquo;approved\u0026rdquo; images are deployed in your Kubernetes cluster. The goal here is to leverage Notary and the a project called \u0026ldquo;Portieris\u0026rdquo; created by IBM.\n\n Installing Hypriot OS  Also ARM processor on the Raspberry PI 3B+ is a 64 bit processor, because the board is only equipped with 1G of RAM, a 64 bit operating system is not really needed except when\u0026hellip;.\n\n Update Kubernetes to 1.11 on Ubuntu  Kubeadm is coming with an upgrade option. The goal of this study is to leverage the option.\n\n Build and Deploy Kubernetes Kustomize  kustomize seems to help the setup of multiple clusters by removing copy paste accross cluster and still keeping the configuration file has plain yaml instead of the template like it is often the case with t\n\n Build and Deploy Kubernetes test-infra  test-infra seems to somewhat overlap with sonobuoy features. The purpose of this post is to fetch the code, compile and deploy it on a Kubernetes cluster.\n\n Build and Deploy Kubernetes Istio  Istio is aiming at improving security of the containers. One of the key aspects is the end to end encryption of the commnucation, the role of citadel to ensure the management of the certificates, the renewal of the certificates. As always, the goal of this post is to study that new tool and figure out I can leverage it in my day to day work.\n\n Build and Deploy Kubernetes Hashicorp Vault  Vault is aiming at improving security of the containers by rotating token and credential much more often than usual. Looks like it is especially effectiv to help rotate passwords used to access internal databases.\n\n "
},
{
	"uri": "/devops/microservices/",
	"title": "MicroServices",
	"tags": [],
	"description": "This is pi_cluster/microservices tutorial page",
	"content": " Micro Services  Python  As we did for Go and Java, where is was possible to create an Dockerfile starting from \u0026ldquo;scratch\u0026rdquo;, the goal of this post is to create a python base server container with a minimum about of packages (debian and python) to reduce the security exposure of the container as well as the image size.\n\n Creating simple GO server container  GO is perfectly adapted to microservices since it can build standalone executable.\n\n Creating simple Java 10 server container  Very often people associated Java to quite bulky and difficult to use in the microservice context, unless you have very large image containing the JRE. But since Java 9, Java did kind of catchup with golang on the subject. Where you can obtain a standalone executable when we running go build, java is now proposing jlink which always you to acheive a very similar result. The goal of this post is to build a container image as small as possible running Java.\n\n Setup your GOLANG environment  A lot of the opensource projects evolvoving around Kubernetes are written in go. It is very usefull to be able to rebuild so projects using go get or go build.\n\n "
},
{
	"uri": "/lte_to_5g/nsa_and_sa/",
	"title": "NSA vs SA. Option 3",
	"tags": [],
	"description": "This is lte_to_5g/nsa_and_sa tutorial page",
	"content": " NSA and SA  "
},
{
	"uri": "/pi_cluster/applications/children/kibana/",
	"title": "Installing Kibana",
	"tags": ["kubernetes", "fluentd", "rpi"],
	"description": "",
	"content": "Install Kibana \u0026amp; fluentd on the PI Cluster. Kibana is a good maintenance/OSS tools allowing traces and logs analysis.\n\nKey Aspects  Install Dashboard Access the dashboard for a web browser  Deploy WIP\n Conclusion WIP\n Reference Links WIP\n"
},
{
	"uri": "/pi_cluster/os_installation/children/ansible/",
	"title": "Using Ansible to manage Raspberry PI cluster",
	"tags": ["ansible", "rpi"],
	"description": "",
	"content": "Even if the ultimate goal is to manage completly the cluster using Kubernetes, the ability to use Ansible during debug process is very usefull. The goal here is to setup ansible inventory, basic playbooks.\n\nAnsible Installation on the master node Let\u0026rsquo;s install ansible using apt-get. A lot of python related depedencies are also installed.\nsudo apt-get install ansible Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: ieee-data libyaml-0-2 python-cffi-backend python-crypto python-cryptography python-enum34 python-httplib2 python-idna python-ipaddress python-jinja2 python-kerberos python-markupsafe python-netaddr python-paramiko python-pkg-resources python-pyasn1 python-selinux python-setuptools python-six python-xmltodict python-yaml Suggested packages: cowsay sshpass python-crypto-dbg python-crypto-doc python-cryptography-doc python-cryptography-vectors python-enum34-doc python-jinja2-doc ipython python-netaddr-docs python-gssapi doc-base python-setuptools-doc Recommended packages: python-winrm The following NEW packages will be installed: ansible ieee-data libyaml-0-2 python-cffi-backend python-crypto python-cryptography python-enum34 python-httplib2 python-idna python-ipaddress python-jinja2 python-kerberos python-markupsafe python-netaddr python-paramiko python-pkg-resources python-pyasn1 python-selinux python-setuptools python-six python-xmltodict python-yaml 0 upgraded, 22 newly installed, 0 to remove and 6 not upgraded. Need to get 4,556 kB of archives. After this operation, 28.4 MB of additional disk space will be used. Do you want to continue? [Y/n] y  $ ansible --version ansible 2.2.1.0 config file = /etc/ansible/ansible.cfg configured module search path = Default w/o overrides  sudo apt-get install sshpass Reading package lists... Done Building dependency tree Reading state information... Done The following NEW packages will be installed: sshpass 0 upgraded, 1 newly installed, 0 to remove and 6 not upgraded. Need to get 11.2 kB of archives. After this operation, 30.7 kB of additional disk space will be used. Get:1 http://raspbian.mirrors.lucidnetworks.net/raspbian stretch/main armhf sshpass armhf 1.06-1 [11.2 kB] Fetched 11.2 kB in 2s (4,785 B/s) Selecting previously unselected package sshpass. (Reading database ... 26786 files and directories currently installed.) Preparing to unpack .../sshpass_1.06-1_armhf.deb ... Unpacking sshpass (1.06-1) ... Setting up sshpass (1.06-1) ... Processing triggers for man-db (2.7.6.1-2) ...  Configure ansible Create directorties for ansible\nmkdir -p mgt/inventory mkdir -p mgt/playbooks mkdir -p mgt/roles mkdir -p mgt/group_vars mkdir -p mgt/files mkdir -p mgt/inventory/host_vars  Let\u0026rsquo;s check the internal cluster network\ncat /etc/hosts 192.168.2.1 kubemaster-pi.clusterX.kubedge.cloud kubemaster-pi 192.168.2.101 kube-node01.clusterX.kubedge.cloud kube-node01 192.168.2.102 kube-node02.clusterX.kubedge.cloud kube-node02 192.168.2.103 kube-node03.clusterX.kubedge.cloud kube-node03 192.168.2.104 kube-node04.clusterX.kubedge.cloud kube-node04  Let\u0026rsquo;s create an rsa key for Ansible SSH. Note the cluster is still using the default pirate account created by HypriotOS. Will change is later once ansible is up.\ncd ~/mgt ssh-keygen -t rsa -f mgtkey ssh-copy-id -i mgtkey.pub pirate@kubemaster-pi ssh-copy-id -i mgtkey.pub pirate@kube-node01 ssh-copy-id -i mgtkey.pub pirate@kube-node02 ssh-copy-id -i mgtkey.pub pirate@kube-node03 ssh-copy-id -i mgtkey.pub pirate@kube-node04  Create a first ansible host_var. We will use the mgtkey for ssh/ansible.\ncd ~/mgt/inventory/host_vars cat kubemaster-pi.clusterX.kubedge.cloud ansible_host: 192.168.2.FOOBAR ansible_port: 22 ansible_user: pirate ansible_ssh_private_key_file: mgtkey  for i in kube-node01 kube-node02 kube-node03 kube-node04 do cp kubemaster-pi.clusterX.kubedge.cloud $i.clusterX.kubedge.cloud done  Replace FOOBAR by the proper value\nvi *.clusterX.kubedge.cloud  Create the main inventory file\ncd ~/mgt/inventory cat hosts --- [picluster:children] masters workers [masters] kubemaster-pi.clusterX.kubedge.cloud [workers] kube-node01.clusterX.kubedge.cloud kube-node02.clusterX.kubedge.cloud kube-node03.clusterX.kubedge.cloud kube-node04.clusterX.kubedge.cloud  Use ansible Simple ping ansible picluster -i inventory/ -m ping kube-node01.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node04.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node03.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kubemaster-pi.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node02.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; }  ansible masters -i inventory/ -m ping kubemaster-pi.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; }  ansible workers -i inventory/ -m ping kube-node01.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node03.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node02.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot; } kube-node04.clusterX.kubedge.cloud | SUCCESS =\u0026gt; { \u0026quot;changed\u0026quot;: false, \u0026quot;ping\u0026quot;: \u0026quot;pong\u0026quot;  Retrieve Facts ansible picluster -i inventory/ -m setup  Update all the nodes in the cluster Create a simple playbook\ncat playbooks/aptupdate.yml --- - hosts: picluster tasks: - name: update apt become: true apt: update_cache: yes  Run the simple aptupdate playbook\nansible-playbook -i inventory/ playbooks/aptupdate.yml PLAY [picluster] *************************************************************** TASK [setup] ******************************************************************* ok: [kubemaster-pi.clusterX.kubedge.cloud] ok: [kube-node03.clusterX.kubedge.cloud] ok: [kube-node04.clusterX.kubedge.cloud] ok: [kube-node02.clusterX.kubedge.cloud] ok: [kube-node01.clusterX.kubedge.cloud] TASK [update apt] ************************************************************** changed: [kube-node02.clusterX.kubedge.cloud] changed: [kubemaster-pi.clusterX.kubedge.cloud] changed: [kube-node01.clusterX.kubedge.cloud] changed: [kube-node03.clusterX.kubedge.cloud] changed: [kube-node04.clusterX.kubedge.cloud] PLAY RECAP ********************************************************************* kube-node01.clusterX.kubedge.cloud : ok=2 changed=1 unreachable=0 failed=0 kube-node02.clusterX.kubedge.cloud : ok=2 changed=1 unreachable=0 failed=0 kube-node03.clusterX.kubedge.cloud : ok=2 changed=1 unreachable=0 failed=0 kube-node04.clusterX.kubedge.cloud : ok=2 changed=1 unreachable=0 failed=0 kubemaster-pi.clusterX.kubedge.cloud : ok=2 changed=1 unreachable=0 failed=0  Check the version of kubeadm ansible picluster -i inventory -m shell -a \u0026quot;kubeadm version\u0026quot; kube-node04.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.8\u0026quot;, GitCommit:\u0026quot;c138b85178156011dc934c2c9f4837476876fb07\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-05-21T18:53:18Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} kube-node03.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.8\u0026quot;, GitCommit:\u0026quot;c138b85178156011dc934c2c9f4837476876fb07\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-05-21T18:53:18Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} kube-node01.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.8\u0026quot;, GitCommit:\u0026quot;c138b85178156011dc934c2c9f4837476876fb07\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-05-21T18:53:18Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} kubemaster-pi.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.8\u0026quot;, GitCommit:\u0026quot;c138b85178156011dc934c2c9f4837476876fb07\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-05-21T18:53:18Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;} kube-node02.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;9\u0026quot;, GitVersion:\u0026quot;v1.9.8\u0026quot;, GitCommit:\u0026quot;c138b85178156011dc934c2c9f4837476876fb07\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-05-21T18:53:18Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/arm\u0026quot;}  Check the temperature ansible picluster -i inventory -m shell -a \u0026quot;vcgencmd measure_temp\u0026quot; kube-node03.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; temp=36.5'C kubemaster-pi.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; temp=49.4'C kube-node02.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; temp=33.2'C kube-node04.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; temp=34.3'C kube-node01.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; temp=32.2'C  Check the connected devices Check the components of the PI composing the cluster (Those are PI 3B+)\nansible picluster -i inventory -m shell -a \u0026quot;lsusb\u0026quot; kubemaster-pi.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub kube-node02.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub kube-node01.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub kube-node04.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub kube-node03.clusterX.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  On the second cluster, some of the nodes are PI 3B instead of PI 3B+. LAN and WLAN are different.\nansible picluster -i inventory -m shell -s -a \u0026quot;lsusb\u0026quot; master-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 0424:7800 Standard Microsystems Corp. Bus 001 Device 003: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 002: ID 0424:2514 Standard Microsystems Corp. USB 2.0 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub home-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 004: ID 10c4:8a2a Cygnal Integrated Products, Inc. Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. SMC9514 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub nas-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; Bus 001 Device 003: ID 0424:ec00 Standard Microsystems Corp. SMSC9512/9514 Fast Ethernet Adapter Bus 001 Device 002: ID 0424:9514 Standard Microsystems Corp. SMC9514 Hub Bus 001 Device 001: ID 1d6b:0002 Linux Foundation 2.0 root hub  Other usefull commands ansible picluster -i inventory -m shell -a \u0026quot;lsusb\u0026quot; ansible picluster -i inventory -m shell -a \u0026quot;dmesg\u0026quot; ansible picluster -i inventory -m shell -a \u0026quot;usb-devices\u0026quot; ansible picluster -i inventory -m shell -a \u0026quot;lsblk\u0026quot; ansible picluster -i inventory -m shell -a \u0026quot;blkid\u0026quot; ansible picluster -i inventory -m shell -a \u0026quot;fdisk -l\u0026quot;  Reference Links  TBD "
},
{
	"uri": "/pi_cluster/advanced/children/2018-06-30-a/",
	"title": "Update Kubernetes to 1.11 on Ubuntu",
	"tags": ["kubernetes"],
	"description": "",
	"content": "Kubeadm is coming with an upgrade option. The goal of this study is to leverage the option.\n\nInitial set up Kubernetes 1.10.4 is installed\ncat /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main  sudo dpkg -l kubeadm Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubeadm 1.10.4-00 amd64 Kubernetes Cluster Bootstrapping Tool  sudo dpkg -l kubelet Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubelet 1.10.4-00 amd64 Kubernetes Node Agent  sudo dpkg -l kubectl Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubectl 1.10.4-00 amd64 Kubernetes Command Line Tool  kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;10\u0026quot;, GitVersion:\u0026quot;v1.10.4\u0026quot;, GitCommit:\u0026quot;5ca598b4ba5abb89bb773071ce452e33fb66339d\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-06T08:13:03Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;10\u0026quot;, GitVersion:\u0026quot;v1.10.4\u0026quot;, GitCommit:\u0026quot;5ca598b4ba5abb89bb773071ce452e33fb66339d\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-06T08:00:59Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Update kubelet and kubectl Update aptitude\nsudo apt-get update Hit:2 http://ppa.launchpad.net/longsleep/golang-backports/ubuntu xenial InRelease Get:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB] Get:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [511 kB] Get:5 http://security.ubuntu.com/ubuntu xenial-security/main i386 Packages [452 kB] Get:6 http://security.ubuntu.com/ubuntu xenial-security/main amd64 DEP-11 Metadata [67.7 kB] Get:7 http://security.ubuntu.com/ubuntu xenial-security/main DEP-11 64x64 Icons [68.0 kB] Get:8 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [355 kB] Get:9 http://security.ubuntu.com/ubuntu xenial-security/universe i386 Packages [303 kB] Get:10 http://security.ubuntu.com/ubuntu xenial-security/universe Translation-en [133 kB] Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 DEP-11 Metadata [107 kB] Get:12 http://security.ubuntu.com/ubuntu xenial-security/universe DEP-11 64x64 Icons [147 kB] Hit:13 http://us.archive.ubuntu.com/ubuntu xenial InRelease Get:14 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB] Get:15 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB] Get:16 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [796 kB] Get:17 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [728 kB] Get:18 http://us.archive.ubuntu.com/ubuntu xenial-updates/main Translation-en [329 kB] Get:19 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 DEP-11 Metadata [318 kB] Get:20 http://us.archive.ubuntu.com/ubuntu xenial-updates/main DEP-11 64x64 Icons [228 kB] Get:21 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [640 kB] Get:1 https://packages.cloud.google.com/apt kubernetes-xenial InRelease [8,993 B] Get:22 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [585 kB] Get:23 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 Packages [17.9 kB] Get:24 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe Translation-en [257 kB] Get:25 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 DEP-11 Metadata [246 kB] Get:26 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe DEP-11 64x64 Icons [331 kB] Get:27 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 DEP-11 Metadata [5,964 B] Get:28 http://us.archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [6,740 B] Get:29 http://us.archive.ubuntu.com/ubuntu xenial-backports/main i386 Packages [6,732 B] Get:30 http://us.archive.ubuntu.com/ubuntu xenial-backports/main Translation-en [4,180 B] Get:31 http://us.archive.ubuntu.com/ubuntu xenial-backports/main amd64 DEP-11 Metadata [3,328 B] Get:32 http://us.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [7,412 B] Get:33 http://us.archive.ubuntu.com/ubuntu xenial-backports/universe i386 Packages [7,104 B] Get:34 http://us.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 DEP-11 Metadata [5,100 B] Get:35 http://us.archive.ubuntu.com/ubuntu xenial-backports/universe DEP-11 64x64 Icons [1,789 B] Fetched 7,000 kB in 9s (757 kB/s)  Run the apt-get upgrade command. Notice how the kubeadm is \u0026ldquo;kept back\u0026rdquo;\nsudo apt-get upgrade Reading package lists... Done Building dependency tree Reading state information... Done Calculating upgrade... Done The following packages were automatically installed and are no longer required: linux-headers-4.13.0-26 linux-headers-4.13.0-26-generic linux-image-4.13.0-26-generic linux-image-extra-4.13.0-26-generic Use 'sudo apt autoremove' to remove them. The following packages have been kept back: kubeadm The following packages will be upgraded: amd64-microcode console-setup console-setup-linux cpp-5 desktop-file-utils ebtables g++-5 gcc-5 gcc-5-base gir1.2-javascriptcoregtk-4.0 gir1.2-webkit2-4.0 gnome-software gnome-software-common keyboard-configuration kubectl kubelet libasan2 libatomic1 libcc1-0 libcilkrts5 libgcc-5-dev libgcrypt20 libgomp1 libitm1 libjasper1 libjavascriptcoregtk-4.0-18 libldap-2.4-2 liblsan0 libmpx0 libplymouth4 libquadmath0 libssl-dev libssl1.0.0 libstdc++-5-dev libstdc++6 libtsan0 libubsan0 libwebkit2gtk-4.0-37 libwebkit2gtk-4.0-37-gtk2 linux-firmware openssl plymouth plymouth-label plymouth-theme-ubuntu-logo plymouth-theme-ubuntu-text rfkill snapd ubuntu-core-launcher ubuntu-software update-notifier update-notifier-common wireless-regdb 52 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.  sudo apt-get upgrade  sudo apt-get autoremove  sudo reboot  sudo dpkg -l kubectl Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubectl 1.11.0-00 amd64 Kubernetes Command Line Tool  sudo dpkg -l kubeadm Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubeadm 1.10.4-00 amd64 Kubernetes Cluster Bootstrapping Tool  sudo dpkg -l kubelet Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-======================================-========================-========================-================================================================================= ii kubelet 1.11.0-00 amd64 Kubernetes Node Agent  kubectl version Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;11\u0026quot;, GitVersion:\u0026quot;v1.11.0\u0026quot;, GitCommit:\u0026quot;91e7b4fd31fcd3d5f436da26c980becec37ceefe\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-27T20:17:28Z\u0026quot;, GoVersion:\u0026quot;go1.10.2\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;} Server Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;10\u0026quot;, GitVersion:\u0026quot;v1.10.4\u0026quot;, GitCommit:\u0026quot;5ca598b4ba5abb89bb773071ce452e33fb66339d\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-06T08:00:59Z\u0026quot;, GoVersion:\u0026quot;go1.9.3\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Upgrading kubeadm Install new kubeadm and associated new cri-tools\nsudo apt-get install kubeadm Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: cri-tools The following NEW packages will be installed: cri-tools The following packages will be upgraded: kubeadm 1 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 14.7 MB of archives. After this operation, 70.9 MB disk space will be freed. Do you want to continue? [Y/n] y Get:1 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 cri-tools amd64 1.11.0-00 [5,309 kB] Get:2 https://packages.cloud.google.com/apt kubernetes-xenial/main amd64 kubeadm amd64 1.11.0-00 [9,422 kB] Fetched 14.7 MB in 5s (2,780 kB/s) Selecting previously unselected package cri-tools. (Reading database ... 225649 files and directories currently installed.) Preparing to unpack .../cri-tools_1.11.0-00_amd64.deb ... Unpacking cri-tools (1.11.0-00) ... Preparing to unpack .../kubeadm_1.11.0-00_amd64.deb ... Unpacking kubeadm (1.11.0-00) over (1.10.4-00) ... Setting up cri-tools (1.11.0-00) ... Setting up kubeadm (1.11.0-00) ... Installing new version of config file /etc/systemd/system/kubelet.service.d/10-kubeadm.conf ...  kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;11\u0026quot;, GitVersion:\u0026quot;v1.11.0\u0026quot;, GitCommit:\u0026quot;91e7b4fd31fcd3d5f436da26c980becec37ceefe\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-27T20:14:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.2\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Upgrading your Kubernetes cluster Cluster does not seems be in such a great shape\nNAME READY STATUS RESTARTS AGE pod/calico-etcd-j8zqp 1/1 Running 22 11d pod/calico-kube-controllers-679568f47c-wgz7n 1/1 Running 27 11d pod/calico-node-knstj 2/2 Running 39 11d pod/etcd-xxxxxxx 0/1 Pending 0 1h pod/kube-apiserver-xxxxxxx 0/1 Pending 0 1h pod/kube-controller-manager-xxxxxxx 0/1 Pending 0 1h pod/kube-dns-86f4d74b45-fs7bq 3/3 Running 66 11d pod/kube-proxy-mpbf7 1/1 Running 22 11d pod/kube-scheduler-xxxxxxx 0/1 Pending 0 1h pod/tiller-deploy-5c688d5f9b-6cl5n 1/1 Running 23 11d NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/calico-etcd ClusterIP 10.96.232.136 \u0026lt;none\u0026gt; 6666/TCP 11d service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 11d service/tiller-deploy ClusterIP 10.97.44.240 \u0026lt;none\u0026gt; 44134/TCP 11d NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/calico-etcd 1 1 0 1 0 node-role.kubernetes.io/master= 11d daemonset.apps/calico-node 1 1 0 1 0 \u0026lt;none\u0026gt; 11d daemonset.apps/kube-proxy 1 1 0 1 0 \u0026lt;none\u0026gt; 11d NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/calico-kube-controllers 1 1 1 0 11d deployment.apps/kube-dns 1 1 1 0 11d deployment.apps/tiller-deploy 1 1 1 0 11d NAME DESIRED CURRENT READY AGE replicaset.apps/calico-kube-controllers-679568f47c 1 1 0 11d replicaset.apps/kube-dns-86f4d74b45 1 1 0 11d replicaset.apps/tiller-deploy-5c688d5f9b 1 1 0 11d  sudo kubeadm upgrade apply --dry-run 1.11.0 \u0026gt; /tmp/traces [upgrade/health] FATAL: [preflight] Some fatal errors occurred: [ERROR MasterNodesReady]: there are NotReady masters in the cluster: [xxxxxxx] [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`  Usefull Links TBD\n"
},
{
	"uri": "/devops/advanced/",
	"title": "Advanced &amp; WIP",
	"tags": [],
	"description": "Advanced tutorials and Work In Progress",
	"content": " Advanced  Rebuild Hyperkube images  This post is to validate that it would be possible if urgency dictates it to rebuild the hyperkube Kubernetes image.\n\n Zuul  OpenStack project are using Zuul for CI/CD process. Zuul itself is based on Ansible to perform the tasks. This post is the collection of notes and tips used during the couple of update I did to some openstack projects.\n\n Setup github/gerrit behind a corporate http proxy  This is a post of the small set of notes taken to setup gerrit review behind corporate proxy.\n\n Setup multiple GitHub account on a single machine  In orderer to manage your personal GitHub projects or an your compagny projects, it is usefull to be able to conigure your .ssh directory.\n\n "
},
{
	"uri": "/lte_to_5g/advanced/",
	"title": "Advanced &amp; WIP",
	"tags": [],
	"description": "Advanced tutorials and Work In Progress",
	"content": " Advanced  "
},
{
	"uri": "/pi_cluster/advanced/children/2018-07-29-a/",
	"title": "Build and Deploy Kubernetes Kustomize",
	"tags": ["kubernetes", "kustomize", "rpi"],
	"description": "",
	"content": "kustomize seems to help the setup of multiple clusters by removing copy paste accross cluster and still keeping the configuration file has plain yaml instead of the template like it is often the case with t\n\nKey Aspects  Compile and deploy the Kustomize code in Kustomize  Deploy  WIP  Conclusion  WIP  Reference Links  Kustomize Description Official Kustomize Code "
},
{
	"uri": "/pi_cluster/maintenance/children/2018-07-02-a/",
	"title": "Compile and Test SONOBUOY",
	"tags": ["kubernetes", "sonobuoy", "test-infra", "testing"],
	"description": "",
	"content": "Sonobouy, deploys in a Kubernetes cluster and helps to assesse the compliance of that cluster\n\nKey Aspects  Fork Sonobuoy Compile the tools Test it  Clone and Compile mkdir -p $HOME/src/github.com/heptio cd $HOME/src/github.com/heptio git clone git@github.com:jbrette/sonobuoy.git  export GOPATH=$HOME go version go version go1.10.1 linux/amd64  go get -u -v github.com/heptio/sonobuoy  sonobuoy run sonobuoy status  kubectl get all -n heptio-sonobuoy NAME READY STATUS RESTARTS AGE pod/sonobuoy 1/1 Running 0 14m pod/sonobuoy-e2e-job-5fff584d11364ca1 2/2 Running 0 12m pod/sonobuoy-systemd-logs-daemon-set-1c53f31cf14246ca-mhztp 2/2 Running 0 12m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/sonobuoy-master ClusterIP 10.99.111.192 \u0026lt;none\u0026gt; 8080/TCP 14m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/sonobuoy-systemd-logs-daemon-set-1c53f31cf14246ca 1 1 1 1 1 \u0026lt;none\u0026gt; 12m  sonobuoy logs see end of the file  Usefull Links  Link1  SONOBUOY LOGS namespace=\u0026quot;heptio-sonobuoy\u0026quot; pod=\u0026quot;sonobuoy-e2e-job-5fff584d11364ca1\u0026quot; container=\u0026quot;sonobuoy-worker\u0026quot; time=\u0026quot;2018-07-03T06:01:55Z\u0026quot; level=info msg=\u0026quot;Waiting for waitfile\u0026quot; waitfile=/tmp/results/done namespace=\u0026quot;heptio-sonobuoy\u0026quot; pod=\u0026quot;sonobuoy\u0026quot; container=\u0026quot;kube-sonobuoy\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Scanning plugins in ./plugins.d (pwd: /)\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Scanning plugins in /etc/sonobuoy/plugins.d (pwd: /)\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Directory (/etc/sonobuoy/plugins.d) does not exist\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Scanning plugins in ~/sonobuoy/plugins.d (pwd: /)\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Directory (~/sonobuoy/plugins.d) does not exist\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Filtering namespaces based on the following regex:.*|heptio-sonobuoy\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Namespace default Matched=true\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Namespace heptio-sonobuoy Matched=true\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Namespace kube-public Matched=true\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Namespace kube-system Matched=true\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Starting server Expected Results: [{ e2e} {ubuntuvm systemd_logs}]\u0026quot; time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;starting aggregation server\u0026quot; address=0.0.0.0 port=8080 time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Running plugin\u0026quot; plugin=e2e time=\u0026quot;2018-07-03T06:00:23Z\u0026quot; level=info msg=\u0026quot;Running plugin\u0026quot; plugin=systemd-logs time=\u0026quot;2018-07-03T06:01:53Z\u0026quot; level=info msg=\u0026quot;received aggregator request\u0026quot; client_cert=systemd-logs node=ubuntuvm plugin_name=systemd_logs namespace=\u0026quot;heptio-sonobuoy\u0026quot; pod=\u0026quot;sonobuoy-e2e-job-5fff584d11364ca1\u0026quot; container=\u0026quot;e2e\u0026quot; /usr/local/bin/ginkgo --focus=\\[Conformance\\] --skip=Alpha|Kubectl|\\[(Disruptive|Feature:[^\\]]+|Flaky)\\] --noColor=true --nodes=1 /usr/local/bin/e2e.test -- --disable-log-dump --repo-root=/kubernetes --provider=\u0026quot;local\u0026quot; --report-dir=\u0026quot;/tmp/results\u0026quot; --kubeconfig=\u0026quot;\u0026quot; Jul 3 06:01:53.046: INFO: Overriding default scale value of zero to 1 Jul 3 06:01:53.046: INFO: Overriding default milliseconds value of zero to 5000 I0703 06:01:53.233115 17 test_context.go:382] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-559575868 I0703 06:01:53.233377 17 e2e.go:333] Starting e2e run \u0026quot;954001dd-7e86-11e8-9aa2-c6770944a2e6\u0026quot; on Ginkgo node 1 Running Suite: Kubernetes e2e suite =================================== Random Seed: 1530597712 - Will randomize all specs Will run 144 of 998 specs Jul 3 06:01:53.320: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 Jul 3 06:01:53.322: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable Jul 3 06:01:53.333: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready Jul 3 06:01:53.371: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed) Jul 3 06:01:53.371: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready. Jul 3 06:01:53.377: INFO: Waiting for pods to enter Success, but no pods in \u0026quot;kube-system\u0026quot; match label map[name:e2e-image-puller] Jul 3 06:01:53.377: INFO: Dumping network health container logs from all nodes to file /tmp/results/nethealth.txt Jul 3 06:01:53.381: INFO: e2e test version: v1.11.0 Jul 3 06:01:53.382: INFO: kube-apiserver version: v1.11.0 SSS ------------------------------ [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:01:53.382: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object Jul 3 06:01:53.446: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled. STEP: Waiting for a default service account to be provisioned in namespace [It] should support (non-root,0777,tmpfs) [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test emptydir 0777 on tmpfs Jul 3 06:01:53.458: INFO: Waiting up to 5m0s for pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-emptydir-hhx7j\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:01:53.462: INFO: Pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.316916ms Jul 3 06:01:55.470: INFO: Pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.012312995s Jul 3 06:01:57.483: INFO: Pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.025109994s Jul 3 06:01:59.488: INFO: Pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 6.030152763s STEP: Saw pod success Jul 3 06:01:59.488: INFO: Pod \u0026quot;pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:01:59.496: INFO: Trying to get logs from node ubuntuvm pod pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6 container test-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:01:59.570: INFO: Waiting for pod pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:01:59.578: INFO: Pod pod-958bb7dc-7e86-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:01:59.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-emptydir-hhx7j\u0026quot; for this suite. Jul 3 06:02:05.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:02:05.743: INFO: namespace: e2e-tests-emptydir-hhx7j, resource: bindings, ignored listing per whitelist Jul 3 06:02:05.789: INFO: namespace e2e-tests-emptydir-hhx7j deletion completed in 6.205463311s  [SLOW TEST:12.407 seconds] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40 should support (non-root,0777,tmpfs) [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] Secrets /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:02:05.790: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating secret with name secret-test-9cfc3c2a-7e86-11e8-9aa2-c6770944a2e6 STEP: Creating a pod to test consume secrets Jul 3 06:02:05.945: INFO: Waiting up to 5m0s for pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-secrets-27nw4\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:02:05.956: INFO: Pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 11.604319ms Jul 3 06:02:07.962: INFO: Pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.017009791s Jul 3 06:02:09.967: INFO: Pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.022881248s Jul 3 06:02:11.971: INFO: Pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 6.026083073s STEP: Saw pod success Jul 3 06:02:11.971: INFO: Pod \u0026quot;pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:02:11.974: INFO: Trying to get logs from node ubuntuvm pod pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6 container secret-volume-test: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:02:11.999: INFO: Waiting for pod pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:02:12.003: INFO: Pod pod-secrets-9cfd5a21-7e86-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] Secrets /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:02:12.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-secrets-27nw4\u0026quot; for this suite. Jul 3 06:02:18.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:02:18.135: INFO: namespace: e2e-tests-secrets-27nw4, resource: bindings, ignored listing per whitelist Jul 3 06:02:18.225: INFO: namespace e2e-tests-secrets-27nw4 deletion completed in 6.218371459s  [SLOW TEST:12.436 seconds] [sig-storage] namespace=\u0026quot;heptio-sonobuoy\u0026quot; pod=\u0026quot;sonobuoy-systemd-logs-daemon-set-1c53f31cf14246ca-mhztp\u0026quot; container=\u0026quot;sonobuoy-worker\u0026quot; time=\u0026quot;2018-07-03T06:01:52Z\u0026quot; level=info msg=\u0026quot;Waiting for waitfile\u0026quot; waitfile=/tmp/results/done time=\u0026quot;2018-07-03T06:01:53Z\u0026quot; level=info msg=\u0026quot;Detected done file, transmitting result file\u0026quot; resultFile=/tmp/results/systemd_logs namespace=\u0026quot;heptio-sonobuoy\u0026quot; pod=\u0026quot;sonobuoy-e2e-job-5fff584d11364ca1\u0026quot; container=\u0026quot;e2e\u0026quot; Secrets /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33 should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSSSSSSS ------------------------------ [k8s.io] Probing container should have monotonically increasing restart count [Slow][NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:02:18.233: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48 [It] should have monotonically increasing restart count [Slow][NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-vwfbg Jul 3 06:02:24.338: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-vwfbg STEP: checking the pod's current state and verifying that restartCount is present Jul 3 06:02:24.342: INFO: Initial restart count of pod liveness-http is 0 Jul 3 06:02:42.390: INFO: Restart count of pod e2e-tests-container-probe-vwfbg/liveness-http is now 1 (18.04818225s elapsed) Jul 3 06:03:00.450: INFO: Restart count of pod e2e-tests-container-probe-vwfbg/liveness-http is now 2 (36.107665419s elapsed) Jul 3 06:03:20.540: INFO: Restart count of pod e2e-tests-container-probe-vwfbg/liveness-http is now 3 (56.197646321s elapsed) Jul 3 06:03:41.043: INFO: Restart count of pod e2e-tests-container-probe-vwfbg/liveness-http is now 4 (1m16.286139808s elapsed) Jul 3 06:04:51.429: INFO: Restart count of pod e2e-tests-container-probe-vwfbg/liveness-http is now 5 (2m26.671984101s elapsed) STEP: deleting the pod [AfterEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:04:51.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-container-probe-vwfbg\u0026quot; for this suite. Jul 3 06:04:57.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:04:57.689: INFO: namespace: e2e-tests-container-probe-vwfbg, resource: bindings, ignored listing per whitelist Jul 3 06:04:57.716: INFO: namespace e2e-tests-container-probe-vwfbg deletion completed in 6.21869089s  [SLOW TEST:159.069 seconds] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 should have monotonically increasing restart count [Slow][NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSS ------------------------------ [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:04:57.716: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating configMap with name configmap-test-volume-map-03686ea1-7e87-11e8-9aa2-c6770944a2e6 STEP: Creating a pod to test consume configMaps Jul 3 06:04:57.775: INFO: Waiting up to 5m0s for pod \u0026quot;pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-configmap-5bcbx\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:04:57.780: INFO: Pod \u0026quot;pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 5.029515ms Jul 3 06:04:59.791: INFO: Pod \u0026quot;pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.015570127s Jul 3 06:05:01.796: INFO: Pod \u0026quot;pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.020671875s STEP: Saw pod success Jul 3 06:05:01.796: INFO: Pod \u0026quot;pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:05:01.798: INFO: Trying to get logs from node ubuntuvm pod pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6 container configmap-volume-test: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:05:01.824: INFO: Waiting for pod pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:05:01.828: INFO: Pod pod-configmaps-0368d2ec-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:05:01.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-configmap-5bcbx\u0026quot; for this suite. Jul 3 06:05:07.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:05:07.942: INFO: namespace: e2e-tests-configmap-5bcbx, resource: bindings, ignored listing per whitelist Jul 3 06:05:07.965: INFO: namespace e2e-tests-configmap-5bcbx deletion completed in 6.134761009s  [SLOW TEST:10.249 seconds] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32 should be consumable from pods in volume with mappings and Item mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSS ------------------------------ [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be submitted and removed [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [k8s.io] [sig-node] Pods Extended /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:05:07.965: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [k8s.io] Pods Set QOS Class /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:199 [It] should be submitted and removed [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: creating the pod STEP: submitting the pod to kubernetes STEP: verifying QOS class is set on the pod [AfterEach] [k8s.io] [sig-node] Pods Extended /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:05:08.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-pods-fgrwq\u0026quot; for this suite. Jul 3 06:05:30.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:05:30.221: INFO: namespace: e2e-tests-pods-fgrwq, resource: bindings, ignored listing per whitelist Jul 3 06:05:30.286: INFO: namespace e2e-tests-pods-fgrwq deletion completed in 22.188767463s  [SLOW TEST:22.321 seconds] [k8s.io] [sig-node] Pods Extended /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 [k8s.io] Pods Set QOS Class /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 should be submitted and removed [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSS ------------------------------ [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:05:30.286: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99 [It] should retry creating failed daemon pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a simple DaemonSet \u0026quot;daemon-set\u0026quot; STEP: Check that daemon pods launch on every node of the cluster. Jul 3 06:05:30.421: INFO: Number of nodes with available pods: 0 Jul 3 06:05:30.421: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:31.736: INFO: Number of nodes with available pods: 0 Jul 3 06:05:31.736: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:32.438: INFO: Number of nodes with available pods: 0 Jul 3 06:05:32.438: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:33.468: INFO: Number of nodes with available pods: 0 Jul 3 06:05:33.468: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:34.432: INFO: Number of nodes with available pods: 0 Jul 3 06:05:34.432: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:35.429: INFO: Number of nodes with available pods: 0 Jul 3 06:05:35.429: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:36.434: INFO: Number of nodes with available pods: 0 Jul 3 06:05:36.434: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:37.450: INFO: Number of nodes with available pods: 0 Jul 3 06:05:37.450: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:38.428: INFO: Number of nodes with available pods: 0 Jul 3 06:05:38.428: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:39.444: INFO: Number of nodes with available pods: 1 Jul 3 06:05:39.444: INFO: Number of running nodes: 1, number of available pods: 1 STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. Jul 3 06:05:39.484: INFO: Number of nodes with available pods: 0 Jul 3 06:05:39.485: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:40.496: INFO: Number of nodes with available pods: 0 Jul 3 06:05:40.496: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:05:41.497: INFO: Number of nodes with available pods: 1 Jul 3 06:05:41.498: INFO: Number of running nodes: 1, number of available pods: 1 STEP: Wait for the failed daemon pod to be completely deleted. [AfterEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:65 STEP: Deleting DaemonSet \u0026quot;daemon-set\u0026quot; STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-tl4wf, will wait for the garbage collector to delete the pods Jul 3 06:05:41.590: INFO: Deleting {extensions DaemonSet} daemon-set took: 31.578878ms Jul 3 06:05:41.693: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 102.749663ms Jul 3 06:05:49.499: INFO: Number of nodes with available pods: 0 Jul 3 06:05:49.499: INFO: Number of running nodes: 0, number of available pods: 0 Jul 3 06:05:49.505: INFO: daemonset: {\u0026quot;kind\u0026quot;:\u0026quot;DaemonSetList\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;apps/v1\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;selfLink\u0026quot;:\u0026quot;/apis/apps/v1/namespaces/e2e-tests-daemonsets-tl4wf/daemonsets\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;142028\u0026quot;},\u0026quot;items\u0026quot;:null} Jul 3 06:05:49.507: INFO: pods: {\u0026quot;kind\u0026quot;:\u0026quot;PodList\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;selfLink\u0026quot;:\u0026quot;/api/v1/namespaces/e2e-tests-daemonsets-tl4wf/pods\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;142028\u0026quot;},\u0026quot;items\u0026quot;:null} [AfterEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:05:49.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-daemonsets-tl4wf\u0026quot; for this suite. Jul 3 06:05:55.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:05:55.576: INFO: namespace: e2e-tests-daemonsets-tl4wf, resource: bindings, ignored listing per whitelist Jul 3 06:05:55.647: INFO: namespace e2e-tests-daemonsets-tl4wf deletion completed in 6.132212024s  [SLOW TEST:25.362 seconds] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22 should retry creating failed daemon pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSS ------------------------------ [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:05:55.648: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be able to start watching from a specific resource version [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: creating a new configmap STEP: modifying the configmap once STEP: modifying the configmap a second time STEP: deleting the configmap STEP: creating a watch on configmaps from the resource version returned by the first update STEP: Expecting to observe notifications for all changes to the configmap after the first update Jul 3 06:05:55.725: INFO: Got : MODIFIED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-975cf,SelfLink:/api/v1/namespaces/e2e-tests-watch-975cf/configmaps/e2e-watch-test-resource-version,UID:25f18e8f-7e87-11e8-afec-0800272e6982,ResourceVersion:142061,Generation:0,CreationTimestamp:2018-07-03 06:05:55 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},} Jul 3 06:05:55.725: INFO: Got : DELETED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:e2e-tests-watch-975cf,SelfLink:/api/v1/namespaces/e2e-tests-watch-975cf/configmaps/e2e-watch-test-resource-version,UID:25f18e8f-7e87-11e8-afec-0800272e6982,ResourceVersion:142062,Generation:0,CreationTimestamp:2018-07-03 06:05:55 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},} [AfterEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:05:55.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-watch-975cf\u0026quot; for this suite. Jul 3 06:06:01.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:06:01.845: INFO: namespace: e2e-tests-watch-975cf, resource: bindings, ignored listing per whitelist Jul 3 06:06:01.897: INFO: namespace e2e-tests-watch-975cf deletion completed in 6.169150968s  [SLOW TEST:6.250 seconds] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22 should be able to start watching from a specific resource version [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSSSSSSS ------------------------------ [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-network] Networking /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:06:01.898: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should function for node-pod communication: udp [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Performing setup for networking test in namespace e2e-tests-pod-network-test-gsz2s STEP: creating a selector STEP: Creating the service pods in kubernetes Jul 3 06:06:01.970: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable STEP: Creating test pods Jul 3 06:06:32.080: INFO: ExecWithOptions {Command:[/bin/sh -c echo 'hostName' | timeout -t 2 nc -w 1 -u 192.168.196.147 8081 | grep -v '^\\s*$'] Namespace:e2e-tests-pod-network-test-gsz2s PodName:host-test-container-pod ContainerName:hostexec Stdin:\u0026lt;nil\u0026gt; CaptureStdout:true CaptureStderr:true PreserveWhitespace:false} Jul 3 06:06:32.080: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 Jul 3 06:06:33.341: INFO: Found all expected endpoints: [netserver-0] [AfterEach] [sig-network] Networking /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:06:33.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-pod-network-test-gsz2s\u0026quot; for this suite. Jul 3 06:06:57.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:06:57.453: INFO: namespace: e2e-tests-pod-network-test-gsz2s, resource: bindings, ignored listing per whitelist Jul 3 06:06:57.569: INFO: namespace e2e-tests-pod-network-test-gsz2s deletion completed in 24.222268521s  [SLOW TEST:55.671 seconds] [sig-network] Networking /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25 Granular Checks: Pods /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28 should function for node-pod communication: udp [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSS ------------------------------ [sig-network] Services should serve multiport endpoints from pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-network] Services /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:06:57.569: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-network] Services /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:83 [It] should serve multiport endpoints from pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: creating service multi-endpoint-test in namespace e2e-tests-services-ndg9v STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ndg9v to expose endpoints map[] Jul 3 06:06:57.684: INFO: Get endpoints failed (9.21395ms elapsed, ignoring for 5s): endpoints \u0026quot;multi-endpoint-test\u0026quot; not found Jul 3 06:06:58.688: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ndg9v exposes endpoints map[] (1.01352677s elapsed) STEP: Creating pod pod1 in namespace e2e-tests-services-ndg9v STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ndg9v to expose endpoints map[pod1:[100]] Jul 3 06:07:01.824: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ndg9v exposes endpoints map[pod1:[100]] (3.12548547s elapsed) STEP: Creating pod pod2 in namespace e2e-tests-services-ndg9v STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ndg9v to expose endpoints map[pod2:[101] pod1:[100]] Jul 3 06:07:04.994: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ndg9v exposes endpoints map[pod1:[100] pod2:[101]] (3.156212674s elapsed) STEP: Deleting pod pod1 in namespace e2e-tests-services-ndg9v STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ndg9v to expose endpoints map[pod2:[101]] Jul 3 06:07:06.045: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ndg9v exposes endpoints map[pod2:[101]] (1.037349952s elapsed) STEP: Deleting pod pod2 in namespace e2e-tests-services-ndg9v STEP: waiting up to 3m0s for service multi-endpoint-test in namespace e2e-tests-services-ndg9v to expose endpoints map[] Jul 3 06:07:06.073: INFO: successfully validated that service multi-endpoint-test in namespace e2e-tests-services-ndg9v exposes endpoints map[] (8.460255ms elapsed) [AfterEach] [sig-network] Services /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:07:06.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-services-ndg9v\u0026quot; for this suite. Jul 3 06:07:28.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:07:28.268: INFO: namespace: e2e-tests-services-ndg9v, resource: bindings, ignored listing per whitelist Jul 3 06:07:28.280: INFO: namespace e2e-tests-services-ndg9v deletion completed in 22.170933437s [AfterEach] [sig-network] Services /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88  [SLOW TEST:30.711 seconds] [sig-network] Services /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22 should serve multiport endpoints from pods [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [sig-network] Proxy version v1 should proxy through a service and a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:07:28.280: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should proxy through a service and a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: starting an echo server on multiple ports STEP: creating replication controller proxy-service-24n8l in namespace e2e-tests-proxy-2d6tn I0703 06:07:28.400922 17 runners.go:177] Created replication controller with name: proxy-service-24n8l, namespace: e2e-tests-proxy-2d6tn, replica count: 1 I0703 06:07:29.458438 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I0703 06:07:30.458708 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I0703 06:07:31.459561 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I0703 06:07:32.462091 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I0703 06:07:33.462439 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady I0703 06:07:34.463500 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady I0703 06:07:35.464181 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady I0703 06:07:36.464839 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady I0703 06:07:37.467414 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady I0703 06:07:38.468453 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady I0703 06:07:39.469525 17 runners.go:177] proxy-service-24n8l Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady Jul 3 06:07:39.480: INFO: setup took 11.110959762s, starting test cases STEP: running 16 cases, 20 attempts per case, 320 total attempts Jul 3 06:07:39.505: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 22.984351ms) Jul 3 06:07:39.506: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 23.941675ms) Jul 3 06:07:39.507: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 24.686163ms) Jul 3 06:07:39.508: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 25.541054ms) Jul 3 06:07:39.508: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 26.104103ms) Jul 3 06:07:39.508: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 25.809024ms) Jul 3 06:07:39.510: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 27.843628ms) Jul 3 06:07:39.514: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 31.815332ms) Jul 3 06:07:39.514: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 32.024406ms) Jul 3 06:07:39.522: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 40.252476ms) Jul 3 06:07:39.526: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 43.116815ms) Jul 3 06:07:39.526: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 44.071781ms) Jul 3 06:07:39.530: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 48.139042ms) Jul 3 06:07:39.531: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 49.398749ms) Jul 3 06:07:39.533: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 51.469648ms) Jul 3 06:07:39.536: INFO: (0) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 53.40901ms) Jul 3 06:07:39.540: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 4.093323ms) Jul 3 06:07:39.546: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.210369ms) Jul 3 06:07:39.546: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 9.713037ms) Jul 3 06:07:39.547: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 9.718204ms) Jul 3 06:07:39.547: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 10.382863ms) Jul 3 06:07:39.553: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 16.479995ms) Jul 3 06:07:39.554: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 16.889733ms) Jul 3 06:07:39.554: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 16.893044ms) Jul 3 06:07:39.554: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 16.943884ms) Jul 3 06:07:39.558: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 21.311901ms) Jul 3 06:07:39.558: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 21.148017ms) Jul 3 06:07:39.558: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 21.105103ms) Jul 3 06:07:39.558: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 21.174644ms) Jul 3 06:07:39.559: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 21.825229ms) Jul 3 06:07:39.559: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 22.406455ms) Jul 3 06:07:39.560: INFO: (1) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 23.590981ms) Jul 3 06:07:39.573: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 12.636295ms) Jul 3 06:07:39.573: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 12.124205ms) Jul 3 06:07:39.574: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 12.93577ms) Jul 3 06:07:39.574: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 13.271759ms) Jul 3 06:07:39.576: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 15.487461ms) Jul 3 06:07:39.576: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 15.203026ms) Jul 3 06:07:39.577: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 15.620689ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 16.58259ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 16.64646ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 16.788563ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 16.926443ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 16.935266ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 16.806309ms) Jul 3 06:07:39.578: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 16.770624ms) Jul 3 06:07:39.579: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 17.369824ms) Jul 3 06:07:39.579: INFO: (2) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 17.84942ms) Jul 3 06:07:39.588: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.387169ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.749045ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.636944ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 9.560885ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 9.555833ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 9.908539ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 9.861313ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 9.806847ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.741395ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 10.0292ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 9.918786ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 9.754599ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.915328ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 9.816536ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 9.924633ms) Jul 3 06:07:39.589: INFO: (3) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 9.837021ms) Jul 3 06:07:39.596: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.820563ms) Jul 3 06:07:39.596: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 7.183957ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 8.189952ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 7.875646ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 7.926349ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.070557ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 7.944024ms) Jul 3 06:07:39.597: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.26913ms) Jul 3 06:07:39.598: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 8.600459ms) Jul 3 06:07:39.598: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.958053ms) Jul 3 06:07:39.601: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 12.289458ms) Jul 3 06:07:39.602: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 12.520927ms) Jul 3 06:07:39.603: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 13.80259ms) Jul 3 06:07:39.603: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 13.938307ms) Jul 3 06:07:39.603: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 13.798029ms) Jul 3 06:07:39.603: INFO: (4) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 14.035034ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 8.461128ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 8.598935ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.474331ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 8.53614ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 8.883358ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 8.784084ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 8.655623ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 8.55204ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 8.967046ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 8.628987ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 8.675398ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.737646ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.715562ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 8.658073ms) Jul 3 06:07:39.612: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.684735ms) Jul 3 06:07:39.615: INFO: (5) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 11.408713ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 6.303757ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 6.231001ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.215339ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.256391ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 6.255782ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 6.331148ms) Jul 3 06:07:39.622: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 6.244161ms) Jul 3 06:07:39.623: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 6.875476ms) Jul 3 06:07:39.623: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 7.871699ms) Jul 3 06:07:39.623: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 7.501482ms) Jul 3 06:07:39.624: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 8.340597ms) Jul 3 06:07:39.626: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 9.737824ms) Jul 3 06:07:39.626: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 10.23094ms) Jul 3 06:07:39.627: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 11.440538ms) Jul 3 06:07:39.628: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 12.671053ms) Jul 3 06:07:39.628: INFO: (6) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 12.640137ms) Jul 3 06:07:39.634: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 5.355297ms) Jul 3 06:07:39.634: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 5.14686ms) Jul 3 06:07:39.634: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 5.532388ms) Jul 3 06:07:39.635: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 5.666625ms) Jul 3 06:07:39.635: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 6.029568ms) Jul 3 06:07:39.635: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 5.795261ms) Jul 3 06:07:39.635: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 5.675582ms) Jul 3 06:07:39.637: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 7.198059ms) Jul 3 06:07:39.637: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 8.775757ms) Jul 3 06:07:39.639: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 9.026406ms) Jul 3 06:07:39.639: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 10.491945ms) Jul 3 06:07:39.640: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 10.257608ms) Jul 3 06:07:39.640: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 11.047619ms) Jul 3 06:07:39.640: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.081534ms) Jul 3 06:07:39.641: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.6419ms) Jul 3 06:07:39.641: INFO: (7) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 11.542745ms) Jul 3 06:07:39.649: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 6.703966ms) Jul 3 06:07:39.651: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 8.75293ms) Jul 3 06:07:39.651: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.6867ms) Jul 3 06:07:39.651: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 8.909901ms) Jul 3 06:07:39.651: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.675271ms) Jul 3 06:07:39.652: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.681117ms) Jul 3 06:07:39.652: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 9.795024ms) Jul 3 06:07:39.653: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 10.724903ms) Jul 3 06:07:39.654: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.100304ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 13.017776ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 12.075244ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 13.178331ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 12.246035ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 12.706858ms) Jul 3 06:07:39.655: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 12.642589ms) Jul 3 06:07:39.656: INFO: (8) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 14.20149ms) Jul 3 06:07:39.665: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.341409ms) Jul 3 06:07:39.665: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 7.82984ms) Jul 3 06:07:39.668: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 11.113905ms) Jul 3 06:07:39.669: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 11.8164ms) Jul 3 06:07:39.669: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 11.993539ms) Jul 3 06:07:39.669: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 12.063037ms) Jul 3 06:07:39.669: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 11.62943ms) Jul 3 06:07:39.669: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 11.994793ms) Jul 3 06:07:39.670: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 13.021585ms) Jul 3 06:07:39.670: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 12.795193ms) Jul 3 06:07:39.670: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 12.90527ms) Jul 3 06:07:39.670: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 13.401155ms) Jul 3 06:07:39.671: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 13.758112ms) Jul 3 06:07:39.672: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 14.68876ms) Jul 3 06:07:39.672: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 14.707223ms) Jul 3 06:07:39.672: INFO: (9) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 14.719786ms) Jul 3 06:07:39.680: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 7.570336ms) Jul 3 06:07:39.680: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 7.626954ms) Jul 3 06:07:39.680: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.072456ms) Jul 3 06:07:39.681: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 8.276817ms) Jul 3 06:07:39.681: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.69902ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 9.244248ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.578596ms) Jul 3 06:07:39.683: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 10.467993ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 9.630068ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.655504ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 9.639837ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.798602ms) Jul 3 06:07:39.682: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 10.221717ms) Jul 3 06:07:39.683: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 10.912476ms) Jul 3 06:07:39.685: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 12.368545ms) Jul 3 06:07:39.686: INFO: (10) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 13.706676ms) Jul 3 06:07:39.690: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 3.70745ms) Jul 3 06:07:39.690: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 3.701704ms) Jul 3 06:07:39.691: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 3.83924ms) Jul 3 06:07:39.695: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 7.9486ms) Jul 3 06:07:39.695: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 8.292899ms) Jul 3 06:07:39.695: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 8.22671ms) Jul 3 06:07:39.695: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 8.145833ms) Jul 3 06:07:39.697: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 9.445816ms) Jul 3 06:07:39.697: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.513803ms) Jul 3 06:07:39.697: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.608955ms) Jul 3 06:07:39.697: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 9.564723ms) Jul 3 06:07:39.698: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.345681ms) Jul 3 06:07:39.698: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 11.329885ms) Jul 3 06:07:39.698: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 11.28516ms) Jul 3 06:07:39.699: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.862803ms) Jul 3 06:07:39.700: INFO: (11) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 12.641143ms) Jul 3 06:07:39.705: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 5.328836ms) Jul 3 06:07:39.705: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 5.512445ms) Jul 3 06:07:39.705: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 5.466995ms) Jul 3 06:07:39.705: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 5.65714ms) Jul 3 06:07:39.706: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 5.743735ms) Jul 3 06:07:39.706: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 5.82226ms) Jul 3 06:07:39.710: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 10.434795ms) Jul 3 06:07:39.711: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 10.698721ms) Jul 3 06:07:39.711: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.01204ms) Jul 3 06:07:39.711: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 10.767322ms) Jul 3 06:07:39.711: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 11.191239ms) Jul 3 06:07:39.711: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.547994ms) Jul 3 06:07:39.712: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 11.803176ms) Jul 3 06:07:39.713: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 13.341002ms) Jul 3 06:07:39.714: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 13.620961ms) Jul 3 06:07:39.714: INFO: (12) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 14.058587ms) Jul 3 06:07:39.721: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.424664ms) Jul 3 06:07:39.721: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 6.732971ms) Jul 3 06:07:39.721: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 6.528326ms) Jul 3 06:07:39.724: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 10.162831ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 10.199442ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 10.184131ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 10.113374ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 10.311161ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 10.212956ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 10.170489ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 10.218579ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 10.234843ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 10.350547ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 10.310824ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 10.418044ms) Jul 3 06:07:39.725: INFO: (13) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 10.558637ms) Jul 3 06:07:39.731: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 5.861544ms) Jul 3 06:07:39.731: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 6.24859ms) Jul 3 06:07:39.731: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.145305ms) Jul 3 06:07:39.731: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.30522ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.548633ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.725206ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 9.695757ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 9.592566ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 9.739772ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 9.605244ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 9.644933ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.790708ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.603307ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 9.581454ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 9.640984ms) Jul 3 06:07:39.735: INFO: (14) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 9.779968ms) Jul 3 06:07:39.742: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.455086ms) Jul 3 06:07:39.742: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.782064ms) Jul 3 06:07:39.742: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.747632ms) Jul 3 06:07:39.746: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 10.785384ms) Jul 3 06:07:39.746: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 10.93803ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 12.250607ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 12.166366ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 12.247814ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 12.110371ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 12.327679ms) Jul 3 06:07:39.746: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 11.036002ms) Jul 3 06:07:39.747: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.662474ms) Jul 3 06:07:39.747: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 11.277015ms) Jul 3 06:07:39.748: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 12.998036ms) Jul 3 06:07:39.749: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 13.208001ms) Jul 3 06:07:39.749: INFO: (15) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 13.737721ms) Jul 3 06:07:39.754: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 4.358522ms) Jul 3 06:07:39.756: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 6.104772ms) Jul 3 06:07:39.756: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 6.422685ms) Jul 3 06:07:39.758: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 7.919086ms) Jul 3 06:07:39.758: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 7.634486ms) Jul 3 06:07:39.758: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 7.990733ms) Jul 3 06:07:39.759: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 8.268858ms) Jul 3 06:07:39.759: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 9.35517ms) Jul 3 06:07:39.759: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 8.830292ms) Jul 3 06:07:39.760: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 9.816839ms) Jul 3 06:07:39.760: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.810505ms) Jul 3 06:07:39.760: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 10.193995ms) Jul 3 06:07:39.760: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 9.776765ms) Jul 3 06:07:39.762: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 10.961793ms) Jul 3 06:07:39.762: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.54841ms) Jul 3 06:07:39.763: INFO: (16) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 12.641653ms) Jul 3 06:07:39.768: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 4.299456ms) Jul 3 06:07:39.768: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 4.739242ms) Jul 3 06:07:39.769: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 5.472944ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.407085ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 8.519286ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.534537ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 8.113633ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 8.43944ms) Jul 3 06:07:39.772: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 8.222509ms) Jul 3 06:07:39.773: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 9.509357ms) Jul 3 06:07:39.775: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 11.17629ms) Jul 3 06:07:39.775: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.463021ms) Jul 3 06:07:39.776: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 11.811409ms) Jul 3 06:07:39.776: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 12.431881ms) Jul 3 06:07:39.776: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 12.664851ms) Jul 3 06:07:39.778: INFO: (17) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 13.881855ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 6.595958ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.626811ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 6.936054ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 6.663949ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 6.88998ms) Jul 3 06:07:39.785: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 6.722557ms) Jul 3 06:07:39.786: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 7.331539ms) Jul 3 06:07:39.788: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 9.9549ms) Jul 3 06:07:39.788: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 10.735268ms) Jul 3 06:07:39.789: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.872255ms) Jul 3 06:07:39.789: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 10.530949ms) Jul 3 06:07:39.790: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.578224ms) Jul 3 06:07:39.791: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 12.322089ms) Jul 3 06:07:39.792: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 13.214314ms) Jul 3 06:07:39.792: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 13.466455ms) Jul 3 06:07:39.793: INFO: (18) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 14.087847ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.516847ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:443/proxy/... (200; 7.986127ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.050382ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:462/proxy/: tls qux (200; 8.296865ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:1080/proxy/rewri... (200; 8.457665ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:160/proxy/: foo (200; 8.847864ms) Jul 3 06:07:39.802: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/http:proxy-service-24n8l-qqhrc:1080/proxy/... (200; 8.533851ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname1/proxy/: tls baz (200; 8.482214ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname2/proxy/: bar (200; 8.778312ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/https:proxy-service-24n8l-qqhrc:460/proxy/: tls baz (200; 8.967689ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc:162/proxy/: bar (200; 8.669839ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/: \u0026lt;a href=\u0026quot;/api/v1/namespaces/e2e-tests-proxy-2d6tn/pods/proxy-service-24n8l-qqhrc/proxy/rewriteme\u0026quot;... (200; 9.392164ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname1/proxy/: foo (200; 9.888833ms) Jul 3 06:07:39.803: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/proxy-service-24n8l:portname2/proxy/: bar (200; 9.632785ms) Jul 3 06:07:39.804: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/http:proxy-service-24n8l:portname1/proxy/: foo (200; 10.481342ms) Jul 3 06:07:39.806: INFO: (19) /api/v1/namespaces/e2e-tests-proxy-2d6tn/services/https:proxy-service-24n8l:tlsportname2/proxy/: tls qux (200; 11.660339ms) STEP: deleting { ReplicationController} proxy-service-24n8l in namespace e2e-tests-proxy-2d6tn, will wait for the garbage collector to delete the pods Jul 3 06:07:39.885: INFO: Deleting { ReplicationController} proxy-service-24n8l took: 24.687435ms Jul 3 06:07:39.986: INFO: Terminating { ReplicationController} proxy-service-24n8l pods took: 101.657967ms [AfterEach] version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:07:49.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-proxy-2d6tn\u0026quot; for this suite. Jul 3 06:07:55.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:07:55.580: INFO: namespace: e2e-tests-proxy-2d6tn, resource: bindings, ignored listing per whitelist Jul 3 06:07:55.644: INFO: namespace e2e-tests-proxy-2d6tn deletion completed in 6.149415761s  [SLOW TEST:27.364 seconds] [sig-network] Proxy /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22 version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56 should proxy through a service and a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [sig-storage] Projected should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:07:55.647: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858 [It] should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating projection with secret that has name projected-secret-test-map-6d7ebdb8-7e87-11e8-9aa2-c6770944a2e6 STEP: Creating a pod to test consume secrets Jul 3 06:07:55.763: INFO: Waiting up to 5m0s for pod \u0026quot;pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-projected-k8m2l\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:07:55.778: INFO: Pod \u0026quot;pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 15.231796ms Jul 3 06:07:57.786: INFO: Pod \u0026quot;pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.02288338s Jul 3 06:07:59.790: INFO: Pod \u0026quot;pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.026687063s STEP: Saw pod success Jul 3 06:07:59.790: INFO: Pod \u0026quot;pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:07:59.792: INFO: Trying to get logs from node ubuntuvm pod pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6 container projected-secret-volume-test: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:07:59.813: INFO: Waiting for pod pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:07:59.814: INFO: Pod pod-projected-secrets-6d7f63bd-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:07:59.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-projected-k8m2l\u0026quot; for this suite. Jul 3 06:08:05.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:08:05.925: INFO: namespace: e2e-tests-projected-k8m2l, resource: bindings, ignored listing per whitelist Jul 3 06:08:05.991: INFO: namespace e2e-tests-projected-k8m2l deletion completed in 6.175208514s  [SLOW TEST:10.345 seconds] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34 should be consumable from pods in volume with mappings and Item Mode set [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSS ------------------------------ [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-api-machinery] Garbage collector /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:08:05.996: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: create the rc1 STEP: create the rc2 STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well STEP: delete the rc simpletest-rc-to-be-deleted STEP: wait for the rc to be deleted STEP: Gathering metrics W0703 06:08:16.225194 17 metrics_grabber.go:81] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled. Jul 3 06:08:16.225: INFO: For apiserver_request_count: For apiserver_request_latencies_summary: For etcd_helper_cache_entry_count: For etcd_helper_cache_hit_count: For etcd_helper_cache_miss_count: For etcd_request_cache_add_latencies_summary: For etcd_request_cache_get_latencies_summary: For etcd_request_latencies_summary: For garbage_collector_attempt_to_delete_queue_latency: For garbage_collector_attempt_to_delete_work_duration: For garbage_collector_attempt_to_orphan_queue_latency: For garbage_collector_attempt_to_orphan_work_duration: For garbage_collector_dirty_processing_latency_microseconds: For garbage_collector_event_processing_latency_microseconds: For garbage_collector_graph_changes_queue_latency: For garbage_collector_graph_changes_work_duration: For garbage_collector_orphan_processing_latency_microseconds: For namespace_queue_latency: For namespace_queue_latency_sum: For namespace_queue_latency_count: For namespace_retries: For namespace_work_duration: For namespace_work_duration_sum: For namespace_work_duration_count: For function_duration_seconds: For errors_total: For evicted_pods_total: [AfterEach] [sig-api-machinery] Garbage collector /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:08:16.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-gc-x2qjc\u0026quot; for this suite. Jul 3 06:08:22.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:08:22.361: INFO: namespace: e2e-tests-gc-x2qjc, resource: bindings, ignored listing per whitelist Jul 3 06:08:22.433: INFO: namespace e2e-tests-gc-x2qjc deletion completed in 6.18451645s  [SLOW TEST:16.437 seconds] [sig-api-machinery] Garbage collector /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22 should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ S ------------------------------ [sig-network] Proxy version v1 should proxy logs on node using proxy subresource [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:08:22.433: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should proxy logs on node using proxy subresource [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 Jul 3 06:08:22.521: INFO: (0) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 6.888637ms) Jul 3 06:08:22.526: INFO: (1) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 4.483475ms) Jul 3 06:08:22.530: INFO: (2) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 4.032924ms) Jul 3 06:08:22.535: INFO: (3) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 4.970921ms) Jul 3 06:08:22.542: INFO: (4) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 6.702057ms) Jul 3 06:08:22.547: INFO: (5) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 4.620394ms) Jul 3 06:08:22.564: INFO: (6) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 17.443485ms) Jul 3 06:08:22.576: INFO: (7) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 9.188638ms) Jul 3 06:08:22.593: INFO: (8) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 17.254712ms) Jul 3 06:08:22.599: INFO: (9) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 5.990922ms) Jul 3 06:08:22.609: INFO: (10) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 9.945971ms) Jul 3 06:08:22.618: INFO: (11) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 6.843684ms) Jul 3 06:08:22.627: INFO: (12) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 8.030184ms) Jul 3 06:08:22.654: INFO: (13) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 27.455682ms) Jul 3 06:08:22.661: INFO: (14) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 6.839686ms) Jul 3 06:08:22.670: INFO: (15) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 8.531996ms) Jul 3 06:08:22.675: INFO: (16) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 5.599557ms) Jul 3 06:08:22.685: INFO: (17) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 10.221834ms) Jul 3 06:08:22.710: INFO: (18) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 20.052405ms) Jul 3 06:08:22.716: INFO: (19) /api/v1/nodes/ubuntuvm/proxy/logs/: \u0026lt;pre\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log\u0026quot;\u0026gt;Xorg.0.log\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;Xorg.0.log.old\u0026quot;\u0026gt;Xorg.0.log.old\u0026lt;/a\u0026gt; \u0026lt;a href=\u0026quot;al... (200; 5.632301ms) [AfterEach] version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:08:22.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-proxy-62wvl\u0026quot; for this suite. Jul 3 06:08:28.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:08:28.795: INFO: namespace: e2e-tests-proxy-62wvl, resource: bindings, ignored listing per whitelist Jul 3 06:08:28.850: INFO: namespace e2e-tests-proxy-62wvl deletion completed in 6.127164652s  [SLOW TEST:6.417 seconds] [sig-network] Proxy /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:22 version v1 /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:56 should proxy logs on node using proxy subresource [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSS ------------------------------ [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:08:28.851: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] volume on default medium should have the correct mode [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test emptydir volume type on node default medium Jul 3 06:08:28.932: INFO: Waiting up to 5m0s for pod \u0026quot;pod-81443822-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-emptydir-2qs2r\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:08:28.947: INFO: Pod \u0026quot;pod-81443822-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 15.043415ms Jul 3 06:08:30.957: INFO: Pod \u0026quot;pod-81443822-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.02443708s Jul 3 06:08:32.988: INFO: Pod \u0026quot;pod-81443822-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.056026943s STEP: Saw pod success Jul 3 06:08:32.988: INFO: Pod \u0026quot;pod-81443822-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:08:32.994: INFO: Trying to get logs from node ubuntuvm pod pod-81443822-7e87-11e8-9aa2-c6770944a2e6 container test-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:08:33.067: INFO: Waiting for pod pod-81443822-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:08:33.103: INFO: Pod pod-81443822-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:08:33.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-emptydir-2qs2r\u0026quot; for this suite. Jul 3 06:08:39.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:08:39.311: INFO: namespace: e2e-tests-emptydir-2qs2r, resource: bindings, ignored listing per whitelist Jul 3 06:08:39.351: INFO: namespace e2e-tests-emptydir-2qs2r deletion completed in 6.242001093s  [SLOW TEST:10.500 seconds] [sig-storage] EmptyDir volumes /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:40 volume on default medium should have the correct mode [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ S ------------------------------ [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:08:39.351: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:79 Jul 3 06:08:39.469: INFO: Waiting up to 1m0s for all nodes to be ready Jul 3 06:09:39.502: INFO: Waiting for terminating namespaces to be deleted... Jul 3 06:09:39.511: INFO: Waiting up to 5m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready Jul 3 06:09:39.525: INFO: 11 / 11 pods in namespace 'kube-system' are running and ready (0 seconds elapsed) Jul 3 06:09:39.525: INFO: expected 4 pod replicas in namespace 'kube-system', 4 are Running and Ready. Jul 3 06:09:39.530: INFO: Waiting for pods to enter Success, but no pods in \u0026quot;kube-system\u0026quot; match label map[name:e2e-image-puller] Jul 3 06:09:39.530: INFO: Logging pods the kubelet thinks is on node ubuntuvm before test Jul 3 06:09:39.556: INFO: calico-node-8j695 from kube-system started at 2018-06-30 19:28:15 +0000 UTC (2 container statuses recorded) Jul 3 06:09:39.556: INFO: Container calico-node ready: true, restart count 5 Jul 3 06:09:39.556: INFO: Container install-cni ready: true, restart count 4 Jul 3 06:09:39.557: INFO: calico-kube-controllers-84fd4db7cd-m6hld from kube-system started at 2018-06-30 19:29:23 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.557: INFO: Container calico-kube-controllers ready: true, restart count 4 Jul 3 06:09:39.557: INFO: coredns-78fcdf6894-85qtr from kube-system started at 2018-06-30 19:29:23 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.557: INFO: Container coredns ready: true, restart count 4 Jul 3 06:09:39.557: INFO: tiller-deploy-759cb9df9-cdqkj from kube-system started at 2018-06-30 19:47:09 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.557: INFO: Container tiller ready: true, restart count 4 Jul 3 06:09:39.557: INFO: virtuous-quail-kubeplay-679d96fd55-cjhbx from default started at 2018-06-30 20:16:04 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.557: INFO: Container kubeplay ready: true, restart count 4 Jul 3 06:09:39.557: INFO: sonobuoy-e2e-job-5fff584d11364ca1 from heptio-sonobuoy started at 2018-07-03 06:00:23 +0000 UTC (2 container statuses recorded) Jul 3 06:09:39.557: INFO: Container e2e ready: true, restart count 0 Jul 3 06:09:39.558: INFO: Container sonobuoy-worker ready: true, restart count 0 Jul 3 06:09:39.558: INFO: kube-apiserver-ubuntuvm from kube-system started at \u0026lt;nil\u0026gt; (0 container statuses recorded) Jul 3 06:09:39.558: INFO: coredns-78fcdf6894-4qwq6 from kube-system started at 2018-06-30 19:29:23 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.558: INFO: Container coredns ready: true, restart count 4 Jul 3 06:09:39.558: INFO: kube-proxy-mjsc2 from kube-system started at 2018-06-30 19:26:56 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.558: INFO: Container kube-proxy ready: true, restart count 4 Jul 3 06:09:39.558: INFO: calico-etcd-gctks from kube-system started at 2018-06-30 19:28:14 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.558: INFO: Container calico-etcd ready: true, restart count 4 Jul 3 06:09:39.558: INFO: sonobuoy from heptio-sonobuoy started at 2018-07-03 05:59:09 +0000 UTC (1 container statuses recorded) Jul 3 06:09:39.558: INFO: Container kube-sonobuoy ready: true, restart count 0 Jul 3 06:09:39.558: INFO: sonobuoy-systemd-logs-daemon-set-1c53f31cf14246ca-mhztp from heptio-sonobuoy started at 2018-07-03 06:00:23 +0000 UTC (2 container statuses recorded) Jul 3 06:09:39.558: INFO: Container sonobuoy-systemd-logs-config ready: true, restart count 0 Jul 3 06:09:39.558: INFO: Container sonobuoy-worker ready: true, restart count 0 Jul 3 06:09:39.558: INFO: kube-controller-manager-ubuntuvm from kube-system started at \u0026lt;nil\u0026gt; (0 container statuses recorded) Jul 3 06:09:39.558: INFO: kube-scheduler-ubuntuvm from kube-system started at \u0026lt;nil\u0026gt; (0 container statuses recorded) Jul 3 06:09:39.558: INFO: etcd-ubuntuvm from kube-system started at \u0026lt;nil\u0026gt; (0 container statuses recorded) [It] validates that NodeSelector is respected if not matching [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Trying to schedule Pod with nonempty NodeSelector. STEP: Considering event: Type = [Warning], Name = [restricted-pod.153dc6ef571069ad], Reason = [FailedScheduling], Message = [0/1 nodes are available: 1 node(s) didn't match node selector.] [AfterEach] [sig-scheduling] SchedulerPredicates [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:09:40.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-sched-pred-8n5wv\u0026quot; for this suite. Jul 3 06:10:02.675: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:10:02.799: INFO: namespace: e2e-tests-sched-pred-8n5wv, resource: bindings, ignored listing per whitelist Jul 3 06:10:02.856: INFO: namespace e2e-tests-sched-pred-8n5wv deletion completed in 22.200268317s [AfterEach] [sig-scheduling] SchedulerPredicates [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:70  [SLOW TEST:83.506 seconds] [sig-scheduling] SchedulerPredicates [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:22 validates that NodeSelector is respected if not matching [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSSSSSSSSSSSSSSSSSS ------------------------------ [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:10:02.862: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:99 [It] should run and stop simple daemon [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating simple DaemonSet \u0026quot;daemon-set\u0026quot; STEP: Check that daemon pods launch on every node of the cluster. Jul 3 06:10:02.990: INFO: Number of nodes with available pods: 0 Jul 3 06:10:02.990: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:03.996: INFO: Number of nodes with available pods: 0 Jul 3 06:10:03.996: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:05.001: INFO: Number of nodes with available pods: 1 Jul 3 06:10:05.001: INFO: Number of running nodes: 1, number of available pods: 1 STEP: Stop a daemon pod, check that the daemon pod is revived. Jul 3 06:10:05.035: INFO: Number of nodes with available pods: 0 Jul 3 06:10:05.035: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:06.058: INFO: Number of nodes with available pods: 0 Jul 3 06:10:06.059: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:07.050: INFO: Number of nodes with available pods: 0 Jul 3 06:10:07.050: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:08.052: INFO: Number of nodes with available pods: 0 Jul 3 06:10:08.052: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:09.043: INFO: Number of nodes with available pods: 0 Jul 3 06:10:09.043: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:10.059: INFO: Number of nodes with available pods: 0 Jul 3 06:10:10.060: INFO: Node ubuntuvm is running more than one daemon pod Jul 3 06:10:11.054: INFO: Number of nodes with available pods: 1 Jul 3 06:10:11.055: INFO: Number of running nodes: 1, number of available pods: 1 [AfterEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:65 STEP: Deleting DaemonSet \u0026quot;daemon-set\u0026quot; STEP: deleting {extensions DaemonSet} daemon-set in namespace e2e-tests-daemonsets-xnztd, will wait for the garbage collector to delete the pods Jul 3 06:10:11.157: INFO: Deleting {extensions DaemonSet} daemon-set took: 30.388855ms Jul 3 06:10:11.259: INFO: Terminating {extensions DaemonSet} daemon-set pods took: 101.655658ms Jul 3 06:10:19.876: INFO: Number of nodes with available pods: 0 Jul 3 06:10:19.876: INFO: Number of running nodes: 0, number of available pods: 0 Jul 3 06:10:19.878: INFO: daemonset: {\u0026quot;kind\u0026quot;:\u0026quot;DaemonSetList\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;apps/v1\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;selfLink\u0026quot;:\u0026quot;/apis/apps/v1/namespaces/e2e-tests-daemonsets-xnztd/daemonsets\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;142861\u0026quot;},\u0026quot;items\u0026quot;:null} Jul 3 06:10:19.881: INFO: pods: {\u0026quot;kind\u0026quot;:\u0026quot;PodList\u0026quot;,\u0026quot;apiVersion\u0026quot;:\u0026quot;v1\u0026quot;,\u0026quot;metadata\u0026quot;:{\u0026quot;selfLink\u0026quot;:\u0026quot;/api/v1/namespaces/e2e-tests-daemonsets-xnztd/pods\u0026quot;,\u0026quot;resourceVersion\u0026quot;:\u0026quot;142861\u0026quot;},\u0026quot;items\u0026quot;:null} [AfterEach] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:10:19.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-daemonsets-xnztd\u0026quot; for this suite. Jul 3 06:10:25.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:10:25.921: INFO: namespace: e2e-tests-daemonsets-xnztd, resource: bindings, ignored listing per whitelist Jul 3 06:10:26.002: INFO: namespace e2e-tests-daemonsets-xnztd deletion completed in 6.112351584s  [SLOW TEST:22.727 seconds] [sig-apps] Daemon set [Serial] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:22 should run and stop simple daemon [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [sig-api-machinery] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-api-machinery] Downward API /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:10:26.003: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test downward api env vars Jul 3 06:10:26.099: INFO: Waiting up to 5m0s for pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-downward-api-mj4ht\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:10:26.109: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 9.782819ms Jul 3 06:10:28.113: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.01420854s Jul 3 06:10:30.120: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.021268684s Jul 3 06:10:32.128: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 6.028757065s Jul 3 06:10:34.136: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 8.036636044s STEP: Saw pod success Jul 3 06:10:34.136: INFO: Pod \u0026quot;downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:10:34.140: INFO: Trying to get logs from node ubuntuvm pod downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6 container dapi-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:10:34.190: INFO: Waiting for pod downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:10:34.194: INFO: Pod downward-api-c71ad49f-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-api-machinery] Downward API /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:10:34.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-downward-api-mj4ht\u0026quot; for this suite. Jul 3 06:10:40.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:10:40.296: INFO: namespace: e2e-tests-downward-api-mj4ht, resource: bindings, ignored listing per whitelist Jul 3 06:10:40.383: INFO: namespace e2e-tests-downward-api-mj4ht deletion completed in 6.177052302s  [SLOW TEST:14.379 seconds] [sig-api-machinery] Downward API /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:37 should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [k8s.io] Docker Containers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:10:40.383: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test override all Jul 3 06:10:40.503: INFO: Waiting up to 5m0s for pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-containers-4d7qg\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:10:40.510: INFO: Pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 7.185996ms Jul 3 06:10:42.520: INFO: Pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.016708124s Jul 3 06:10:44.525: INFO: Pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.022154595s Jul 3 06:10:46.529: INFO: Pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 6.025953515s STEP: Saw pod success Jul 3 06:10:46.529: INFO: Pod \u0026quot;client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:10:46.532: INFO: Trying to get logs from node ubuntuvm pod client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6 container test-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:10:46.567: INFO: Waiting for pod client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:10:46.589: INFO: Pod client-containers-cfb0845a-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [k8s.io] Docker Containers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:10:46.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-containers-4d7qg\u0026quot; for this suite. Jul 3 06:10:52.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:10:52.757: INFO: namespace: e2e-tests-containers-4d7qg, resource: bindings, ignored listing per whitelist Jul 3 06:10:52.784: INFO: namespace e2e-tests-containers-4d7qg deletion completed in 6.190371691s  [SLOW TEST:12.401 seconds] [k8s.io] Docker Containers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 should be able to override the image's default command and arguments [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSS ------------------------------ [k8s.io] [sig-node] PreStop should call prestop when killing a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [k8s.io] [sig-node] PreStop /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:10:52.787: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should call prestop when killing a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating server pod server in namespace e2e-tests-prestop-bjnd8 STEP: Waiting for pods to come up. STEP: Creating tester pod tester in namespace e2e-tests-prestop-bjnd8 STEP: Deleting pre-stop pod Jul 3 06:11:11.921: INFO: Saw: { \u0026quot;Hostname\u0026quot;: \u0026quot;server\u0026quot;, \u0026quot;Sent\u0026quot;: null, \u0026quot;Received\u0026quot;: { \u0026quot;prestop\u0026quot;: 1 }, \u0026quot;Errors\u0026quot;: null, \u0026quot;Log\u0026quot;: [ \u0026quot;default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.\u0026quot;, \u0026quot;default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.\u0026quot;, \u0026quot;default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.\u0026quot; ], \u0026quot;StillContactingPeers\u0026quot;: true } STEP: Deleting the server pod [AfterEach] [k8s.io] [sig-node] PreStop /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:11:11.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-prestop-bjnd8\u0026quot; for this suite. Jul 3 06:11:49.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:11:49.991: INFO: namespace: e2e-tests-prestop-bjnd8, resource: bindings, ignored listing per whitelist Jul 3 06:11:50.081: INFO: namespace e2e-tests-prestop-bjnd8 deletion completed in 38.147106656s  [SLOW TEST:57.295 seconds] [k8s.io] [sig-node] PreStop /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 should call prestop when killing a pod [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SS ------------------------------ [sig-storage] Projected should provide container's memory limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:11:50.083: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:858 [It] should provide container's memory limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test downward API volume plugin Jul 3 06:11:50.158: INFO: Waiting up to 5m0s for pod \u0026quot;downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-projected-4pdz5\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:11:50.162: INFO: Pod \u0026quot;downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 3.921209ms Jul 3 06:11:52.164: INFO: Pod \u0026quot;downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.006433292s Jul 3 06:11:54.168: INFO: Pod \u0026quot;downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.010529015s STEP: Saw pod success Jul 3 06:11:54.169: INFO: Pod \u0026quot;downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:11:54.172: INFO: Trying to get logs from node ubuntuvm pod downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6 container client-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:11:54.197: INFO: Waiting for pod downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:11:54.199: INFO: Pod downwardapi-volume-f935879b-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:11:54.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-projected-4pdz5\u0026quot; for this suite. Jul 3 06:12:00.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:12:00.251: INFO: namespace: e2e-tests-projected-4pdz5, resource: bindings, ignored listing per whitelist Jul 3 06:12:00.404: INFO: namespace e2e-tests-projected-4pdz5 deletion completed in 6.2014029s  [SLOW TEST:10.322 seconds] [sig-storage] Projected /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected.go:34 should provide container's memory limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] Downward API volume /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:12:00.405: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-storage] Downward API volume /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:38 [It] should provide container's cpu limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a pod to test downward API volume plugin Jul 3 06:12:00.562: INFO: Waiting up to 5m0s for pod \u0026quot;downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-downward-api-98rjf\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:12:00.577: INFO: Pod \u0026quot;downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 14.56459ms Jul 3 06:12:02.589: INFO: Pod \u0026quot;downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.026525202s Jul 3 06:12:04.604: INFO: Pod \u0026quot;downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.041917935s STEP: Saw pod success Jul 3 06:12:04.605: INFO: Pod \u0026quot;downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:12:04.612: INFO: Trying to get logs from node ubuntuvm pod downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6 container client-container: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:12:04.680: INFO: Waiting for pod downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:12:04.684: INFO: Pod downwardapi-volume-ff663e88-7e87-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] Downward API volume /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:12:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-downward-api-98rjf\u0026quot; for this suite. Jul 3 06:12:10.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:12:10.846: INFO: namespace: e2e-tests-downward-api-98rjf, resource: bindings, ignored listing per whitelist Jul 3 06:12:10.852: INFO: namespace e2e-tests-downward-api-98rjf deletion completed in 6.163639204s  [SLOW TEST:10.447 seconds] [sig-storage] Downward API volume /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:33 should provide container's cpu limit [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SS ------------------------------ [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:12:10.852: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: creating a watch on configmaps with a certain label STEP: creating a new configmap STEP: modifying the configmap once STEP: changing the label value of the configmap STEP: Expecting to observe a delete notification for the watched object Jul 3 06:12:10.933: INFO: Got : ADDED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143167,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},} Jul 3 06:12:10.933: INFO: Got : MODIFIED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143168,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},} Jul 3 06:12:10.934: INFO: Got : DELETED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143169,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},} STEP: modifying the configmap a second time STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements STEP: changing the label value of the configmap back STEP: modifying the configmap a third time STEP: deleting the configmap STEP: Expecting to observe an add notification for the watched object when the label value was restored Jul 3 06:12:20.976: INFO: Got : ADDED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143183,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},} Jul 3 06:12:20.982: INFO: Got : MODIFIED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143184,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},} Jul 3 06:12:20.985: INFO: Got : DELETED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:e2e-tests-watch-q7f6p,SelfLink:/api/v1/namespaces/e2e-tests-watch-q7f6p/configmaps/e2e-watch-test-label-changed,UID:0595b33e-7e88-11e8-afec-0800272e6982,ResourceVersion:143185,Generation:0,CreationTimestamp:2018-07-03 06:12:10 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},} [AfterEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:12:20.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-watch-q7f6p\u0026quot; for this suite. Jul 3 06:12:27.011: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:12:27.117: INFO: namespace: e2e-tests-watch-q7f6p, resource: bindings, ignored listing per whitelist Jul 3 06:12:27.192: INFO: namespace e2e-tests-watch-q7f6p deletion completed in 6.198067444s  [SLOW TEST:16.340 seconds] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22 should observe an object deletion if it stops meeting the requirements of the selector [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSS ------------------------------ [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:12:27.193: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:48 [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating pod liveness-http in namespace e2e-tests-container-probe-t89js Jul 3 06:12:31.325: INFO: Started pod liveness-http in namespace e2e-tests-container-probe-t89js STEP: checking the pod's current state and verifying that restartCount is present Jul 3 06:12:31.331: INFO: Initial restart count of pod liveness-http is 0 Jul 3 06:12:51.443: INFO: Restart count of pod e2e-tests-container-probe-t89js/liveness-http is now 1 (20.111946656s elapsed) STEP: deleting the pod [AfterEach] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:12:51.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-container-probe-t89js\u0026quot; for this suite. Jul 3 06:12:57.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:12:57.637: INFO: namespace: e2e-tests-container-probe-t89js, resource: bindings, ignored listing per whitelist Jul 3 06:12:57.654: INFO: namespace e2e-tests-container-probe-t89js deletion completed in 6.151550499s  [SLOW TEST:30.462 seconds] [k8s.io] Probing container /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:679 should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSSSSSSSSS ------------------------------ [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:12:57.658: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: creating a watch on configmaps STEP: creating a new configmap STEP: modifying the configmap once STEP: closing the watch once it receives two notifications Jul 3 06:12:57.744: INFO: Got : ADDED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xb2xg,SelfLink:/api/v1/namespaces/e2e-tests-watch-xb2xg/configmaps/e2e-watch-test-watch-closed,UID:217d94bb-7e88-11e8-afec-0800272e6982,ResourceVersion:143270,Generation:0,CreationTimestamp:2018-07-03 06:12:57 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{},BinaryData:map[string][]byte{},} Jul 3 06:12:57.745: INFO: Got : MODIFIED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xb2xg,SelfLink:/api/v1/namespaces/e2e-tests-watch-xb2xg/configmaps/e2e-watch-test-watch-closed,UID:217d94bb-7e88-11e8-afec-0800272e6982,ResourceVersion:143271,Generation:0,CreationTimestamp:2018-07-03 06:12:57 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},} STEP: modifying the configmap a second time, while the watch is closed STEP: creating a new watch on configmaps from the last resource version observed by the first watch STEP: deleting the configmap STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed Jul 3 06:12:57.762: INFO: Got : MODIFIED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xb2xg,SelfLink:/api/v1/namespaces/e2e-tests-watch-xb2xg/configmaps/e2e-watch-test-watch-closed,UID:217d94bb-7e88-11e8-afec-0800272e6982,ResourceVersion:143272,Generation:0,CreationTimestamp:2018-07-03 06:12:57 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},} Jul 3 06:12:57.762: INFO: Got : DELETED \u0026amp;ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:e2e-tests-watch-xb2xg,SelfLink:/api/v1/namespaces/e2e-tests-watch-xb2xg/configmaps/e2e-watch-test-watch-closed,UID:217d94bb-7e88-11e8-afec-0800272e6982,ResourceVersion:143273,Generation:0,CreationTimestamp:2018-07-03 06:12:57 +0000 UTC,DeletionTimestamp:\u0026lt;nil\u0026gt;,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},} [AfterEach] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:12:57.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-watch-xb2xg\u0026quot; for this suite. Jul 3 06:13:03.794: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:13:03.920: INFO: namespace: e2e-tests-watch-xb2xg, resource: bindings, ignored listing per whitelist Jul 3 06:13:04.011: INFO: namespace e2e-tests-watch-xb2xg deletion completed in 6.245271295s  [SLOW TEST:6.353 seconds] [sig-api-machinery] Watchers /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:22 should be able to restart watching from the last resource version observed by the previous watch [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSSSSSSSSSSSS ------------------------------ [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:13:04.011: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating configMap with name configmap-test-volume-2541dc76-7e88-11e8-9aa2-c6770944a2e6 STEP: Creating a pod to test consume configMaps Jul 3 06:13:04.062: INFO: Waiting up to 5m0s for pod \u0026quot;pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6\u0026quot; in namespace \u0026quot;e2e-tests-configmap-nw9xh\u0026quot; to be \u0026quot;success or failure\u0026quot; Jul 3 06:13:04.068: INFO: Pod \u0026quot;pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 5.263313ms Jul 3 06:13:06.086: INFO: Pod \u0026quot;pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Pending\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 2.023042273s Jul 3 06:13:08.093: INFO: Pod \u0026quot;pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6\u0026quot;: Phase=\u0026quot;Succeeded\u0026quot;, Reason=\u0026quot;\u0026quot;, readiness=false. Elapsed: 4.030624253s STEP: Saw pod success Jul 3 06:13:08.094: INFO: Pod \u0026quot;pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6\u0026quot; satisfied condition \u0026quot;success or failure\u0026quot; Jul 3 06:13:08.099: INFO: Trying to get logs from node ubuntuvm pod pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6 container configmap-volume-test: \u0026lt;nil\u0026gt; STEP: delete the pod Jul 3 06:13:08.133: INFO: Waiting for pod pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6 to disappear Jul 3 06:13:08.136: INFO: Pod pod-configmaps-25425d98-7e88-11e8-9aa2-c6770944a2e6 no longer exists [AfterEach] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:142 Jul 3 06:13:08.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready STEP: Destroying namespace \u0026quot;e2e-tests-configmap-nw9xh\u0026quot; for this suite. Jul 3 06:13:14.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered Jul 3 06:13:14.309: INFO: namespace: e2e-tests-configmap-nw9xh, resource: bindings, ignored listing per whitelist Jul 3 06:13:14.340: INFO: namespace e2e-tests-configmap-nw9xh deletion completed in 6.201346759s  [SLOW TEST:10.329 seconds] [sig-storage] ConfigMap /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32 should be consumable from pods in volume as non-root [NodeConformance] [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 ------------------------------ SSSSSSS ------------------------------ [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 [BeforeEach] [sig-apps] StatefulSet /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:141 STEP: Creating a kubernetes client Jul 3 06:13:14.340: INFO: \u0026gt;\u0026gt;\u0026gt; kubeConfig: /tmp/kubeconfig-559575868 STEP: Building a namespace api object STEP: Waiting for a default service account to be provisioned in namespace [BeforeEach] [sig-apps] StatefulSet /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:57 [BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:72 STEP: Creating service test in namespace e2e-tests-statefulset-kntgb [It] should perform rolling updates and roll backs of template modifications [Conformance] /workspace/anago-v1.11.0-rc.3.3+91e7b4fd31fcd3/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:684 STEP: Creating a new StatefulSet Jul 3 06:13:14.444: INFO: Found 0 stateful pods, waiting for 3 Jul 3 06:13:24.450: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true Jul 3 06:13:24.450: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true Jul 3 06:13:24.450: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true Jul 3 06:13:24.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559575868 exec --namespace=e2e-tests-statefulset-kntgb ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true' Jul 3 06:13:24.728: INFO: stderr: \u0026quot;\u0026quot; Jul 3 06:13:24.728: INFO: stdout: \u0026quot;'/usr/share/nginx/html/index.html' -\u0026gt; '/tmp/index.html'\\n\u0026quot; Jul 3 06:13:24.728: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -\u0026gt; '/tmp/index.html' STEP: Updating StatefulSet template: update image from k8s.gcr.io/nginx-slim-amd64:0.20 to k8s.gcr.io/nginx-slim-amd64:0.21 Jul 3 06:13:34.779: INFO: Updating stateful set ss2 STEP: Creating a new revision STEP: Updating Pods in reverse ordinal order Jul 3 06:13:44.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559575868 exec --namespace=e2e-tests-statefulset-kntgb ss2-1 -- /bin/sh -c mv -v /tmp/index.html /usr/share/nginx/html/ || true' Jul 3 06:13:45.165: INFO: stderr: \u0026quot;\u0026quot; Jul 3 06:13:45.165: INFO: stdout: \u0026quot;'/tmp/index.html' -\u0026gt; '/usr/share/nginx/html/index.html'\\n\u0026quot; Jul 3 06:13:45.165: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -\u0026gt; '/usr/share/nginx/html/index.html' Jul 3 06:13:55.183: INFO: Waiting for StatefulSet e2e-tests-statefulset-kntgb/ss2 to complete update Jul 3 06:13:55.183: INFO: Waiting for Pod e2e-tests-statefulset-kntgb/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff Jul 3 06:13:55.183: INFO: Waiting for Pod e2e-tests-statefulset-kntgb/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff Jul 3 06:14:05.194: INFO: Waiting for StatefulSet e2e-tests-statefulset-kntgb/ss2 to complete update Jul 3 06:14:05.194: INFO: Waiting for Pod e2e-tests-statefulset-kntgb/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff Jul 3 06:14:05.194: INFO: Waiting for Pod e2e-tests-statefulset-kntgb/ss2-1 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff Jul 3 06:14:15.197: INFO: Waiting for StatefulSet e2e-tests-statefulset-kntgb/ss2 to complete update Jul 3 06:14:15.197: INFO: Waiting for Pod e2e-tests-statefulset-kntgb/ss2-0 to have revision ss2-56dd5fb9c4 update revision ss2-76cb68b6ff STEP: Rolling back to a previous revision Jul 3 06:14:25.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-559575868 exec --namespace=e2e-tests-statefulset-kntgb ss2-1 -- /bin/sh -c mv -v /usr/share/nginx/html/index.html /tmp/ || true' Jul 3 06:14:25.478: INFO: stderr: \u0026quot;\u0026quot; Jul 3 06:14:25.478: INFO: stdout: \u0026quot;'/usr/share/nginx/html/index.html' -\u0026gt; '/tmp/index.html'\\n\u0026quot; Jul 3 06:14:25.478: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -\u0026gt; '/tmp/index.html' Jul 3 06:14:35.535: INFO: Updating stateful set ss2 "
},
{
	"uri": "/pi_cluster/os_installation/children/dongle/",
	"title": "Create a Rapsberry PI Rescue Dongle",
	"tags": ["rpi"],
	"description": "",
	"content": "I encountered multiple issues trying to repartition my SD on my PI. Because the / directory is mounted, it never really worked safely for me to use fdisk. Morevoer some of the powerfull tools such as gparted need X11 installed, which I don\u0026rsquo;t have by default.\nHopefully the new PI3 B and B+ are able to boot from USB, hence the idea of creating a Rescue Dongle\n\nConsideration regarding USB boot. It seems that all new PI 3B+ have OTP for USB boot mode setup by default. For the PC 3B, you have to activate using the /boot/config.txt\nmaster-pi is a 3B+, nas-pi and home-pi are 3B:\nLet\u0026rsquo;s check the /boot/config.txt\nansible picluster -i inventory/ -m shell -a \u0026quot;grep program_usb_boot_mode /boot/config.txt\u0026quot; master-pi.kubedge.cloud | FAILED | rc=1 \u0026gt;\u0026gt; nas-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; program_usb_boot_mode=1 home-pi.kubedge.cloud | FAILED | rc=1 \u0026gt;\u0026gt;  The flag for OTP USB flag is set on the 3B+ (by default on master-pi) and on the 3B where I did add the entry to the config.txt (nas-pi)\nansible picluster -i inventory/ -m shell -a \u0026quot;vcgencmd otp_dump | grep 17:\u0026quot; master-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; 17:3020000a nas-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; 17:3020000a home-pi.kubedge.cloud | SUCCESS | rc=0 \u0026gt;\u0026gt; 17:1020000a  Creation of the Rescue Dongle  Flash Raspbian on the Dongle using the normal procedure (Wind32DiskImager,..) Remove the SD card from the PI3. Plug keyboard, mouse, screen \u0026hellip; onto the PI Boot the PI3 on the Dongle by removing the SD card Take the time to setup the Raspbian:  Enable VNC Enable SSH Setup default hostname Setup password for Pi account Setup resolution (for VNC later) Install tools such as gparted  Shutdown the PI and put back the normal SD card.  Usage The idea is to boot a PI from the Dongle and then apply the fixes to the SD card.\n Shutdown the PI. Remove the SD card. Insert the Dongle into USB port of PI Reboot the PI. Connect to the PI using the VNC or SSH. The PI has started from the OS installed on the Dongle. Insert the SD card into the PI. You will most likely have a popup and the filesystem from the SD card is automatically mounted. Start to use the tools to fix your SD card. For instance:  Shutdown and reboot. The PI will restart from the SD card.  Application: Change SD card partition First step is to reboot a PI without SD card from the Dongle and connect VNC Viewer to it.\nFirst insert the SD and close the popups In a terminal run, sudo gparted\nsudo gparted unmount the mmc root partition  Resize the current partition (down to 16G), create an extended one and 7*2G logical partition in that 14G partition)  Apply the changes  Shutdown or Reboot. The PI should restart from the SD anyway.  Reference Links  TBD "
},
{
	"uri": "/nfv/advanced/",
	"title": "Advanced &amp; WIP",
	"tags": [],
	"description": "Advanced tutorials and Work In Progress",
	"content": " Advanced  ONAP  ONAP Tutorial\n "
},
{
	"uri": "/about/",
	"title": "About KUBEDGE",
	"tags": [],
	"description": "",
	"content": " About WIP\n"
},
{
	"uri": "/pi_cluster/advanced/children/2018-07-30-a/",
	"title": "Build and Deploy Kubernetes test-infra",
	"tags": ["kubernetes", "test-infra", "sonobuoy", "testing", "rpi"],
	"description": "",
	"content": "test-infra seems to somewhat overlap with sonobuoy features. The purpose of this post is to fetch the code, compile and deploy it on a Kubernetes cluster.\n\nKey Aspects  Compile and deploy the test-infra code in test-infra  Deploy  WIP  Conclusion  WIP  Reference Links  Official test-infra Code "
},
{
	"uri": "/pi_cluster/advanced/children/2018-07-31-a/",
	"title": "Build and Deploy Kubernetes Istio",
	"tags": ["kubernetes", "security", "istio", "rpi"],
	"description": "",
	"content": "Istio is aiming at improving security of the containers. One of the key aspects is the end to end encryption of the commnucation, the role of citadel to ensure the management of the certificates, the renewal of the certificates. As always, the goal of this post is to study that new tool and figure out I can leverage it in my day to day work.\n\nKey Aspects  Compile and deploy the istio code in istio  Deploy  WIP  Conclusion  WIP  Reference Links  Official istio Code "
},
{
	"uri": "/pi_cluster/advanced/children/2018-08-01-a/",
	"title": "Build and Deploy Kubernetes Hashicorp Vault",
	"tags": ["kubernetes", "security", "vault", "rpi"],
	"description": "",
	"content": "Vault is aiming at improving security of the containers by rotating token and credential much more often than usual. Looks like it is especially effectiv to help rotate passwords used to access internal databases.\n\nKey Aspects  Compile and deploy the vault code in vault  Deploy  WIP  Conclusion  WIP  Reference Links  Official HashiCorp Code "
},
{
	"uri": "/devops/advanced/children/2018-07-16-a/",
	"title": "Rebuild Hyperkube images",
	"tags": ["kubernetes"],
	"description": "",
	"content": "This post is to validate that it would be possible if urgency dictates it to rebuild the hyperkube Kubernetes image.\n\nKey Aspects  Rebuild Hyperkube images for amd64 and arm32v7 Rebuild the individual images deployed by kubeadm  Build Kubernetes executables for AMD64 and ARM  WIP  Conclusion  WIP  Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/devops/kubedgesdk/children/2018-07-15-a/",
	"title": "Recompile Kubernetes components for Raspberry PI",
	"tags": ["kubernetes", "rpi"],
	"description": "",
	"content": "During the installation of official Kubernetes 1.11.0 on RPI Cluster 1, encountered a bug on the controller manager preventing the controller-manager from starting. The problem here was to be able to cross compiled the latest version of Kubernetes 1.11.1 before the code was officially released and of course rebuild the images.\n\nKey Aspects  The bug had been fixed by the Kubernetes kube-controller-manager - panic: runtime error: index out of range has been fixed and will be built as part of 1.11.1 The goal is to learn how to recompile Kubernetes from the source to be able to contribute when possible and address problems as soon as possible  Build Kubernetes executables for AMD64 and ARM Cross Compiling from Ubuntu Machine First check the go setup. Fetch the code\nLet\u0026rsquo;s check the go environment. See Setup GOLANG environment\n$ which go /usr/bin/go  $ go version go version go1.10.1 linux/amd64  Clone the code\n$ export GOPATH=$HOME/src $ mkdir -p src/k8s.io $ cd src/k8s.io $ git clone -b release-1.11 git@github.com:kubernetes/kubernetes.git $ cd kubernertes  To save time, I removed platforms I had not use of and only kept amd64 and arm on linux\n$ cat hack/lib/golang.sh  # The server platform we are building on. readonly KUBE_SERVER_PLATFORMS=( linux/amd64 linux/arm ) # The node platforms we build for readonly KUBE_NODE_PLATFORMS=( linux/amd64 linux/arm ) # If we update this we should also update the set of platforms whose standard library is precompiled for in build/build-image/cross/Dockerfile readonly KUBE_CLIENT_PLATFORMS=( linux/amd64 linux/arm ) # Which platforms we should compile test targets for. Not all client platforms need these tests readonly KUBE_TEST_PLATFORMS=( linux/amd64 linux/arm )  Double check your Docker setup is correct\n$ docker run hello-world  Clean the directory\n$ ./build/make-clean.sh  Rebuild the amd64 and arm executable\n$ ./build/run.sh make cross  Check the executables transfered from the docker build container to your vm by rsync\n$ cd _output/dockerized/bin/linux  Check the amd64 executables\nls -lt amd64 total 2322696 -rwxr-xr-x 1 xxxxxx yyyyyy 209961168 Jul 16 01:57 e2e_node.test -rwxr-xr-x 1 xxxxxx yyyyyy 10645252 Jul 16 01:57 ginkgo -rwxr-xr-x 1 xxxxxx yyyyyy 160051600 Jul 16 01:57 kubemark -rwxr-xr-x 1 xxxxxx yyyyyy 231997872 Jul 16 01:55 genman -rwxr-xr-x 1 xxxxxx yyyyyy 54099081 Jul 16 01:55 gendocs -rwxr-xr-x 1 xxxxxx yyyyyy 54039865 Jul 16 01:55 genyaml -rwxr-xr-x 1 xxxxxx yyyyyy 6694536 Jul 16 01:55 linkcheck -rwxr-xr-x 1 xxxxxx yyyyyy 5481582 Jul 16 01:55 genswaggertypedocs -rwxr-xr-x 1 xxxxxx yyyyyy 226061456 Jul 16 01:55 genkubedocs -rwxr-xr-x 1 xxxxxx yyyyyy 173411368 Jul 16 01:55 e2e.test -rwxr-xr-x 1 xxxxxx yyyyyy 55241186 Jul 16 01:52 kubectl -rwxr-xr-x 1 xxxxxx yyyyyy 51912114 Jul 16 01:52 kube-proxy -rwxr-xr-x 1 xxxxxx yyyyyy 57246829 Jul 16 01:51 kubeadm -rwxr-xr-x 1 xxxxxx yyyyyy 162716256 Jul 16 01:51 kubelet -rwxr-xr-x 1 xxxxxx yyyyyy 57908740 Jul 16 01:50 kube-aggregator -rwxr-xr-x 1 xxxxxx yyyyyy 185152632 Jul 16 01:50 kube-apiserver -rwxr-xr-x 1 xxxxxx yyyyyy 2330265 Jul 16 01:50 mounter -rwxr-xr-x 1 xxxxxx yyyyyy 138048783 Jul 16 01:50 cloud-controller-manager -rwxr-xr-x 1 xxxxxx yyyyyy 153798994 Jul 16 01:50 kube-controller-manager -rwxr-xr-x 1 xxxxxx yyyyyy 227283184 Jul 16 01:50 hyperkube -rwxr-xr-x 1 xxxxxx yyyyyy 59296776 Jul 16 01:50 apiextensions-apiserver -rwxr-xr-x 1 xxxxxx yyyyyy 55471235 Jul 16 01:50 kube-scheduler -rwxr-xr-x 1 xxxxxx yyyyyy 2835466 Jul 16 01:40 go-bindata -rwxr-xr-x 1 xxxxxx yyyyyy 13630237 Jul 16 01:40 openapi-gen -rwxr-xr-x 1 xxxxxx yyyyyy 7699847 Jul 16 01:40 conversion-gen -rwxr-xr-x 1 xxxxxx yyyyyy 7669238 Jul 16 01:39 defaulter-gen -rwxr-xr-x 1 xxxxxx yyyyyy 7695690 Jul 16 01:39 deepcopy-gen  Check the RPI executables\n$ ls -lt arm total 2089216 -rwxr-xr-x 1 xxxxxx yyyyyy 191189688 Jul 16 01:58 e2e_node.test -rwxr-xr-x 1 xxxxxx yyyyyy 9286102 Jul 16 01:58 ginkgo -rwxr-xr-x 1 xxxxxx yyyyyy 142111524 Jul 16 01:58 kubemark -rwxr-xr-x 1 xxxxxx yyyyyy 210989712 Jul 16 01:55 genman -rwxr-xr-x 1 xxxxxx yyyyyy 48169969 Jul 16 01:55 gendocs -rwxr-xr-x 1 xxxxxx yyyyyy 48102745 Jul 16 01:55 genyaml -rwxr-xr-x 1 xxxxxx yyyyyy 5847821 Jul 16 01:55 linkcheck -rwxr-xr-x 1 xxxxxx yyyyyy 4927730 Jul 16 01:55 genswaggertypedocs -rwxr-xr-x 1 xxxxxx yyyyyy 205458748 Jul 16 01:55 genkubedocs -rwxr-xr-x 1 xxxxxx yyyyyy 155653412 Jul 16 01:55 e2e.test -rwxr-xr-x 1 xxxxxx yyyyyy 49252169 Jul 16 01:52 kubectl -rwxr-xr-x 1 xxxxxx yyyyyy 46156229 Jul 16 01:52 kube-proxy -rwxr-xr-x 1 xxxxxx yyyyyy 50989514 Jul 16 01:52 kubeadm -rwxr-xr-x 1 xxxxxx yyyyyy 144291264 Jul 16 01:52 kubelet -rwxr-xr-x 1 xxxxxx yyyyyy 52024300 Jul 16 01:50 kube-aggregator -rwxr-xr-x 1 xxxxxx yyyyyy 168751462 Jul 16 01:50 kube-apiserver -rwxr-xr-x 1 xxxxxx yyyyyy 2231154 Jul 16 01:50 mounter -rwxr-xr-x 1 xxxxxx yyyyyy 122701327 Jul 16 01:50 cloud-controller-manager -rwxr-xr-x 1 xxxxxx yyyyyy 136828051 Jul 16 01:50 kube-controller-manager -rwxr-xr-x 1 xxxxxx yyyyyy 206769888 Jul 16 01:50 hyperkube -rwxr-xr-x 1 xxxxxx yyyyyy 53220784 Jul 16 01:50 apiextensions-apiserver -rwxr-xr-x 1 xxxxxx yyyyyy 49501810 Jul 16 01:50 kube-scheduler -rwxr-xr-x 1 xxxxxx yyyyyy 2649635 Jul 16 01:40 go-bindata -rwxr-xr-x 1 xxxxxx yyyyyy 11885551 Jul 16 01:40 openapi-gen -rwxr-xr-x 1 xxxxxx yyyyyy 6770458 Jul 16 01:40 conversion-gen -rwxr-xr-x 1 xxxxxx yyyyyy 6764729 Jul 16 01:39 defaulter-gen -rwxr-xr-x 1 xxxxxx yyyyyy 6770309 Jul 16 01:39 deepcopy-gen  Transfer the binaries to RPI machine\nscp -r arm rpiuser@192.168.1.94:/home/rpiuser/kubernetes_binaries  Build the docker image $ mkdir -p images/kube-controller-manager $ cd images/kube-controller-manager $ cp $HOME/kubernetes_binaries/kube-controller-manager  Create a simple Dockerfile to update the executable\n$ cat Dockerfile  FROM k8s.gcr.io/kube-controller-manager-arm:v1.11.0 COPY kube-controller-manager /usr/local/bin/kube-controller-manager  Build the image\n$ docker build -t jbrette/kube-controller-manager-arm:v1.11.1  Because of the brute force approach the image is twice as big since the executable (130MB) is contained in two layers of the container image\n$ docker image list REPOSITORY TAG IMAGE ID CREATED SIZE jbrette/kube-controller-manager-arm v1.11.1 b7023eef8fdf 8 hours ago 275MB k8s.gcr.io/kube-apiserver-arm v1.11.0 383bd2c4314e 2 weeks ago 170MB k8s.gcr.io/kube-controller-manager-arm v1.11.0 5b25f8a97aec 2 weeks ago 138MB k8s.gcr.io/kube-scheduler-arm v1.11.0 555ee860fa3c 2 weeks ago 50.5MB k8s.gcr.io/kube-proxy-arm v1.11.0 d7ebe361fe95 2 weeks ago 89.1MB k8s.gcr.io/coredns 1.1.3 7ceeb40862fb 7 weeks ago 31.3MB k8s.gcr.io/etcd-arm 3.2.18 ae02bf7047c8 3 months ago 221MB quay.io/coreos/flannel v0.10.0-arm c663d02f7966 5 months ago 39.9MB k8s.gcr.io/pause-arm 3.1 e11a8cbeda86 6 months ago 374kB k8s.gcr.io/pause 3.1 e11a8cbeda86 6 months ago 374kB  First push the controller-manager docker image to the repo\n$ docker login $ docker push jbrette/kube-controller-manager-arm:v1.11.1  Updating Kubeadm and Kubernetes configuration to use the patched images Because I only rebuild one image, I can not use the kubeadm feature which allows to point to a different repo. Have to edit manually the configuration.\n$ more kube-controller-manager.yaml  apiVersion: v1 kind: Pod metadata: annotations: scheduler.alpha.kubernetes.io/critical-pod: \u0026quot;\u0026quot; creationTimestamp: null labels: component: kube-controller-manager tier: control-plane name: kube-controller-manager namespace: kube-system spec: containers: - command: - kube-controller-manager - --address=127.0.0.1 - --allocate-node-cidrs=true - --cluster-cidr=10.244.0.0/16 - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key - --controllers=*,bootstrapsigner,tokencleaner - --kubeconfig=/etc/kubernetes/controller-manager.conf - --leader-elect=true - --node-cidr-mask-size=24 - --root-ca-file=/etc/kubernetes/pki/ca.crt - --service-account-private-key-file=/etc/kubernetes/pki/sa.key - --use-service-account-credentials=true image: jbrette/kube-controller-manager-arm:v1.11.1 imagePullPolicy: IfNotPresent livenessProbe: failureThreshold: 8 httpGet: host: 127.0.0.1 path: /healthz port: 10252 scheme: HTTP initialDelaySeconds: 15 timeoutSeconds: 15 name: kube-controller-manager resources: requests: cpu: 200m volumeMounts: - mountPath: /etc/ssl/certs name: ca-certs readOnly: true - mountPath: /etc/kubernetes/controller-manager.conf name: kubeconfig readOnly: true - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec name: flexvolume-dir - mountPath: /usr/share/ca-certificates name: usr-share-ca-certificates readOnly: true - mountPath: /usr/local/share/ca-certificates name: usr-local-share-ca-certificates readOnly: true - mountPath: /etc/ca-certificates name: etc-ca-certificates readOnly: true - mountPath: /etc/kubernetes/pki name: k8s-certs readOnly: true hostNetwork: true priorityClassName: system-cluster-critical volumes: - hostPath: path: /etc/kubernetes/pki type: DirectoryOrCreate name: k8s-certs - hostPath: path: /etc/ssl/certs type: DirectoryOrCreate name: ca-certs - hostPath: path: /etc/kubernetes/controller-manager.conf type: FileOrCreate name: kubeconfig - hostPath: path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec type: DirectoryOrCreate name: flexvolume-dir - hostPath: path: /usr/share/ca-certificates type: DirectoryOrCreate name: usr-share-ca-certificates - hostPath: path: /usr/local/share/ca-certificates type: DirectoryOrCreate name: usr-local-share-ca-certificates - hostPath: path: /etc/ca-certificates type: DirectoryOrCreate name: etc-ca-certificates status: {}  Conclusion  Need to streamline the build process Need to streamline the image patching processed  Reference Links  [Kubernetes cross build]() "
},
{
	"uri": "/devops/microservices/children/2018-07-01-a/",
	"title": "Python",
	"tags": ["python"],
	"description": "",
	"content": "As we did for Go and Java, where is was possible to create an Dockerfile starting from \u0026ldquo;scratch\u0026rdquo;, the goal of this post is to create a python base server container with a minimum about of packages (debian and python) to reduce the security exposure of the container as well as the image size.\n\nKey Aspects  Use SCRATCH has base image to keep size minimum Simple HelloWorld Python web server Create associated HELM chart for Kubernetes deployment Provide deployment for both amd64 and arm32v7  Simple Python Server The pythonhttpserver repo showcases: - How to create a simple Python3 server - How to leverage Travis to compile for amd64 and arm32v7. - Branch amd64 is for normal PC and HP server. - Branch arm32v7 produces software usable on Raspberry PI 3B+\nUsefull Links TBD\n"
},
{
	"uri": "/devops/microservices/children/2018-06-29-a/",
	"title": "Creating simple GO server container",
	"tags": ["golang"],
	"description": "",
	"content": "GO is perfectly adapted to microservices since it can build standalone executable.\n\nKey Aspects  Use SCRATCH has base image to keep size minimum Simple HelloWorld GO web server Create associated HELM chart for Kubernetes deployment Provide deployment for both amd64 and arm32v7  Simple GO Server compilation The gohttpserver repo showcases: - How to compile a GO process - How to leverage Travis to compile for amd64 and arm32v7. - Branch amd64 is for normal PC and HP server. - Branch arm32v7 produces software usable on Raspberry PI 3B+\nKubeplay Github repo The kubeplay repo describes: - How to compile a GO server and - How to create a Helm chart to easily deploy on - How to use Travis-CI to compile and publish the image on Docker.io with the proper tags. - The amd64 branch is kind of complete - The arm32v7 produces software deployable on Raspberry PI 3B+\nUsefull Links TBD\n"
},
{
	"uri": "/devops/microservices/children/2018-06-28-a/",
	"title": "Creating simple Java 10 server container",
	"tags": ["java"],
	"description": "",
	"content": "Very often people associated Java to quite bulky and difficult to use in the microservice context, unless you have very large image containing the JRE. But since Java 9, Java did kind of catchup with golang on the subject. Where you can obtain a standalone executable when we running go build, java is now proposing jlink which always you to acheive a very similar result. The goal of this post is to build a container image as small as possible running Java.\n\nKey Aspects  Use SCRATCH has base image to keep size minimum Simple HelloWorld Java 10 web server Create associated HELM chart for Kubernetes deployment Provide deployment for both amd64 and arm32v7  Install Java 10 on dev machine sudo add-apt-repository ppa:linuxuprising/java Oracle Java 10 installer Java binaries are not hosted in this PPA due to licensing. The packages in this PPA download and install Oracle Java 10 (JDK 10), so a working Internet connection is required. PPA for https://www.linuxuprising.com/ For feedback, see https://www.linuxuprising.com/2018/04/install-oracle-java-10-in-ubuntu-or.html More info: https://launchpad.net/~linuxuprising/+archive/ubuntu/java Press [ENTER] to continue or ctrl-c to cancel adding it gpg: keyring `/tmp/tmpf0lejdsg/secring.gpg' created gpg: keyring `/tmp/tmpf0lejdsg/pubring.gpg' created gpg: requesting key 73C3DB2A from hkp server keyserver.ubuntu.com gpg: /tmp/tmpf0lejdsg/trustdb.gpg: trustdb created gpg: key 73C3DB2A: public key \u0026quot;Launchpad PPA for Linux Uprising\u0026quot; imported gpg: Total number processed: 1 gpg: imported: 1 (RSA: 1) OK  sudo apt-get update Hit:1 http://ppa.launchpad.net/gophers/archive/ubuntu xenial InRelease Hit:2 https://download.docker.com/linux/ubuntu xenial InRelease Hit:3 https://deb.nodesource.com/node_10.x xenial InRelease Get:4 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial InRelease [18.0 kB] Get:6 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB] Hit:7 http://us.archive.ubuntu.com/ubuntu xenial InRelease Get:8 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB] Hit:9 http://ppa.launchpad.net/longsleep/golang-backports/ubuntu xenial InRelease Get:10 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB] Get:11 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [796 kB] Hit:12 http://ppa.launchpad.net/masterminds/glide/ubuntu xenial InRelease Get:13 http://us.archive.ubuntu.com/ubuntu xenial-updates/main i386 Packages [728 kB] Get:14 http://security.ubuntu.com/ubuntu xenial-security/main amd64 DEP-11 Metadata [67.7 kB] Get:15 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial/main amd64 Packages [1,956 B] Get:16 http://us.archive.ubuntu.com/ubuntu xenial-updates/main amd64 DEP-11 Metadata [318 kB] Get:17 http://security.ubuntu.com/ubuntu xenial-security/main DEP-11 64x64 Icons [72.6 kB] Hit:5 https://packages.cloud.google.com/apt kubernetes-xenial InRelease Get:18 http://us.archive.ubuntu.com/ubuntu xenial-updates/main DEP-11 64x64 Icons [232 kB] Get:19 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial/main Translation-en [1,004 B] Get:20 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 DEP-11 Metadata [107 kB] Get:21 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [640 kB] Get:22 http://security.ubuntu.com/ubuntu xenial-security/universe DEP-11 64x64 Icons [142 kB] Get:23 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe i386 Packages [585 kB] Get:24 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe amd64 DEP-11 Metadata [246 kB] Get:25 http://us.archive.ubuntu.com/ubuntu xenial-updates/universe DEP-11 64x64 Icons [338 kB] Get:26 http://us.archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 DEP-11 Metadata [5,964 B] Get:27 http://us.archive.ubuntu.com/ubuntu xenial-backports/main amd64 DEP-11 Metadata [3,324 B] Get:28 http://us.archive.ubuntu.com/ubuntu xenial-backports/universe amd64 DEP-11 Metadata [5,100 B] Fetched 4,630 kB in 3s (1,333 kB/s) Reading package lists... Done  Download the package and install it. Note the download speed was at time really slow. Could not figure where the source of the issue was.\nsudo apt-get install oracle-java10-installer Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: gsfonts-x11 java-common oracle-java10-set-default Suggested packages: binfmt-support visualvm ttf-baekmuk | ttf-unfonts | ttf-unfonts-core ttf-kochi-gothic | ttf-sazanami-gothic ttf-kochi-mincho | ttf-sazanami-mincho ttf-arphic-uming The following NEW packages will be installed: gsfonts-x11 java-common oracle-java10-installer oracle-java10-set-default 0 upgraded, 4 newly installed, 0 to remove and 0 not upgraded. Need to get 46.6 kB of archives. After this operation, 265 kB of additional disk space will be used. Do you want to continue? [Y/n] y Get:1 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 java-common all 0.56ubuntu2 [7,742 B] Get:2 http://us.archive.ubuntu.com/ubuntu xenial/universe amd64 gsfonts-x11 all 0.24 [7,314 B] Get:3 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial/main amd64 oracle-java10-installer amd64 10.0.1-1~linuxuprising+1 [29.0 kB] Get:4 http://ppa.launchpad.net/linuxuprising/java/ubuntu xenial/main amd64 oracle-java10-set-default amd64 10.0.1-1~linuxuprising+1 [2,464 B] Fetched 46.6 kB in 0s (55.0 kB/s) Preconfiguring packages ... Selecting previously unselected package java-common. (Reading database ... 239646 files and directories currently installed.) Preparing to unpack .../java-common_0.56ubuntu2_all.deb ... Unpacking java-common (0.56ubuntu2) ... Selecting previously unselected package oracle-java10-installer. Preparing to unpack .../oracle-java10-installer_10.0.1-1~linuxuprising+1_amd64.deb ... Unpacking oracle-java10-installer (10.0.1-1~linuxuprising+1) ... Processing triggers for man-db (2.7.5-1) ... Processing triggers for desktop-file-utils (0.22-1ubuntu5.2) ... Processing triggers for bamfdaemon (0.5.3~bzr0+16.04.20180209-0ubuntu1) ... Rebuilding /usr/share/applications/bamf-2.index... Processing triggers for gnome-menus (3.13.3-6ubuntu3.1) ... Processing triggers for mime-support (3.59ubuntu1) ... Processing triggers for hicolor-icon-theme (0.15-0ubuntu1) ... Processing triggers for shared-mime-info (1.5-2ubuntu0.1) ... Setting up java-common (0.56ubuntu2) ... Setting up oracle-java10-installer (10.0.1-1~linuxuprising+1) ... No /var/cache/oracle-jdk10-installer/wgetrc file found. Creating /var/cache/oracle-jdk10-installer/wgetrc and using default oracle-java10-installer wgetrc settings for it. Downloading Oracle Java 10... --2018-07-01 11:04:19-- http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz Resolving download.oracle.com (download.oracle.com)... 23.45.132.164 Connecting to download.oracle.com (download.oracle.com)|23.45.132.164|:80... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: https://edelivery.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz [following] --2018-07-01 11:04:19-- https://edelivery.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz Resolving edelivery.oracle.com (edelivery.oracle.com)... 23.72.169.50 Connecting to edelivery.oracle.com (edelivery.oracle.com)|23.72.169.50|:443... connected. HTTP request sent, awaiting response... 302 Moved Temporarily Location: http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz?AuthParam=1530461180_a5ea3c7d2dd9864c93b5a0d9d202ba07 [following] --2018-07-01 11:04:20-- http://download.oracle.com/otn-pub/java/jdk/10.0.1+10/fb4372174a714e6b8c52526dc134031e/jdk-10.0.1_linux-x64_bin.tar.gz?AuthParam=1530461180_a5ea3c7d2dd9864c93b5a0d9d202ba07 Connecting to download.oracle.com (download.oracle.com)|23.45.132.164|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 354846158 (338M) [application/x-gzip] Saving to: jdk-10.0.1_linux-x64_bin.tar.gz 0K ........ ........ ........ ........ ........ ........ 0% 169K 33m55s 3072K ........ ........ ........ ........ ........ ........ 1% 131K 38m29s 6144K ........ ........ ........ ........ ........ ........ 2% 28.2K 91m46s 9216K ........ ........ ........ ........ ........ ........ 3% 263K 73m30s 12288K ........ ........ ........ ........ ........ ........ 4% 142K 66m2s 15360K ........ ........ ........ ........ ........ ........ 5% 2.31M 54m54s 18432K ........ ........ ........ ........ ........ ........ 6% 2.06M 46m59s 21504K ........ ........ ........ ........ ........ ........ 7% 1.26M 41m15s 24576K ........ ........ ........ ........ ........ ........ 7% 1.71M 36m39s 27648K ........ ........ ........ ........ ........ ........ 8% 1.35M 33m3s 30720K ........ ........ ........ ........ ........ ........ 9% 1.73M 30m1s 33792K ........ ........ ........ ........ ........ ........ 10% 2.27M 27m26s 36864K ........ ........ ........ ........ ........ ........ 11% 1.61M 25m19s 39936K ........ ........ ........ ........ ........ ........ 12% 2.42M 23m25s 43008K ........ ........ ........ ........ ........ ........ 13% 1.50M 21m51s 46080K ........ ........ ........ ........ ........ ........ 14% 2.26M 20m24s 49152K ........ ........ ........ ........ ........ ........ 15% 2.33M 19m8s 52224K ........ ........ ........ ........ ........ ........ 15% 2.21M 18m0s 55296K ........ ........ ........ ........ ........ ........ 16% 1.58M 17m2s 58368K ........ ........ ........ ........ ........ ........ 17% 1.84M 16m8s 61440K ........ ........ ........ ........ ........ ........ 18% 1.61M 15m20s 64512K ........ ........ ........ ........ ........ ........ 19% 3.04M 14m33s 67584K ........ ........ ........ ........ ........ ........ 20% 2.91M 13m49s 70656K ........ ........ ........ ........ ........ ........ 21% 3.17M 13m9s 73728K ........ ........ ........ ........ ........ ........ 22% 1.36M 12m37s 76800K ........ ........ ........ ........ ........ ........ 23% 1.75M 12m5s 79872K ........ ........ ........ ........ ........ ........ 23% 2.08M 11m35s 82944K ........ ........ ........ ........ ........ ........ 24% 2.50M 11m6s 86016K ........ ........ ........ ........ ........ ........ 25% 1.78M 10m40s 89088K ........ ........ ........ ........ ........ ........ 26% 2.14M 10m16s 92160K ........ ........ ........ ........ ........ ........ 27% 1.56M 9m54s 95232K ........ ........ ........ ........ ........ ........ 28% 2.20M 9m31s 98304K ........ ........ ........ ........ ........ ........ 29% 1.98M 9m11s 101376K ........ ........ ........ ........ ........ ........ 30% 2.28M 8m51s 104448K ........ ........ ........ ........ ........ ........ 31% 2.91M 8m32s 107520K ........ ........ ........ ........ ........ ........ 31% 2.12M 8m14s 110592K ........ ........ ........ ........ ........ ........ 32% 1.76M 7m58s 113664K ........ ........ ........ ........ ........ ........ 33% 2.02M 7m42s 116736K ........ ........ ........ ........ ........ ........ 34% 1.21M 7m29s 119808K ........ ........ ........ ........ ........ ........ 35% 1.12M 7m17s 122880K ........ ........ ........ ........ ........ ........ 36% 1.18M 7m5s 125952K ........ ........ ........ ........ ........ ........ 37% 70.0K 8m3s 129024K ........ ........ ........ ........ ........ ........ 38% 1.40M 7m48s 132096K ........ ........ ........ ........ ........ ........ 39% 84.1K 8m28s 135168K ........ ........ ........ ........ ........ ........ 39% 804K 8m15s 138240K ........ ........ ........ ........ ........ ........ 40% 2.67M 7m59s 141312K ........ ........ ........ ........ ........ ........ 41% 2.88M 7m43s 144384K ........ ........ ........ ........ ........ ........ 42% 2.77M 7m28s 147456K ........ ........ ........ ........ ........ ........ 43% 1.88M 7m14s 150528K ........ ........ ........ ........ ........ ........ 44% 2.53M 7m1s 153600K ........ ........ ........ ........ ........ ........ 45% 2.48M 6m47s 156672K ........ ........ ........ ........ ........ ........ 46% 4.11M 6m34s 159744K ........ ........ ........ ........ ........ ........ 46% 3.75M 6m21s 162816K ........ ........ ........ ........ ........ ........ 47% 4.39M 6m8s 165888K ........ ........ ........ ........ ........ ........ 48% 3.77M 5m56s 168960K ........ ........ ........ ........ ........ ........ 49% 4.56M 5m45s 172032K ........ ........ ........ ........ ........ ........ 50% 3.32M 5m33s 175104K ........ ........ ........ ........ ........ ........ 51% 2.91M 5m23s 178176K ........ ........ ........ ........ ........ ........ 52% 2.38M 5m13s 181248K ........ ........ ........ ........ ........ ........ 53% 3.56M 5m3s 184320K ........ ........ ........ ........ ........ ........ 54% 3.06M 4m53s 187392K ........ ........ ........ ........ ........ ........ 54% 2.72M 4m43s 190464K ........ ........ ........ ........ ........ ........ 55% 2.38M 4m34s 193536K ........ ........ ........ ........ ........ ........ 56% 3.57M 4m25s 196608K ........ ........ ........ ........ ........ ........ 57% 3.46M 4m17s 199680K ........ ........ ........ ........ ........ ........ 58% 3.41M 4m8s 202752K ........ ........ ........ ........ ........ ........ 59% 2.67M 4m0s 205824K ........ ........ ........ ........ ........ ........ 60% 2.92M 3m52s 208896K ........ ........ ........ ........ ........ ........ 61% 2.62M 3m44s 211968K ........ ........ ........ ........ ........ ........ 62% 2.77M 3m37s 215040K ........ ........ ........ ........ ........ ........ 62% 2.68M 3m29s 218112K ........ ........ ........ ........ ........ ........ 63% 3.41M 3m22s 221184K ........ ........ ........ ........ ........ ........ 64% 2.99M 3m15s 224256K ........ ........ ........ ........ ........ ........ 65% 3.98M 3m8s 227328K ........ ........ ........ ........ ........ ........ 66% 3.04M 3m1s 230400K ........ ........ ........ ........ ........ ........ 67% 3.69M 2m54s 233472K ........ ........ ........ ........ ........ ........ 68% 2.00M 2m48s 236544K ........ ........ ........ ........ ........ ........ 69% 2.92M 2m42s 239616K ........ ........ ........ ........ ........ ........ 70% 2.35M 2m36s 242688K ........ ........ ........ ........ ........ ........ 70% 2.55M 2m30s 245760K ........ ........ ........ ........ ........ ........ 71% 2.74M 2m24s 248832K ........ ........ ........ ........ ........ ........ 72% 3.33M 2m18s 251904K ........ ........ ........ ........ ........ ........ 73% 3.94M 2m12s 254976K ........ ........ ........ ........ ........ ........ 74% 2.82M 2m6s 258048K ........ ........ ........ ........ ........ ........ 75% 2.62M 2m1s 261120K ........ ........ ........ ........ ........ ........ 76% 2.76M 1m56s 264192K ........ ........ ........ ........ ........ ........ 77% 3.01M 1m50s 267264K ........ ........ ........ ........ ........ ........ 78% 2.75M 1m45s 270336K ........ ........ ........ ........ ........ ........ 78% 3.23M 1m40s 273408K ........ ........ ........ ........ ........ ........ 79% 3.05M 95s 276480K ........ ........ ........ ........ ........ ........ 80% 3.66M 90s 279552K ........ ........ ........ ........ ........ ........ 81% 2.99M 85s 282624K ........ ........ ........ ........ ........ ........ 82% 2.43M 80s 285696K ........ ........ ........ ........ ........ ........ 83% 2.48M 76s 288768K ........ ........ ........ ........ ........ ........ 84% 2.71M 71s 291840K ........ ........ ........ ........ ........ ........ 85% 2.44M 67s 294912K ........ ........ ........ ........ ........ ........ 85% 2.95M 62s 297984K ........ ........ ........ ........ ........ ........ 86% 2.66M 58s 301056K ........ ........ ........ ........ ........ ........ 87% 2.43M 54s 304128K ........ ........ ........ ........ ........ ........ 88% 2.92M 49s 307200K ........ ........ ........ ........ ........ ........ 89% 2.59M 45s 310272K ........ ........ ........ ........ ........ ........ 90% 3.23M 41s 313344K ........ ........ ........ ........ ........ ........ 91% 2.75M 37s 316416K ........ ........ ........ ........ ........ ........ 92% 3.07M 33s 319488K ........ ........ ........ ........ ........ ........ 93% 2.51M 29s 322560K ........ ........ ........ ........ ........ ........ 93% 2.80M 25s 325632K ........ ........ ........ ........ ........ ........ 94% 2.11M 21s 328704K ........ ........ ........ ........ ........ ........ 95% 2.87M 18s 331776K ........ ........ ........ ........ ........ ........ 96% 2.51M 14s 334848K ........ ........ ........ ........ ........ ........ 97% 2.77M 10s 337920K ........ ........ ........ ........ ........ ........ 98% 3.05M 6s 340992K ........ ........ ........ ........ ........ ........ 99% 3.22M 3s 344064K ........ ........ ........ ........ ...... 100% 2.89M=6m40s 2018-07-01 11:11:00 (867 KB/s) - jdk-10.0.1_linux-x64_bin.tar.gz saved [354846158/354846158] Download done. Removing outdated cached downloads... update-alternatives: error: no alternatives for java update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/appletviewer to provide /usr/bin/appletviewer (appletviewer) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jaotc to provide /usr/bin/jaotc (jaotc) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jar to provide /usr/bin/jar (jar) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jarsigner to provide /usr/bin/jarsigner (jarsigner) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/java to provide /usr/bin/java (java) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javac to provide /usr/bin/javac (javac) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javadoc to provide /usr/bin/javadoc (javadoc) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javap to provide /usr/bin/javap (javap) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javapackager to provide /usr/bin/javapackager (javapackager) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javaws to provide /usr/bin/javaws (javaws) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jcmd to provide /usr/bin/jcmd (jcmd) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jconsole to provide /usr/bin/jconsole (jconsole) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jcontrol to provide /usr/bin/jcontrol (jcontrol) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jdb to provide /usr/bin/jdb (jdb) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jdeprscan to provide /usr/bin/jdeprscan (jdeprscan) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jdeps to provide /usr/bin/jdeps (jdeps) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jhsdb to provide /usr/bin/jhsdb (jhsdb) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jimage to provide /usr/bin/jimage (jimage) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jinfo to provide /usr/bin/jinfo (jinfo) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jjs to provide /usr/bin/jjs (jjs) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jlink to provide /usr/bin/jlink (jlink) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jmap to provide /usr/bin/jmap (jmap) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jmc to provide /usr/bin/jmc (jmc) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jmod to provide /usr/bin/jmod (jmod) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jps to provide /usr/bin/jps (jps) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jrunscript to provide /usr/bin/jrunscript (jrunscript) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jshell to provide /usr/bin/jshell (jshell) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jstack to provide /usr/bin/jstack (jstack) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jstat to provide /usr/bin/jstat (jstat) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jstatd to provide /usr/bin/jstatd (jstatd) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/jweblauncher to provide /usr/bin/jweblauncher (jweblauncher) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/keytool to provide /usr/bin/keytool (keytool) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/pack200 to provide /usr/bin/pack200 (pack200) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/rmic to provide /usr/bin/rmic (rmic) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/rmid to provide /usr/bin/rmid (rmid) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/rmiregistry to provide /usr/bin/rmiregistry (rmiregistry) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/serialver to provide /usr/bin/serialver (serialver) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/unpack200 to provide /usr/bin/unpack200 (unpack200) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/lib/jexec to provide /usr/bin/jexec (jexec) in auto mode update-alternatives: using /usr/lib/jvm/java-10-oracle/bin/javaws.real to provide /usr/bin/javaws.real (javaws.real) in auto mode Oracle JDK 10 installed #####Important######## To set Oracle jdk10 as default, install the \u0026quot;oracle-java10-set-default\u0026quot; package. E.g.: sudo apt install oracle-java10-set-default. Selecting previously unselected package oracle-java10-set-default. (Reading database ... 239682 files and directories currently installed.) Preparing to unpack .../oracle-java10-set-default_10.0.1-1~linuxuprising+1_amd64.deb ... Unpacking oracle-java10-set-default (10.0.1-1~linuxuprising+1) ... Selecting previously unselected package gsfonts-x11. Preparing to unpack .../gsfonts-x11_0.24_all.deb ... Unpacking gsfonts-x11 (0.24) ... Processing triggers for fontconfig (2.11.94-0ubuntu1.1) ... Setting up oracle-java10-set-default (10.0.1-1~linuxuprising+1) ... Setting up gsfonts-x11 (0.24) ...  sudo apt-get install oracle-java10-set-default Reading package lists... Done Building dependency tree Reading state information... Done oracle-java10-set-default is already the newest version (10.0.1-1~linuxuprising+1). oracle-java10-set-default set to manually installed. 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.  java --version java 10.0.1 2018-04-17 Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10) Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode)  javac --version javac 10.0.1  Create a micro service on the host Details on how to use the new java 10 to create the service are given in the following repo\nUsefull Links  Link1 Link2 "
},
{
	"uri": "/devops/advanced/children/2018-06-26-a/",
	"title": "Zuul",
	"tags": ["zuul"],
	"description": "",
	"content": "OpenStack project are using Zuul for CI/CD process. Zuul itself is based on Ansible to perform the tasks. This post is the collection of notes and tips used during the couple of update I did to some openstack projects.\n\nUnderstanding Zuul  WIP  Usefull Links  WIP "
},
{
	"uri": "/devops/advanced/children/2018-06-25-a/",
	"title": "Setup github/gerrit behind a corporate http proxy",
	"tags": ["github"],
	"description": "",
	"content": "This is a post of the small set of notes taken to setup gerrit review behind corporate proxy.\n\nAccess GitHub/Gerrit from behind a corporate http proxy TBD\nLinks TBD\n"
},
{
	"uri": "/devops/kubedgesdk/children/2018-06-24-a/",
	"title": "docker.io versus docker-ce",
	"tags": ["docker"],
	"description": "",
	"content": "Wondering why you have a strange error such as \u0026lsquo;from \u0026hellip;\u0026rsquo; when running docker build. The reason is linked to an older version of docker installed.\n\nRemove the current docker.io You need to remove the current docker.io.\nsudo apt-cache policy docker.io curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot; sudo apt-get update sudo apt-cache policy docker-ce sudo apt-get remove docker docker-engine docker.io sudo apt-get install docker-ce  Forced to run \u0026ldquo;sudo docker\u0026rdquo; instead of docker Run the following command:\nsudo usermod -a -G docker \u0026lt;yourusername\u0026gt;  If you still have issue running docker instead of sudo docker, try to reboot. Worked for me.\nRelated documentation  Link1 Link2 "
},
{
	"uri": "/devops/advanced/children/2018-06-23-a/",
	"title": "Setup multiple GitHub account on a single machine",
	"tags": ["github"],
	"description": "",
	"content": "In orderer to manage your personal GitHub projects or an your compagny projects, it is usefull to be able to conigure your .ssh directory.\n\nSetup multiple GitHub account TBD\nUsefull Links  Link1 "
},
{
	"uri": "/devops/microservices/children/2018-06-22-a/",
	"title": "Setup your GOLANG environment",
	"tags": ["golang"],
	"description": "",
	"content": "A lot of the opensource projects evolvoving around Kubernetes are written in go. It is very usefull to be able to rebuild so projects using go get or go build.\n\nInstalling the right version of GO If you have strange errors, when running go get \u0026hellip;., chances are that your version of GO is old. On Ubuntu, it is actually quite simple to address the issue.\nIf you are still running Ubuntu 16.04 LTS\nsudo add-apt-repository ppa:longsleep/golang-backports sudo apt-get update sudo apt-get install golang-go  How to setup your GOPATH A lot of things seems to work much better if you edit your .bashrc to set GOPATH=$HOME. The advantage of such a setup is that future \u0026ldquo;go get xxx\u0026rdquo; calls, will compile the go file into $HOME/bin which means that new executable will be available without changing your PATH.\nReferences  Link1 "
},
{
	"uri": "/devops/kubedgesdk/children/2018-06-21-a/",
	"title": "Setup SingleNode Kubernetes Cluster using kubeadm",
	"tags": ["kubernetes", "kubeadm"],
	"description": "",
	"content": "Setup simple kubernetes cluster for test purposes.\n\nAdd Kubernetes APT Repo If you are still running Ubuntu 16.04 LTS\nsudo xxx sudo apt-get update sudo apt-get install kubeadm kubelet kubectl  sudo docker version Client: Version: 1.13.1 API version: 1.26 Go version: go1.6.2 Git commit: 092cba3 Built: Thu Nov 2 20:40:23 2017 OS/Arch: linux/amd64 Server: Version: 1.13.1 API version: 1.26 (minimum version 1.12) Go version: go1.6.2 Git commit: 092cba3 Built: Thu Nov 2 20:40:23 2017 OS/Arch: linux/amd64 Experimental: false  sudo kubeadm version kubeadm version: \u0026amp;version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;11\u0026quot;, GitVersion:\u0026quot;v1.11.0\u0026quot;, GitCommit:\u0026quot;91e7b4fd31fcd3d5f436da26c980becec37ceefe\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-06-27T20:14:41Z\u0026quot;, GoVersion:\u0026quot;go1.10.2\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  Run Kubeadm First command needs to use sudo\nsudo kubeadm config images pull sudo kubeadm init  Let copy the .kube config into current user home directoru\nmkdir -p $HOME/.kube sudo cp /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config kubectl get all --all-namespaces  From that point don\u0026rsquo;t really need sudo anymore\nAdd Calico At that point most of the node will stay in mode \u0026ldquo;NotReady\u0026rdquo;\nkubectl get nodes NAME STATUS ROLES AGE VERSION allinone NotReady master 1m v1.11.0  To install Calico\nkubectl apply -f https://docs.projectcalico.org/v3.1/getting-started/kubernetes/installation/hosted/kubeadm/1.7/calico.yaml configmap/calico-config created daemonset.extensions/calico-etcd created service/calico-etcd created daemonset.extensions/calico-node created deployment.extensions/calico-kube-controllers created clusterrolebinding.rbac.authorization.k8s.io/calico-cni-plugin created clusterrole.rbac.authorization.k8s.io/calico-cni-plugin created serviceaccount/calico-cni-plugin created clusterrolebinding.rbac.authorization.k8s.io/calico-kube-controllers created clusterrole.rbac.authorization.k8s.io/calico-kube-controllers created serviceaccount/calico-kube-controllers created  Check again\nkubectl get nodes NAME STATUS ROLES AGE VERSION allinone Ready master 2m v1.11.0  Allow payload on master node kubectl get all --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system pod/calico-etcd-d6gn7 1/1 Running 0 1m kube-system pod/calico-kube-controllers-84fd4db7cd-ctctz 1/1 Running 0 1m kube-system pod/calico-node-bg4w8 2/2 Running 0 1m kube-system pod/coredns-78fcdf6894-7d5mf 1/1 Running 0 2m kube-system pod/coredns-78fcdf6894-xv9lm 1/1 Running 0 2m kube-system pod/etcd-allinone 1/1 Running 0 1m kube-system pod/kube-apiserver-allinone 1/1 Running 0 1m kube-system pod/kube-controller-manager-allinone 1/1 Running 0 1m kube-system pod/kube-proxy-zz9t9 1/1 Running 0 2m kube-system pod/kube-scheduler-allinone 1/1 Running 0 1m NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 2m kube-system service/calico-etcd ClusterIP 10.96.232.136 \u0026lt;none\u0026gt; 6666/TCP 1m kube-system service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 2m NAMESPACE NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE kube-system daemonset.apps/calico-etcd 1 1 1 1 1 node-role.kubernetes.io/master= 1m kube-system daemonset.apps/calico-node 1 1 1 1 1 \u0026lt;none\u0026gt; 1m kube-system daemonset.apps/kube-proxy 1 1 1 1 1 beta.kubernetes.io/arch=amd64 2m NAMESPACE NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE kube-system deployment.apps/calico-kube-controllers 1 1 1 1 1m kube-system deployment.apps/coredns 2 2 2 2 2m NAMESPACE NAME DESIRED CURRENT READY AGE kube-system replicaset.apps/calico-kube-controllers-84fd4db7cd 1 1 1 1m kube-system replicaset.apps/coredns-78fcdf6894  To remove the taint:\nkubectl taint node allinone node-role.kubernetes.io/master:NoSchedule-  Install Helm Create the service account\nkubectl create serviceaccount tiller --namespace kube-system serviceaccount/tiller created  kubectl replace -f rbac-config.yaml serviceaccount/tiller replaced clusterrolebinding.rbac.authorization.k8s.io/tiller replaced  Init Tiller\nhelm init --service-account tiller Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster  kubectl get all -n kube-system NAME READY STATUS RESTARTS AGE pod/calico-etcd-d6gn7 1/1 Running 0 4m pod/calico-kube-controllers-84fd4db7cd-ctctz 1/1 Running 0 4m pod/calico-node-bg4w8 2/2 Running 0 4m pod/coredns-78fcdf6894-7d5mf 1/1 Running 0 5m pod/coredns-78fcdf6894-xv9lm 1/1 Running 0 5m pod/etcd-allinone 1/1 Running 0 5m pod/kube-apiserver-allinone 1/1 Running 0 4m pod/kube-controller-manager-allinone 1/1 Running 0 5m pod/kube-proxy-zz9t9 1/1 Running 0 5m pod/kube-scheduler-allinone 1/1 Running 0 4m pod/tiller-deploy-759cb9df9-wrznl 1/1 Running 0 46s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/calico-etcd ClusterIP 10.96.232.136 \u0026lt;none\u0026gt; 6666/TCP 4m service/kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 6m service/tiller-deploy ClusterIP 10.103.151.9 \u0026lt;none\u0026gt; 44134/TCP 46s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/calico-etcd 1 1 1 1 1 node-role.kubernetes.io/master= 4m daemonset.apps/calico-node 1 1 1 1 1 \u0026lt;none\u0026gt; 4m daemonset.apps/kube-proxy 1 1 1 1 1 beta.kubernetes.io/arch=amd64 6m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/calico-kube-controllers 1 1 1 1 4m deployment.apps/coredns 2 2 2 2 6m deployment.apps/tiller-deploy 1 1 1 1 46s NAME DESIRED CURRENT READY AGE replicaset.apps/calico-kube-controllers-84fd4db7cd 1 1 1 4m replicaset.apps/coredns-78fcdf6894 2 2 2 5m replicaset.apps/tiller-deploy-759cb9df9 1 1 1 46s  Update the repo\nhelm repo add stable https://kubernetes-charts.storage.googleapis.com \u0026quot;stable\u0026quot; has been added to your repositories  Install and remove nginx\nhelm install stable/nginx-ingress NAME: messy-snake LAST DEPLOYED: Sat Jun 30 18:05:58 2018 NAMESPACE: default STATUS: DEPLOYED RESOURCES: ==\u0026gt; v1beta1/ClusterRoleBinding NAME AGE messy-snake-nginx-ingress 1s ==\u0026gt; v1beta1/Role NAME AGE messy-snake-nginx-ingress 1s ==\u0026gt; v1/ServiceAccount NAME SECRETS AGE messy-snake-nginx-ingress 1 1s ==\u0026gt; v1beta1/ClusterRole NAME AGE messy-snake-nginx-ingress 1s ==\u0026gt; v1/Service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE messy-snake-nginx-ingress-controller LoadBalancer 10.98.107.113 \u0026lt;pending\u0026gt; 80:31098/TCP,443:32158/TCP 1s messy-snake-nginx-ingress-default-backend ClusterIP 10.105.239.60 \u0026lt;none\u0026gt; 80/TCP 1s ==\u0026gt; v1beta1/Deployment NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE messy-snake-nginx-ingress-controller 1 1 1 0 1s messy-snake-nginx-ingress-default-backend 1 1 1 0 1s ==\u0026gt; v1beta1/PodDisruptionBudget NAME MIN AVAILABLE MAX UNAVAILABLE ALLOWED DISRUPTIONS AGE messy-snake-nginx-ingress-controller 1 N/A 0 1s messy-snake-nginx-ingress-default-backend 1 N/A 0 1s ==\u0026gt; v1/Pod(related) NAME READY STATUS RESTARTS AGE messy-snake-nginx-ingress-controller-5db5f96774-ql6nf 0/1 ContainerCreating 0 1s messy-snake-nginx-ingress-default-backend-7f877996b6-zzp82 0/1 ContainerCreating 0 1s ==\u0026gt; v1/ConfigMap NAME DATA AGE messy-snake-nginx-ingress-controller 1 1s ==\u0026gt; v1beta1/RoleBinding NAME AGE messy-snake-nginx-ingress 1s NOTES: The nginx-ingress controller has been installed. It may take a few minutes for the LoadBalancer IP to be available. You can watch the status by running 'kubectl --namespace default get services -o wide -w messy-snake-nginx-ingress-controller' An example Ingress that makes use of the controller: apiVersion: extensions/v1beta1 kind: Ingress metadata: annotations: kubernetes.io/ingress.class: nginx name: example namespace: foo spec: rules: - host: www.example.com http: paths: - backend: serviceName: exampleService servicePort: 80 path: / # This section is only required if TLS is to be enabled for the Ingress tls: - hosts: - www.example.com secretName: example-tls If TLS is enabled for the Ingress, a Secret containing the certificate and key must also be provided: apiVersion: v1 kind: Secret metadata: name: example-tls namespace: foo data: tls.crt: \u0026lt;base64 encoded cert\u0026gt; tls.key: \u0026lt;base64 encoded key\u0026gt; type: kubernetes.io/tls  helm ls NAME REVISION UPDATED STATUS CHART NAMESPACE messy-snake 1 Sat Jun 30 18:05:58 2018 DEPLOYED nginx-ingress-0.22.0 default  kubectl get all NAME READY STATUS RESTARTS AGE pod/messy-snake-nginx-ingress-controller-5db5f96774-ql6nf 1/1 Running 0 1m pod/messy-snake-nginx-ingress-default-backend-7f877996b6-zzp82 1/1 Running 0 1m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 9m service/messy-snake-nginx-ingress-controller LoadBalancer 10.98.107.113 \u0026lt;pending\u0026gt; 80:31098/TCP,443:32158/TCP 1m service/messy-snake-nginx-ingress-default-backend ClusterIP 10.105.239.60 \u0026lt;none\u0026gt; 80/TCP 1m NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE deployment.apps/messy-snake-nginx-ingress-controller 1 1 1 1 1m deployment.apps/messy-snake-nginx-ingress-default-backend 1 1 1 1 1m NAME DESIRED CURRENT READY AGE replicaset.apps/messy-snake-nginx-ingress-controller-5db5f96774 1 1 1 1m replicaset.apps/messy-snake-nginx-ingress-default-backend-7f877996b6 1 1 1 1m  Remove the test chart\nhelm delete messy-snake release \u0026quot;messy-snake\u0026quot; deleted  Use kubeadm to remove everything sudo kubeadm reset sudo docker rm $(sudo docker ps -qa) sudo docker image rm -f $(sudo docker image list -qa)  References  TBD "
},
{
	"uri": "/tags/5gc/",
	"title": "5gc",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/64/",
	"title": "64",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/odl/",
	"title": "ODL",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/ansible/",
	"title": "ansible",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/blinkt/",
	"title": "blinkt",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/calico/",
	"title": "calico",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/cni/",
	"title": "cni",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/dashboard/",
	"title": "dashboard",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/devops/",
	"title": "devops",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/dhcpd/",
	"title": "dhcpd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/docker/",
	"title": "docker",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/elte/",
	"title": "elte",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/epc/",
	"title": "epc",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/fluentd/",
	"title": "fluentd",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/github/",
	"title": "github",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/golang/",
	"title": "golang",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/grafana/",
	"title": "grafana",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/helm/",
	"title": "helm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/hypriot/",
	"title": "hypriot",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/istio/",
	"title": "istio",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/java/",
	"title": "java",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kubeadm/",
	"title": "kubeadm",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kubedge/",
	"title": "kubedge",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kubernetes/",
	"title": "kubernetes",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/kustomize/",
	"title": "kustomize",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/linux-bridging/",
	"title": "linux bridging",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/lte/",
	"title": "lte",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/nats/",
	"title": "nats",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/nr/",
	"title": "nr",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/portieris/",
	"title": "portieris",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/prometheus/",
	"title": "prometheus",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/python/",
	"title": "python",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/rpi/",
	"title": "rpi",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/security/",
	"title": "security",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/sonobuoy/",
	"title": "sonobuoy",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/test-infra/",
	"title": "test-infra",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/testing/",
	"title": "testing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/vault/",
	"title": "vault",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/wiki/",
	"title": "wiki",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/categories/wip/",
	"title": "wip",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/wpa_supplicant/",
	"title": "wpa_supplicant",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/zuul/",
	"title": "zuul",
	"tags": [],
	"description": "",
	"content": ""
}]